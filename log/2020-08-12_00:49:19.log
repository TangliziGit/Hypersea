
---------------------------------- [[#0 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+-----+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth | Acc |
+---------+----------+--------------+-------------+--------------+-------------+-----+
| current |   512    |      3       |      5      |      2       |      2      | 0.0 |
|   best  |    0     |      0       |      0      |      0       |      0      | 0.0 |
|  worst  |    0     |      0       |      0      |      0       |      0      | 1.0 |
+---------+----------+--------------+-------------+--------------+-------------+-----+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3979, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   5.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([480.,   3.,   5.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.990009720520595 acc: 0.6432
[Epoch 7] loss: 2.476622619058775 acc: 0.6892
[Epoch 11] loss: 0.829766438883322 acc: 0.6765
[Epoch 15] loss: 0.37911544224995253 acc: 0.6984
[Epoch 19] loss: 0.268418330708733 acc: 0.6798
[Epoch 23] loss: 0.21564598794302445 acc: 0.6934
[Epoch 27] loss: 0.1920432548164902 acc: 0.6905
[Epoch 31] loss: 0.16777719949345912 acc: 0.6824
[Epoch 35] loss: 0.1534125678970114 acc: 0.6914
[Epoch 39] loss: 0.1300407048687577 acc: 0.6962
[Epoch 43] loss: 0.12937429181926544 acc: 0.6958
[Epoch 47] loss: 0.11930648773394124 acc: 0.6925
[Epoch 51] loss: 0.11405852791267301 acc: 0.6886
[Epoch 55] loss: 0.10213444343573935 acc: 0.687
[Epoch 59] loss: 0.09906633129662565 acc: 0.6878
[Epoch 63] loss: 0.10168308319400071 acc: 0.6761
[Epoch 67] loss: 0.08213280818289351 acc: 0.6896
[Epoch 71] loss: 0.09033283485692767 acc: 0.6957
--> [test] acc: 0.6985
--> [accuracy] finished 0.6985
new state: tensor([480.,   3.,   5.,   2.,   2.], device='cuda:0')
new reward: 0.6985
--> [reward] 0.6985
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|   best  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|  worst  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3979, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([480.,   3.,   5.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([480.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.15931583761864 acc: 0.6107
[Epoch 7] loss: 2.7032000928583657 acc: 0.6807
[Epoch 11] loss: 0.9832560156979372 acc: 0.6921
[Epoch 15] loss: 0.4327128346142409 acc: 0.6775
[Epoch 19] loss: 0.2945412659155362 acc: 0.6797
[Epoch 23] loss: 0.244966822431025 acc: 0.6842
[Epoch 27] loss: 0.20466990348742442 acc: 0.68
[Epoch 31] loss: 0.18687987475729811 acc: 0.6683
[Epoch 35] loss: 0.16504945716989772 acc: 0.6778
[Epoch 39] loss: 0.14935292238059938 acc: 0.6783
[Epoch 43] loss: 0.13665183028801703 acc: 0.6696
[Epoch 47] loss: 0.13193503038629012 acc: 0.681
[Epoch 51] loss: 0.1290946631999615 acc: 0.6607
[Epoch 55] loss: 0.11488211967046265 acc: 0.6716
[Epoch 59] loss: 0.10722374217584729 acc: 0.6754
[Epoch 63] loss: 0.10063577796477834 acc: 0.6771
[Epoch 67] loss: 0.11099639431188774 acc: 0.6721
[Epoch 71] loss: 0.08644449693219894 acc: 0.675
--> [test] acc: 0.6811
--> [accuracy] finished 0.6811
new state: tensor([480.,   3.,   4.,   2.,   2.], device='cuda:0')
new reward: 0.6811
--> [reward] 0.6811
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  480.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6811 |
|   best  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|  worst  |  480.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6811 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0908, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3979, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([480.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.131424597309678 acc: 0.6266
[Epoch 7] loss: 2.695358050448815 acc: 0.6866
[Epoch 11] loss: 0.9627811394993911 acc: 0.6805
[Epoch 15] loss: 0.4339968564841525 acc: 0.6771
[Epoch 19] loss: 0.2974710267899401 acc: 0.675
[Epoch 23] loss: 0.23646207476544487 acc: 0.6736
[Epoch 27] loss: 0.2028774713616237 acc: 0.675
[Epoch 31] loss: 0.17819697089979183 acc: 0.6787
[Epoch 35] loss: 0.16011245007557637 acc: 0.6798
[Epoch 39] loss: 0.16036815947645802 acc: 0.6802
[Epoch 43] loss: 0.13910046496840617 acc: 0.6732
[Epoch 47] loss: 0.13532732808938647 acc: 0.6779
[Epoch 51] loss: 0.11522895980702566 acc: 0.6716
[Epoch 55] loss: 0.11125837735619748 acc: 0.6689
[Epoch 59] loss: 0.11603485491520385 acc: 0.6703
[Epoch 63] loss: 0.10399976513012672 acc: 0.6762
[Epoch 67] loss: 0.10145145449596826 acc: 0.6695
[Epoch 71] loss: 0.09727277744637655 acc: 0.6625
--> [test] acc: 0.6619
--> [accuracy] finished 0.6619
new state: tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
new reward: 0.6619
--> [reward] 0.6619
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
|   best  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([512.,   3.,   4.,   1.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.333900255620327 acc: 0.7108
[Epoch 7] loss: 1.9043580475060835 acc: 0.769
[Epoch 11] loss: 0.5667019232612132 acc: 0.7652
[Epoch 15] loss: 0.2990397089625449 acc: 0.7647
[Epoch 19] loss: 0.20606794026072905 acc: 0.7579
[Epoch 23] loss: 0.17444107596002653 acc: 0.7663
[Epoch 27] loss: 0.1542976321271428 acc: 0.7689
[Epoch 31] loss: 0.14365241241872387 acc: 0.755
[Epoch 35] loss: 0.12077227419675768 acc: 0.7491
[Epoch 39] loss: 0.11611106678190858 acc: 0.7605
[Epoch 43] loss: 0.11706586377552289 acc: 0.7617
[Epoch 47] loss: 0.10300841991601468 acc: 0.7562
[Epoch 51] loss: 0.09655278600523691 acc: 0.7512
[Epoch 55] loss: 0.09100009858265252 acc: 0.7586
[Epoch 59] loss: 0.08896075982955949 acc: 0.7607
[Epoch 63] loss: 0.08272621063373374 acc: 0.7544
[Epoch 67] loss: 0.0888739759714314 acc: 0.7591
[Epoch 71] loss: 0.07739561214439257 acc: 0.759
--> [test] acc: 0.7535
--> [accuracy] finished 0.7535
new state: tensor([512.,   3.,   4.,   1.,   2.], device='cuda:0')
new reward: 0.7535
--> [reward] 0.7535
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   1.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.151465825869909 acc: 0.6156
[Epoch 7] loss: 2.7291731557730214 acc: 0.6872
[Epoch 11] loss: 0.9815312771464858 acc: 0.6903
[Epoch 15] loss: 0.43948596629702374 acc: 0.6883
[Epoch 19] loss: 0.30189216119306317 acc: 0.6856
[Epoch 23] loss: 0.24740915475865763 acc: 0.6745
[Epoch 27] loss: 0.2069993614816986 acc: 0.676
[Epoch 31] loss: 0.19354518713748745 acc: 0.6834
[Epoch 35] loss: 0.1626808324023662 acc: 0.6792
[Epoch 39] loss: 0.15142295727996952 acc: 0.6718
[Epoch 43] loss: 0.14258145221301813 acc: 0.6792
[Epoch 47] loss: 0.12939865638732986 acc: 0.675
[Epoch 51] loss: 0.1272754234278484 acc: 0.6791
[Epoch 55] loss: 0.11550288612488657 acc: 0.6688
[Epoch 59] loss: 0.11155879724999447 acc: 0.6807
[Epoch 63] loss: 0.11010203649655527 acc: 0.6757
[Epoch 67] loss: 0.09105152322385518 acc: 0.6863
[Epoch 71] loss: 0.09824261732418518 acc: 0.6753
--> [test] acc: 0.6772
--> [accuracy] finished 0.6772
new state: tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
new reward: 0.6772
--> [reward] 0.6772
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6772 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.906394550410073 acc: 0.6606
[Epoch 7] loss: 2.4659550529154366 acc: 0.7067
[Epoch 11] loss: 0.8564009316113141 acc: 0.7108
[Epoch 15] loss: 0.3819308122405616 acc: 0.7147
[Epoch 19] loss: 0.2630984958790033 acc: 0.7171
[Epoch 23] loss: 0.21635688648647283 acc: 0.704
[Epoch 27] loss: 0.18826018219523113 acc: 0.7154
[Epoch 31] loss: 0.16043504482299528 acc: 0.717
[Epoch 35] loss: 0.14487309926106115 acc: 0.7168
[Epoch 39] loss: 0.14572927875496694 acc: 0.7105
[Epoch 43] loss: 0.1266570470302992 acc: 0.7183
[Epoch 47] loss: 0.11481950491847819 acc: 0.706
[Epoch 51] loss: 0.1097071745535335 acc: 0.6968
[Epoch 55] loss: 0.11541454252594001 acc: 0.6994
[Epoch 59] loss: 0.09188940894701864 acc: 0.7037
[Epoch 63] loss: 0.10088358583602258 acc: 0.6958
[Epoch 67] loss: 0.09182379016643176 acc: 0.7075
[Epoch 71] loss: 0.08483582817237167 acc: 0.701
--> [test] acc: 0.7097
--> [accuracy] finished 0.7097
new state: tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7097
--> [reward] 0.7097
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     3.0      |     2.0     | 0.7097 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.930322900605018 acc: 0.661
[Epoch 7] loss: 2.475655254157608 acc: 0.7197
[Epoch 11] loss: 0.8651872173933041 acc: 0.7074
[Epoch 15] loss: 0.3848636535036823 acc: 0.6929
[Epoch 19] loss: 0.26342981819377836 acc: 0.7061
[Epoch 23] loss: 0.21164813610103428 acc: 0.7088
[Epoch 27] loss: 0.19138645104673283 acc: 0.7078
[Epoch 31] loss: 0.16218380495915405 acc: 0.7042
[Epoch 35] loss: 0.1547046505966369 acc: 0.7071
[Epoch 39] loss: 0.13449242241356685 acc: 0.7035
[Epoch 43] loss: 0.12193176249408966 acc: 0.7126
[Epoch 47] loss: 0.11439606877382669 acc: 0.7027
[Epoch 51] loss: 0.11304686945217574 acc: 0.7024
[Epoch 55] loss: 0.09878534129332475 acc: 0.6994
[Epoch 59] loss: 0.10480734937564681 acc: 0.7037
[Epoch 63] loss: 0.09560183922300482 acc: 0.6944
[Epoch 67] loss: 0.08395879537157734 acc: 0.7003
[Epoch 71] loss: 0.08996566427785836 acc: 0.7039
--> [test] acc: 0.6993
--> [accuracy] finished 0.6993
new state: tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.6993
--> [reward] 0.6993
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     3.0      |     2.0     | 0.6993 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([512.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.816040892125396 acc: 0.6757
[Epoch 7] loss: 2.3588149222495307 acc: 0.7267
[Epoch 11] loss: 0.8160821267205964 acc: 0.7199
[Epoch 15] loss: 0.3559085613240481 acc: 0.726
[Epoch 19] loss: 0.2628774145175048 acc: 0.7301
[Epoch 23] loss: 0.21241015985922512 acc: 0.7172
[Epoch 27] loss: 0.1826743110192611 acc: 0.7215
[Epoch 31] loss: 0.15967830286844803 acc: 0.7157
[Epoch 35] loss: 0.1453398948730639 acc: 0.7108
[Epoch 39] loss: 0.12544048839154037 acc: 0.7232
[Epoch 43] loss: 0.1294789375746837 acc: 0.7114
[Epoch 47] loss: 0.11904998434721814 acc: 0.7118
[Epoch 51] loss: 0.11003457948975169 acc: 0.7084
[Epoch 55] loss: 0.0998212387675897 acc: 0.7143
[Epoch 59] loss: 0.10375883018054888 acc: 0.7182
[Epoch 63] loss: 0.08879174631984567 acc: 0.7084
[Epoch 67] loss: 0.09021682886149535 acc: 0.7141
[Epoch 71] loss: 0.08998776407947506 acc: 0.7236
--> [test] acc: 0.7087
--> [accuracy] finished 0.7087
new state: tensor([512.,   3.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.7087
--> [reward] 0.7087
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     3.0      |     3.0     | 0.7087 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([544.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.84126653466993 acc: 0.6729
[Epoch 7] loss: 2.3571533239482307 acc: 0.7102
[Epoch 11] loss: 0.8172509450932293 acc: 0.7156
[Epoch 15] loss: 0.37072007740607194 acc: 0.7352
[Epoch 19] loss: 0.2640466430865209 acc: 0.7189
[Epoch 23] loss: 0.20796513153702173 acc: 0.7175
[Epoch 27] loss: 0.1801701035467274 acc: 0.7267
[Epoch 31] loss: 0.16400052887021951 acc: 0.7254
[Epoch 35] loss: 0.1485254560737773 acc: 0.7245
[Epoch 39] loss: 0.13325845835732816 acc: 0.7226
[Epoch 43] loss: 0.12726008919808451 acc: 0.7189
[Epoch 47] loss: 0.11234826605071498 acc: 0.7223
[Epoch 51] loss: 0.1060953471164131 acc: 0.7144
[Epoch 55] loss: 0.10499709259438486 acc: 0.716
[Epoch 59] loss: 0.0940324699237009 acc: 0.7196
[Epoch 63] loss: 0.08732998882994399 acc: 0.7144
[Epoch 67] loss: 0.08204876907679545 acc: 0.7176
[Epoch 71] loss: 0.09616912943382493 acc: 0.7226
--> [test] acc: 0.7124
--> [accuracy] finished 0.7124
new state: tensor([544.,   3.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.7124
--> [reward] 0.7124
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     3.0      |     4.0     |     3.0      |     3.0     | 0.7124 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([544.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([576.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.816857513869205 acc: 0.6513
[Epoch 7] loss: 2.3201236229037385 acc: 0.7181
[Epoch 11] loss: 0.7831668831417551 acc: 0.7127
[Epoch 15] loss: 0.3557751576732987 acc: 0.6981
[Epoch 19] loss: 0.25263676147603087 acc: 0.7202
[Epoch 23] loss: 0.21065582326658622 acc: 0.7151
[Epoch 27] loss: 0.18455977031194112 acc: 0.7163
[Epoch 31] loss: 0.15973438823427719 acc: 0.7202
[Epoch 35] loss: 0.14083464512341867 acc: 0.7183
[Epoch 39] loss: 0.13469377056876544 acc: 0.7182
[Epoch 43] loss: 0.11560629577944032 acc: 0.714
[Epoch 47] loss: 0.1272637384736439 acc: 0.7147
[Epoch 51] loss: 0.10573872725438813 acc: 0.7181
[Epoch 55] loss: 0.10502337777818603 acc: 0.7122
[Epoch 59] loss: 0.08544990725700966 acc: 0.7199
[Epoch 63] loss: 0.09685345903596343 acc: 0.7147
[Epoch 67] loss: 0.08918034023893497 acc: 0.7157
[Epoch 71] loss: 0.08789900424832162 acc: 0.7108
--> [test] acc: 0.712
--> [accuracy] finished 0.712
new state: tensor([576.,   3.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.712
--> [reward] 0.712
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     3.0      |     4.0     |     3.0      |     3.0     | 0.712  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([576.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([576.,   4.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.571665022531739 acc: 0.6838
[Epoch 7] loss: 2.014697411481072 acc: 0.7337
[Epoch 11] loss: 0.5949647559515198 acc: 0.7315
[Epoch 15] loss: 0.2824969606931367 acc: 0.7383
[Epoch 19] loss: 0.2124998050043955 acc: 0.7332
[Epoch 23] loss: 0.18260928314141073 acc: 0.7297
[Epoch 27] loss: 0.14646008003579306 acc: 0.7412
[Epoch 31] loss: 0.13754390252758855 acc: 0.7275
[Epoch 35] loss: 0.13035231112214304 acc: 0.7305
[Epoch 39] loss: 0.11614549698312875 acc: 0.7257
[Epoch 43] loss: 0.10119316139784844 acc: 0.7382
[Epoch 47] loss: 0.09249196195369944 acc: 0.7407
[Epoch 51] loss: 0.09312847522956788 acc: 0.7325
[Epoch 55] loss: 0.08234856635822779 acc: 0.74
[Epoch 59] loss: 0.08461515476588932 acc: 0.733
[Epoch 63] loss: 0.0751213446012972 acc: 0.7329
[Epoch 67] loss: 0.08293134790850873 acc: 0.7319
[Epoch 71] loss: 0.06452424114074587 acc: 0.7328
--> [test] acc: 0.741
--> [accuracy] finished 0.741
new state: tensor([576.,   4.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.741
--> [reward] 0.741
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     4.0     |     3.0      |     3.0     | 0.741  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.634099221747855 acc: 0.6803
[Epoch 7] loss: 2.0975419279102168 acc: 0.735
[Epoch 11] loss: 0.6210175061174442 acc: 0.7386
[Epoch 15] loss: 0.29551740886543487 acc: 0.7423
[Epoch 19] loss: 0.2169201151132012 acc: 0.7315
[Epoch 23] loss: 0.1805432194705738 acc: 0.7308
[Epoch 27] loss: 0.1584315983545216 acc: 0.7412
[Epoch 31] loss: 0.13491928377105375 acc: 0.732
[Epoch 35] loss: 0.12837660387206032 acc: 0.7331
[Epoch 39] loss: 0.11265086383677012 acc: 0.7205
[Epoch 43] loss: 0.10990770785090373 acc: 0.7258
[Epoch 47] loss: 0.09915322661974474 acc: 0.7342
[Epoch 51] loss: 0.0928924212484118 acc: 0.7227
[Epoch 55] loss: 0.0878794494816257 acc: 0.7331
[Epoch 59] loss: 0.07795101212328086 acc: 0.7351
[Epoch 63] loss: 0.08632333772888173 acc: 0.7248
[Epoch 67] loss: 0.0737705435937323 acc: 0.7245
[Epoch 71] loss: 0.0789069974058977 acc: 0.7313
--> [test] acc: 0.7319
--> [accuracy] finished 0.7319
new state: tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7319
--> [reward] 0.7319
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     4.0     |     3.0      |     2.0     | 0.7319 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.560944992715441 acc: 0.6809
[Epoch 7] loss: 2.0533450616671303 acc: 0.7475
[Epoch 11] loss: 0.6328182504714831 acc: 0.7375
[Epoch 15] loss: 0.2931417767482493 acc: 0.7339
[Epoch 19] loss: 0.21899100947086617 acc: 0.733
[Epoch 23] loss: 0.1833278928833354 acc: 0.7208
[Epoch 27] loss: 0.16035485634451632 acc: 0.7194
[Epoch 31] loss: 0.1405430432061291 acc: 0.7379
[Epoch 35] loss: 0.1251165959769693 acc: 0.7347
[Epoch 39] loss: 0.11292671080669174 acc: 0.7257
[Epoch 43] loss: 0.10370537924194409 acc: 0.7335
[Epoch 47] loss: 0.10242776451942028 acc: 0.7237
[Epoch 51] loss: 0.09325666310232314 acc: 0.7288
[Epoch 55] loss: 0.08631275844408313 acc: 0.7298
[Epoch 59] loss: 0.08977858241304489 acc: 0.7254
[Epoch 63] loss: 0.07191629337904322 acc: 0.7357
[Epoch 67] loss: 0.07451394307212737 acc: 0.7288
[Epoch 71] loss: 0.07812190436802166 acc: 0.7233
--> [test] acc: 0.7184
--> [accuracy] finished 0.7184
new state: tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7184
--> [reward] 0.7184
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     4.0     |     3.0      |     2.0     | 0.7184 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([544.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.601696088490889 acc: 0.6961
[Epoch 7] loss: 2.0520454467943563 acc: 0.7443
[Epoch 11] loss: 0.614196887659028 acc: 0.7228
[Epoch 15] loss: 0.2886371792644224 acc: 0.731
[Epoch 19] loss: 0.2126521727765727 acc: 0.7264
[Epoch 23] loss: 0.17856211650430623 acc: 0.7351
[Epoch 27] loss: 0.15946654892524184 acc: 0.7375
[Epoch 31] loss: 0.13986666572978124 acc: 0.7233
[Epoch 35] loss: 0.12760439566369322 acc: 0.73
[Epoch 39] loss: 0.11501521820826527 acc: 0.7301
[Epoch 43] loss: 0.09824461249344031 acc: 0.7352
[Epoch 47] loss: 0.10041545114665747 acc: 0.7281
[Epoch 51] loss: 0.09462506438681709 acc: 0.7413
[Epoch 55] loss: 0.08384672179460868 acc: 0.734
[Epoch 59] loss: 0.08045864981048934 acc: 0.735
[Epoch 63] loss: 0.07750322070696851 acc: 0.7331
[Epoch 67] loss: 0.07656456263537299 acc: 0.7253
[Epoch 71] loss: 0.07763851146586477 acc: 0.7143
--> [test] acc: 0.7285
--> [accuracy] finished 0.7285
new state: tensor([544.,   4.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7285
--> [reward] 0.7285
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     4.0     |     3.0      |     2.0     | 0.7285 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([544.,   5.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.381673050048711 acc: 0.6913
[Epoch 7] loss: 1.8407085911011147 acc: 0.7539
[Epoch 11] loss: 0.4978407429235861 acc: 0.7403
[Epoch 15] loss: 0.2643670494932577 acc: 0.7441
[Epoch 19] loss: 0.20233813711606405 acc: 0.7343
[Epoch 23] loss: 0.16508777702079558 acc: 0.7323
[Epoch 27] loss: 0.13662295448629524 acc: 0.7335
[Epoch 31] loss: 0.12729071704027675 acc: 0.734
[Epoch 35] loss: 0.10812950206210699 acc: 0.7374
[Epoch 39] loss: 0.11056260846774368 acc: 0.7358
[Epoch 43] loss: 0.09475541054902364 acc: 0.7269
[Epoch 47] loss: 0.09108649469588113 acc: 0.7391
[Epoch 51] loss: 0.08255280210914047 acc: 0.7309
[Epoch 55] loss: 0.08888586491162953 acc: 0.736
[Epoch 59] loss: 0.07373407602850872 acc: 0.7351
[Epoch 63] loss: 0.06956776483562034 acc: 0.7289
[Epoch 67] loss: 0.0788945442799817 acc: 0.735
[Epoch 71] loss: 0.06056994798645997 acc: 0.7376
--> [test] acc: 0.7356
--> [accuracy] finished 0.7356
new state: tensor([544.,   5.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7356
--> [reward] 0.7356
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2710]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5421]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7363]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7226]], device='cuda:0')
------ ------
delta_t: tensor([[0.7363]], device='cuda:0')
rewards[i]: 0.7356
values[i+1]: tensor([[-0.0131]], device='cuda:0')
values[i]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7363]], device='cuda:0')
delta_t: tensor([[0.7363]], device='cuda:0')
------ ------
policy_loss: 1.7415450811386108
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7363]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.3340]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.1259]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4580]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4439]], device='cuda:0')
------ ------
delta_t: tensor([[0.7291]], device='cuda:0')
rewards[i]: 0.7285
values[i+1]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0141]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4580]], device='cuda:0')
delta_t: tensor([[0.7291]], device='cuda:0')
------ ------
policy_loss: 5.213724613189697
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4580]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.6715]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.6750]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1622]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1479]], device='cuda:0')
------ ------
delta_t: tensor([[0.7187]], device='cuda:0')
rewards[i]: 0.7184
values[i+1]: tensor([[-0.0141]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0143]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1622]], device='cuda:0')
delta_t: tensor([[0.7187]], device='cuda:0')
------ ------
policy_loss: 10.374532699584961
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1622]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.7980]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.2530]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.8728]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8583]], device='cuda:0')
------ ------
delta_t: tensor([[0.7322]], device='cuda:0')
rewards[i]: 0.7319
values[i+1]: tensor([[-0.0143]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.8728]], device='cuda:0')
delta_t: tensor([[0.7322]], device='cuda:0')
------ ------
policy_loss: 17.239408493041992
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.8728]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[14.2249]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.8538]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.5852]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.5707]], device='cuda:0')
------ ------
delta_t: tensor([[0.7411]], device='cuda:0')
rewards[i]: 0.741
values[i+1]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.5852]], device='cuda:0')
delta_t: tensor([[0.7411]], device='cuda:0')
------ ------
policy_loss: 25.812698364257812
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.5852]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[23.3028]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[18.1557]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2610]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2470]], device='cuda:0')
------ ------
delta_t: tensor([[0.7116]], device='cuda:0')
rewards[i]: 0.712
values[i+1]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0140]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2610]], device='cuda:0')
delta_t: tensor([[0.7116]], device='cuda:0')
------ ------
policy_loss: 36.00589370727539
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2610]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[35.4580]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.3106]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9306]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.9169]], device='cuda:0')
------ ------
delta_t: tensor([[0.7122]], device='cuda:0')
rewards[i]: 0.7124
values[i+1]: tensor([[-0.0140]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9306]], device='cuda:0')
delta_t: tensor([[0.7122]], device='cuda:0')
------ ------
policy_loss: 47.80474090576172
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9306]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[51.0836]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[31.2512]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.5903]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.5765]], device='cuda:0')
------ ------
delta_t: tensor([[0.7090]], device='cuda:0')
rewards[i]: 0.7087
values[i+1]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0138]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.5903]], device='cuda:0')
delta_t: tensor([[0.7090]], device='cuda:0')
------ ------
policy_loss: 61.18537139892578
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.5903]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[70.5161]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[38.8649]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.2342]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2200]], device='cuda:0')
------ ------
delta_t: tensor([[0.6998]], device='cuda:0')
rewards[i]: 0.6993
values[i+1]: tensor([[-0.0138]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0142]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.2342]], device='cuda:0')
delta_t: tensor([[0.6998]], device='cuda:0')
------ ------
policy_loss: 76.11051177978516
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.2342]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[94.1984]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[47.3647]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.8822]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8675]], device='cuda:0')
------ ------
delta_t: tensor([[0.7104]], device='cuda:0')
rewards[i]: 0.7097
values[i+1]: tensor([[-0.0142]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0147]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.8822]], device='cuda:0')
delta_t: tensor([[0.7104]], device='cuda:0')
------ ------
policy_loss: 92.58930206298828
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.8822]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[122.2586]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.1203]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4913]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.4760]], device='cuda:0')
------ ------
delta_t: tensor([[0.6780]], device='cuda:0')
rewards[i]: 0.6772
values[i+1]: tensor([[-0.0147]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0153]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4913]], device='cuda:0')
delta_t: tensor([[0.6780]], device='cuda:0')
------ ------
policy_loss: 110.52873992919922
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4913]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[155.6381]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[66.7590]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.1706]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1548]], device='cuda:0')
------ ------
delta_t: tensor([[0.7542]], device='cuda:0')
rewards[i]: 0.7535
values[i+1]: tensor([[-0.0153]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0159]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.1706]], device='cuda:0')
delta_t: tensor([[0.7542]], device='cuda:0')
------ ------
policy_loss: 130.09693908691406
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.1706]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[193.9302]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[76.5842]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.7512]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7351]], device='cuda:0')
------ ------
delta_t: tensor([[0.6623]], device='cuda:0')
rewards[i]: 0.6619
values[i+1]: tensor([[-0.0159]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0161]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.7512]], device='cuda:0')
delta_t: tensor([[0.6623]], device='cuda:0')
------ ------
policy_loss: 151.0573272705078
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.7512]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[237.5952]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[87.3300]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.3451]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.3289]], device='cuda:0')
------ ------
delta_t: tensor([[0.6813]], device='cuda:0')
rewards[i]: 0.6811
values[i+1]: tensor([[-0.0161]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0162]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.3451]], device='cuda:0')
delta_t: tensor([[0.6813]], device='cuda:0')
------ ------
policy_loss: 173.44200134277344
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.3451]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[287.0695]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[98.9485]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.9473]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.9341]], device='cuda:0')
------ ------
delta_t: tensor([[0.6957]], device='cuda:0')
rewards[i]: 0.6985
values[i+1]: tensor([[-0.0162]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0132]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.9473]], device='cuda:0')
delta_t: tensor([[0.6957]], device='cuda:0')
------ ------
policy_loss: 197.27040100097656
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.9473]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 197.27040100097656
value_loss: 287.0694580078125
loss: 340.80511474609375



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 1.7115e-02,  1.0272e-04,  1.4669e-04,  7.3556e-05,  7.1589e-05],
        [-1.6462e-01, -9.8665e-04, -1.3858e-03, -7.1846e-04, -6.9394e-04],
        [ 2.1713e-05,  1.3429e-07,  1.1702e-07,  7.0385e-08,  9.4613e-08],
        [-2.9437e-01, -1.7641e-03, -2.4461e-03, -1.2975e-03, -1.2494e-03],
        [-1.1380e+00, -6.8203e-03, -9.5060e-03, -5.0105e-03, -4.8185e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-4.8263e-05, -1.5527e-04, -2.1363e-05,  1.4643e-04, -6.3138e-05],
        [ 5.0610e-04,  1.5599e-03,  2.1350e-04, -1.4782e-03,  6.3134e-04],
        [-8.7529e-08, -1.3393e-07, -1.5592e-08,  1.3879e-07, -4.6432e-08],
        [ 9.5505e-04,  2.8679e-03,  3.9122e-04, -2.7262e-03,  1.1574e-03],
        [ 3.5992e-03,  1.0841e-02,  1.4795e-03, -1.0304e-02,  4.3777e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.3642,  0.3408, -0.3626,  0.2794, -0.1596],
        [ 0.1974, -0.1847,  0.1965, -0.1514,  0.0865],
        [ 0.0801, -0.0750,  0.0797, -0.0615,  0.0352],
        [ 0.0165, -0.0155,  0.0165, -0.0127,  0.0072],
        [ 0.0702, -0.0657,  0.0699, -0.0538,  0.0306]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0060, -0.0013,  0.0001,  0.0024,  0.0006],
        [ 0.0219,  0.0609,  0.0081, -0.0582,  0.0242],
        [-0.0140, -0.0406, -0.0055,  0.0388, -0.0163],
        [-0.0024, -0.0125, -0.0018,  0.0111, -0.0052],
        [-0.0051,  0.0004,  0.0003,  0.0017,  0.0007],
        [-0.0140, -0.0406, -0.0055,  0.0388, -0.0163],
        [ 0.0012,  0.0054,  0.0008, -0.0044,  0.0020],
        [ 0.0176,  0.0450,  0.0061, -0.0434,  0.0177],
        [-0.0061, -0.0216, -0.0030,  0.0200, -0.0088],
        [-0.0002, -0.0066, -0.0010,  0.0055, -0.0028],
        [ 0.0070,  0.0117,  0.0014, -0.0123,  0.0044]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 0.8456,  2.4571,  0.3337, -2.3463,  0.9884]], device='cuda:0')
--> [loss] 340.80511474609375

---------------------------------- [[#1 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     5.0      |     4.0     |     3.0      |     2.0     | 0.7356 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([544.,   5.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([544.,   5.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.623253391984174 acc: 0.6816
[Epoch 7] loss: 2.167883849776614 acc: 0.7259
[Epoch 11] loss: 0.6744432548951843 acc: 0.7191
[Epoch 15] loss: 0.32361157107002597 acc: 0.7235
[Epoch 19] loss: 0.23166847268067053 acc: 0.6896
[Epoch 23] loss: 0.19869927406939855 acc: 0.7142
[Epoch 27] loss: 0.16777310717155408 acc: 0.717
[Epoch 31] loss: 0.14092617989291467 acc: 0.7232
[Epoch 35] loss: 0.13345155116620824 acc: 0.7124
[Epoch 39] loss: 0.12561024058267206 acc: 0.7166
[Epoch 43] loss: 0.10807429483521235 acc: 0.7049
[Epoch 47] loss: 0.10836874138530525 acc: 0.7186
[Epoch 51] loss: 0.0899356231295868 acc: 0.7068
[Epoch 55] loss: 0.09884336699857889 acc: 0.7055
[Epoch 59] loss: 0.08778610208388561 acc: 0.7142
[Epoch 63] loss: 0.08497140812419612 acc: 0.7127
[Epoch 67] loss: 0.08126076229471628 acc: 0.7115
[Epoch 71] loss: 0.07303839912313907 acc: 0.7239
--> [test] acc: 0.7121
--> [accuracy] finished 0.7121
new state: tensor([544.,   5.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.7121
--> [reward] 0.7121
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     5.0      |     3.0     |     3.0      |     2.0     | 0.7121 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([544.,   5.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([544.,   5.,   2.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.577200945685892 acc: 0.6843
[Epoch 7] loss: 2.1160353033820076 acc: 0.7373
[Epoch 11] loss: 0.6694119223071944 acc: 0.735
[Epoch 15] loss: 0.3131303842944067 acc: 0.7467
[Epoch 19] loss: 0.23134880171626654 acc: 0.7464
[Epoch 23] loss: 0.19491590728835606 acc: 0.7448
[Epoch 27] loss: 0.16226112495040726 acc: 0.739
[Epoch 31] loss: 0.15445450343229733 acc: 0.7404
[Epoch 35] loss: 0.13448596435606175 acc: 0.739
[Epoch 39] loss: 0.12085152569297186 acc: 0.7482
[Epoch 43] loss: 0.123282295861341 acc: 0.7519
[Epoch 47] loss: 0.10969381391480469 acc: 0.7481
[Epoch 51] loss: 0.09834206217418776 acc: 0.731
[Epoch 55] loss: 0.09671043233036081 acc: 0.7485
[Epoch 59] loss: 0.10108398234702723 acc: 0.7516
[Epoch 63] loss: 0.07960556423180806 acc: 0.7468
[Epoch 67] loss: 0.09174281372325471 acc: 0.7355
[Epoch 71] loss: 0.08185996226501911 acc: 0.7471
--> [test] acc: 0.7422
--> [accuracy] finished 0.7422
new state: tensor([544.,   5.,   2.,   3.,   2.], device='cuda:0')
new reward: 0.7422
--> [reward] 0.7422
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     5.0      |     2.0     |     3.0      |     2.0     | 0.7422 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([544.,   5.,   2.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.737458702853269 acc: 0.6589
[Epoch 7] loss: 2.3010216050059595 acc: 0.7517
[Epoch 11] loss: 0.7860786244749566 acc: 0.7419
[Epoch 15] loss: 0.35744311903958276 acc: 0.74
[Epoch 19] loss: 0.26497560328401415 acc: 0.7401
[Epoch 23] loss: 0.19699233957349568 acc: 0.7442
[Epoch 27] loss: 0.18696118477026902 acc: 0.7463
[Epoch 31] loss: 0.17378621902245353 acc: 0.7353
[Epoch 35] loss: 0.1491567071424821 acc: 0.7348
[Epoch 39] loss: 0.13594619990052545 acc: 0.7238
[Epoch 43] loss: 0.12941549245989822 acc: 0.7332
[Epoch 47] loss: 0.1149515214601241 acc: 0.7475
[Epoch 51] loss: 0.11221356890247682 acc: 0.7305
[Epoch 55] loss: 0.1048854259652612 acc: 0.7467
[Epoch 59] loss: 0.10260053657625309 acc: 0.7429
[Epoch 63] loss: 0.09204788291302826 acc: 0.739
[Epoch 67] loss: 0.09721492136330784 acc: 0.7426
[Epoch 71] loss: 0.09366050575930707 acc: 0.7452
--> [test] acc: 0.7373
--> [accuracy] finished 0.7373
new state: tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
new reward: 0.7373
--> [reward] 0.7373
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     2.0     |     3.0      |     2.0     | 0.7373 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.667269360226438 acc: 0.6922
[Epoch 7] loss: 2.259588076292401 acc: 0.7401
[Epoch 11] loss: 0.7737545835430665 acc: 0.7516
[Epoch 15] loss: 0.357769076205085 acc: 0.7357
[Epoch 19] loss: 0.24926428696440767 acc: 0.7407
[Epoch 23] loss: 0.21285564885439012 acc: 0.738
[Epoch 27] loss: 0.19417922036088717 acc: 0.7267
[Epoch 31] loss: 0.16167339806199607 acc: 0.7402
[Epoch 35] loss: 0.14248282969464807 acc: 0.7325
[Epoch 39] loss: 0.1448056600449602 acc: 0.732
[Epoch 43] loss: 0.12962104414191927 acc: 0.7301
[Epoch 47] loss: 0.12205976453821754 acc: 0.7383
[Epoch 51] loss: 0.12200922293998205 acc: 0.7298
[Epoch 55] loss: 0.10103707709570017 acc: 0.7297
[Epoch 59] loss: 0.1128265164194681 acc: 0.7233
[Epoch 63] loss: 0.09598203023585676 acc: 0.732
[Epoch 67] loss: 0.09986278924750416 acc: 0.7275
[Epoch 71] loss: 0.08841293115559441 acc: 0.7337
--> [test] acc: 0.7427
--> [accuracy] finished 0.7427
new state: tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
new reward: 0.7427
--> [reward] 0.7427
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     2.0     |     3.0      |     2.0     | 0.7427 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([544.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.823553222981865 acc: 0.6701
[Epoch 7] loss: 2.4090204908491097 acc: 0.7143
[Epoch 11] loss: 0.8432403150898264 acc: 0.7172
[Epoch 15] loss: 0.3692647352185853 acc: 0.6992
[Epoch 19] loss: 0.2627135406122031 acc: 0.694
[Epoch 23] loss: 0.2156377204758165 acc: 0.702
[Epoch 27] loss: 0.18794930343994934 acc: 0.7047
[Epoch 31] loss: 0.17274996839806706 acc: 0.7018
[Epoch 35] loss: 0.1432357442337791 acc: 0.7119
[Epoch 39] loss: 0.14510867617014425 acc: 0.6956
[Epoch 43] loss: 0.11918618164651687 acc: 0.7136
[Epoch 47] loss: 0.12150879108282688 acc: 0.6987
[Epoch 51] loss: 0.11394862408089973 acc: 0.7097
[Epoch 55] loss: 0.1042577017198705 acc: 0.7089
[Epoch 59] loss: 0.09815369609414655 acc: 0.7031
[Epoch 63] loss: 0.0967068015776403 acc: 0.7032
[Epoch 67] loss: 0.09251725931754312 acc: 0.6973
[Epoch 71] loss: 0.08861828466598659 acc: 0.7069
--> [test] acc: 0.6963
--> [accuracy] finished 0.6963
new state: tensor([544.,   4.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.6963
--> [reward] 0.6963
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     3.0     |     3.0      |     2.0     | 0.6963 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([576.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.8095106330064255 acc: 0.632
[Epoch 7] loss: 2.4208538857338677 acc: 0.7064
[Epoch 11] loss: 0.8260098576469495 acc: 0.6993
[Epoch 15] loss: 0.37776353199730445 acc: 0.7103
[Epoch 19] loss: 0.256861031609242 acc: 0.7159
[Epoch 23] loss: 0.20344534222646365 acc: 0.7092
[Epoch 27] loss: 0.18477953927558097 acc: 0.7053
[Epoch 31] loss: 0.16877250708258518 acc: 0.7069
[Epoch 35] loss: 0.142358877316184 acc: 0.7042
[Epoch 39] loss: 0.13374948487771898 acc: 0.701
[Epoch 43] loss: 0.12787684202884964 acc: 0.699
[Epoch 47] loss: 0.11590846946857193 acc: 0.7095
[Epoch 51] loss: 0.10349943144510433 acc: 0.7038
[Epoch 55] loss: 0.10611568216610785 acc: 0.7098
[Epoch 59] loss: 0.0965228462983292 acc: 0.7048
[Epoch 63] loss: 0.0966722337014573 acc: 0.7018
[Epoch 67] loss: 0.08881377423876007 acc: 0.7038
[Epoch 71] loss: 0.08558922693746097 acc: 0.7039
--> [test] acc: 0.694
--> [accuracy] finished 0.694
new state: tensor([576.,   4.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.694
--> [reward] 0.694
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     3.0     |     3.0      |     2.0     | 0.694  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([576.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.090855254998902 acc: 0.6326
[Epoch 7] loss: 2.8159675255151053 acc: 0.6787
[Epoch 11] loss: 1.1327215277344522 acc: 0.701
[Epoch 15] loss: 0.4804656084512582 acc: 0.6801
[Epoch 19] loss: 0.3217533864128544 acc: 0.6925
[Epoch 23] loss: 0.2509488267371493 acc: 0.6854
[Epoch 27] loss: 0.21685291712870225 acc: 0.6851
[Epoch 31] loss: 0.19003098395407733 acc: 0.6855
[Epoch 35] loss: 0.17167779875923986 acc: 0.6813
[Epoch 39] loss: 0.15937181365202227 acc: 0.6795
[Epoch 43] loss: 0.15476738550769323 acc: 0.682
[Epoch 47] loss: 0.14143986891135765 acc: 0.6817
[Epoch 51] loss: 0.12699943739692193 acc: 0.6789
[Epoch 55] loss: 0.12256017809047762 acc: 0.6828
[Epoch 59] loss: 0.1155791674940692 acc: 0.6775
[Epoch 63] loss: 0.10770297153731404 acc: 0.6804
[Epoch 67] loss: 0.10311778507355 acc: 0.6719
[Epoch 71] loss: 0.1031392978739066 acc: 0.6817
--> [test] acc: 0.681
--> [accuracy] finished 0.681
new state: tensor([576.,   3.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.681
--> [reward] 0.681
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     3.0      |     3.0     |     3.0      |     2.0     | 0.681  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.134800087155589 acc: 0.6422
[Epoch 7] loss: 2.848535491225055 acc: 0.6985
[Epoch 11] loss: 1.1380004625757942 acc: 0.6836
[Epoch 15] loss: 0.49090894043464645 acc: 0.6807
[Epoch 19] loss: 0.3127564304720258 acc: 0.6846
[Epoch 23] loss: 0.25254589006009387 acc: 0.6796
[Epoch 27] loss: 0.21530102187400812 acc: 0.6771
[Epoch 31] loss: 0.19331587263194802 acc: 0.6854
[Epoch 35] loss: 0.17655917998317563 acc: 0.6894
[Epoch 39] loss: 0.1575582971700522 acc: 0.6819
[Epoch 43] loss: 0.15281330245782804 acc: 0.6868
[Epoch 47] loss: 0.13899916961885597 acc: 0.6898
[Epoch 51] loss: 0.13139786332955256 acc: 0.6752
[Epoch 55] loss: 0.11288564226320942 acc: 0.676
[Epoch 59] loss: 0.12224401262424447 acc: 0.6845
[Epoch 63] loss: 0.10663897168520085 acc: 0.6846
[Epoch 67] loss: 0.1096384418744813 acc: 0.6853
[Epoch 71] loss: 0.10059545589420382 acc: 0.6829
--> [test] acc: 0.688
--> [accuracy] finished 0.688
new state: tensor([608.,   3.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.688
--> [reward] 0.688
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     2.0     | 0.688  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.049155171874844 acc: 0.6543
[Epoch 7] loss: 2.715934088086838 acc: 0.7023
[Epoch 11] loss: 1.058386977333242 acc: 0.6914
[Epoch 15] loss: 0.45172648640144664 acc: 0.7034
[Epoch 19] loss: 0.30605823065980775 acc: 0.708
[Epoch 23] loss: 0.23886242818535136 acc: 0.7019
[Epoch 27] loss: 0.20496251722297554 acc: 0.6999
[Epoch 31] loss: 0.18139920105485965 acc: 0.7017
[Epoch 35] loss: 0.16942394936166685 acc: 0.7016
[Epoch 39] loss: 0.1495476652799017 acc: 0.701
[Epoch 43] loss: 0.14125214529026042 acc: 0.6939
[Epoch 47] loss: 0.12802638712665423 acc: 0.7063
[Epoch 51] loss: 0.11808906233919512 acc: 0.6962
[Epoch 55] loss: 0.11481098567261873 acc: 0.6946
[Epoch 59] loss: 0.10666307975354074 acc: 0.6996
[Epoch 63] loss: 0.10663048756089123 acc: 0.6983
[Epoch 67] loss: 0.09119830146843395 acc: 0.6984
[Epoch 71] loss: 0.09957658085862503 acc: 0.7069
--> [test] acc: 0.7054
--> [accuracy] finished 0.7054
new state: tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
new reward: 0.7054
--> [reward] 0.7054
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     3.0     | 0.7054 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.040465042109379 acc: 0.6432
[Epoch 7] loss: 2.687653335540191 acc: 0.704
[Epoch 11] loss: 1.0438731361247238 acc: 0.7039
[Epoch 15] loss: 0.45540374006285234 acc: 0.711
[Epoch 19] loss: 0.2969255605593438 acc: 0.7087
[Epoch 23] loss: 0.24764724868252072 acc: 0.7088
[Epoch 27] loss: 0.20750979355076696 acc: 0.6937
[Epoch 31] loss: 0.18405744726138423 acc: 0.6971
[Epoch 35] loss: 0.16881823426593676 acc: 0.7002
[Epoch 39] loss: 0.15205544721135092 acc: 0.6955
[Epoch 43] loss: 0.14672227997613876 acc: 0.6979
[Epoch 47] loss: 0.13225224182305054 acc: 0.6999
[Epoch 51] loss: 0.12274608169170216 acc: 0.6953
[Epoch 55] loss: 0.11676207150730407 acc: 0.6954
[Epoch 59] loss: 0.10684343561163301 acc: 0.692
[Epoch 63] loss: 0.10738744379808444 acc: 0.7001
[Epoch 67] loss: 0.10205290710215298 acc: 0.6998
[Epoch 71] loss: 0.09622557061519521 acc: 0.6982
--> [test] acc: 0.699
--> [accuracy] finished 0.699
new state: tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
new reward: 0.699
--> [reward] 0.699
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     3.0     | 0.699  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.052235543727875 acc: 0.6379
[Epoch 7] loss: 2.7047894933187138 acc: 0.6969
[Epoch 11] loss: 1.104557700664796 acc: 0.7127
[Epoch 15] loss: 0.46478188724335656 acc: 0.7001
[Epoch 19] loss: 0.30278291706653204 acc: 0.7113
[Epoch 23] loss: 0.24043406614952761 acc: 0.6888
[Epoch 27] loss: 0.2160142981935569 acc: 0.7036
[Epoch 31] loss: 0.17898436763640635 acc: 0.7066
[Epoch 35] loss: 0.1683375787804537 acc: 0.6926
[Epoch 39] loss: 0.15440849926980102 acc: 0.6883
[Epoch 43] loss: 0.13932377582444522 acc: 0.7054
[Epoch 47] loss: 0.13174776865325422 acc: 0.687
[Epoch 51] loss: 0.11987506584300066 acc: 0.6996
[Epoch 55] loss: 0.11870208566250932 acc: 0.7108
[Epoch 59] loss: 0.11455364591654038 acc: 0.6894
[Epoch 63] loss: 0.10184747837912625 acc: 0.6929
[Epoch 67] loss: 0.10675257239300195 acc: 0.6928
[Epoch 71] loss: 0.10100049076154066 acc: 0.7026
--> [test] acc: 0.6985
--> [accuracy] finished 0.6985
new state: tensor([608.,   3.,   3.,   3.,   4.], device='cuda:0')
new reward: 0.6985
--> [reward] 0.6985
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     4.0     | 0.6985 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.1560194995397195 acc: 0.6347
[Epoch 7] loss: 2.8364138447720073 acc: 0.6909
[Epoch 11] loss: 1.207516530018938 acc: 0.6872
[Epoch 15] loss: 0.5259035167420079 acc: 0.6877
[Epoch 19] loss: 0.34380037577875205 acc: 0.684
[Epoch 23] loss: 0.2545899231310772 acc: 0.6635
[Epoch 27] loss: 0.23373293465055772 acc: 0.6723
[Epoch 31] loss: 0.19996250552051437 acc: 0.6768
[Epoch 35] loss: 0.1819750793240107 acc: 0.6864
[Epoch 39] loss: 0.1715490894435007 acc: 0.6867
[Epoch 43] loss: 0.1496296281047413 acc: 0.6795
[Epoch 47] loss: 0.14217295987999587 acc: 0.684
[Epoch 51] loss: 0.1260603668169378 acc: 0.6787
[Epoch 55] loss: 0.1325109853097202 acc: 0.6828
[Epoch 59] loss: 0.12143613401171573 acc: 0.6852
[Epoch 63] loss: 0.11275172922188592 acc: 0.6811
[Epoch 67] loss: 0.10707437909444047 acc: 0.676
[Epoch 71] loss: 0.1051780029612086 acc: 0.6823
--> [test] acc: 0.6688
--> [accuracy] finished 0.6688
new state: tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6688
--> [reward] 0.6688
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     5.0     | 0.6688 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.173487865406534 acc: 0.6384
[Epoch 7] loss: 2.8654475922474774 acc: 0.6692
[Epoch 11] loss: 1.2443761532302098 acc: 0.68
[Epoch 15] loss: 0.5395238570954717 acc: 0.6957
[Epoch 19] loss: 0.3409828495096101 acc: 0.6846
[Epoch 23] loss: 0.27028293547261023 acc: 0.6927
[Epoch 27] loss: 0.2319030509832913 acc: 0.6884
[Epoch 31] loss: 0.19984046636325548 acc: 0.6911
[Epoch 35] loss: 0.18190167089233467 acc: 0.6892
[Epoch 39] loss: 0.16397581949039264 acc: 0.685
[Epoch 43] loss: 0.16581190356036737 acc: 0.6823
[Epoch 47] loss: 0.13672074950907542 acc: 0.6931
[Epoch 51] loss: 0.13367498726667384 acc: 0.6812
[Epoch 55] loss: 0.13054771969199677 acc: 0.6844
[Epoch 59] loss: 0.1226138729778359 acc: 0.6816
[Epoch 63] loss: 0.1114745893220291 acc: 0.6827
[Epoch 67] loss: 0.1052486116538191 acc: 0.6835
[Epoch 71] loss: 0.09805398269573136 acc: 0.688
--> [test] acc: 0.6856
--> [accuracy] finished 0.6856
new state: tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6856
--> [reward] 0.6856
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     5.0     | 0.6856 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.903191180485289 acc: 0.6559
[Epoch 7] loss: 2.4930776023041563 acc: 0.713
[Epoch 11] loss: 0.9089423707112327 acc: 0.7174
[Epoch 15] loss: 0.3932493316090625 acc: 0.7154
[Epoch 19] loss: 0.26540324133356363 acc: 0.6967
[Epoch 23] loss: 0.21956774171518015 acc: 0.7042
[Epoch 27] loss: 0.19421328403189053 acc: 0.7124
[Epoch 31] loss: 0.166292085569552 acc: 0.7111
[Epoch 35] loss: 0.1419611016378912 acc: 0.6978
[Epoch 39] loss: 0.14131196418686595 acc: 0.7036
[Epoch 43] loss: 0.11979788286811517 acc: 0.7018
[Epoch 47] loss: 0.12394611928564356 acc: 0.7114
[Epoch 51] loss: 0.11112024797522047 acc: 0.7038
[Epoch 55] loss: 0.10409327210320155 acc: 0.7003
[Epoch 59] loss: 0.0948620399542372 acc: 0.7048
[Epoch 63] loss: 0.09450169664788563 acc: 0.7008
[Epoch 67] loss: 0.08746454220466421 acc: 0.7091
[Epoch 71] loss: 0.0900997340552928 acc: 0.7031
--> [test] acc: 0.7127
--> [accuracy] finished 0.7127
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.7127
--> [reward] 0.7127
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.7127 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.916472550700693 acc: 0.6546
[Epoch 7] loss: 2.5041381962921307 acc: 0.7167
[Epoch 11] loss: 0.9263691858047872 acc: 0.7129
[Epoch 15] loss: 0.40238266120976807 acc: 0.7173
[Epoch 19] loss: 0.26167509236785075 acc: 0.7055
[Epoch 23] loss: 0.22320955302899756 acc: 0.7054
[Epoch 27] loss: 0.19382361491518024 acc: 0.6978
[Epoch 31] loss: 0.1686417741934433 acc: 0.6983
[Epoch 35] loss: 0.1505162997261795 acc: 0.6981
[Epoch 39] loss: 0.1314912364625222 acc: 0.704
[Epoch 43] loss: 0.12464175914245113 acc: 0.7054
[Epoch 47] loss: 0.11420705241491766 acc: 0.7037
[Epoch 51] loss: 0.12489552547990837 acc: 0.707
[Epoch 55] loss: 0.09919289017305769 acc: 0.6992
[Epoch 59] loss: 0.09985545180056749 acc: 0.69
[Epoch 63] loss: 0.09184285645376739 acc: 0.7004
[Epoch 67] loss: 0.10044697944320205 acc: 0.7063
[Epoch 71] loss: 0.08070148996975454 acc: 0.7016
--> [test] acc: 0.6925
--> [accuracy] finished 0.6925
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6925
--> [reward] 0.6925
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2399]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4798]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6927]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6830]], device='cuda:0')
------ ------
delta_t: tensor([[0.6927]], device='cuda:0')
rewards[i]: 0.6925
values[i+1]: tensor([[-0.0096]], device='cuda:0')
values[i]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6927]], device='cuda:0')
delta_t: tensor([[0.6927]], device='cuda:0')
------ ------
policy_loss: 1.636962652206421
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6927]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.2181]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.9565]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.3987]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3889]], device='cuda:0')
------ ------
delta_t: tensor([[0.7130]], device='cuda:0')
rewards[i]: 0.7127
values[i+1]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.3987]], device='cuda:0')
delta_t: tensor([[0.7130]], device='cuda:0')
------ ------
policy_loss: 4.967307090759277
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.3987]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.3615]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.2867]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.0704]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0606]], device='cuda:0')
------ ------
delta_t: tensor([[0.6857]], device='cuda:0')
rewards[i]: 0.6856
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.0704]], device='cuda:0')
delta_t: tensor([[0.6857]], device='cuda:0')
------ ------
policy_loss: 9.908119201660156
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.0704]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.0568]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.3907]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.7186]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7088]], device='cuda:0')
------ ------
delta_t: tensor([[0.6689]], device='cuda:0')
rewards[i]: 0.6688
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.7186]], device='cuda:0')
delta_t: tensor([[0.6689]], device='cuda:0')
------ ------
policy_loss: 16.403175354003906
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.7186]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[12.8028]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[11.4920]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3900]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3802]], device='cuda:0')
------ ------
delta_t: tensor([[0.6986]], device='cuda:0')
rewards[i]: 0.6985
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3900]], device='cuda:0')
delta_t: tensor([[0.6986]], device='cuda:0')
------ ------
policy_loss: 24.508230209350586
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3900]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[21.0250]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.4443]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.0552]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.0454]], device='cuda:0')
------ ------
delta_t: tensor([[0.6991]], device='cuda:0')
rewards[i]: 0.699
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.0552]], device='cuda:0')
delta_t: tensor([[0.6991]], device='cuda:0')
------ ------
policy_loss: 34.20783996582031
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.0552]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[32.1634]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.2768]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7198]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7104]], device='cuda:0')
------ ------
delta_t: tensor([[0.7052]], device='cuda:0')
rewards[i]: 0.7054
values[i+1]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0095]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7198]], device='cuda:0')
delta_t: tensor([[0.7052]], device='cuda:0')
------ ------
policy_loss: 45.50181198120117
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7198]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[46.5295]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[28.7323]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.3603]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3513]], device='cuda:0')
------ ------
delta_t: tensor([[0.6876]], device='cuda:0')
rewards[i]: 0.688
values[i+1]: tensor([[-0.0095]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0090]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.3603]], device='cuda:0')
delta_t: tensor([[0.6876]], device='cuda:0')
------ ------
policy_loss: 58.33038330078125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.3603]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[64.4533]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[35.8475]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.9873]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.9787]], device='cuda:0')
------ ------
delta_t: tensor([[0.6806]], device='cuda:0')
rewards[i]: 0.681
values[i+1]: tensor([[-0.0090]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.9873]], device='cuda:0')
delta_t: tensor([[0.6806]], device='cuda:0')
------ ------
policy_loss: 72.66387939453125
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.9873]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[86.3727]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[43.8387]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.6211]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.6130]], device='cuda:0')
------ ------
delta_t: tensor([[0.6937]], device='cuda:0')
rewards[i]: 0.694
values[i+1]: tensor([[-0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.6211]], device='cuda:0')
delta_t: tensor([[0.6937]], device='cuda:0')
------ ------
policy_loss: 88.51560974121094
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.6211]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[112.6623]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[52.5793]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2512]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2431]], device='cuda:0')
------ ------
delta_t: tensor([[0.6963]], device='cuda:0')
rewards[i]: 0.6963
values[i+1]: tensor([[-0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2512]], device='cuda:0')
delta_t: tensor([[0.6963]], device='cuda:0')
------ ------
policy_loss: 105.87959289550781
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2512]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[144.0381]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[62.7516]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9216]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.9134]], device='cuda:0')
------ ------
delta_t: tensor([[0.7429]], device='cuda:0')
rewards[i]: 0.7427
values[i+1]: tensor([[-0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0082]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9216]], device='cuda:0')
delta_t: tensor([[0.7429]], device='cuda:0')
------ ------
policy_loss: 124.85025787353516
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9216]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[180.8464]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[73.6164]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.5800]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.5716]], device='cuda:0')
------ ------
delta_t: tensor([[0.7376]], device='cuda:0')
rewards[i]: 0.7373
values[i+1]: tensor([[-0.0082]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.5800]], device='cuda:0')
delta_t: tensor([[0.7376]], device='cuda:0')
------ ------
policy_loss: 145.40109252929688
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.5800]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[223.5100]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.3274]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2373]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2280]], device='cuda:0')
------ ------
delta_t: tensor([[0.7431]], device='cuda:0')
rewards[i]: 0.7422
values[i+1]: tensor([[-0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0092]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2373]], device='cuda:0')
delta_t: tensor([[0.7431]], device='cuda:0')
------ ------
policy_loss: 167.52676391601562
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2373]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[272.1029]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[97.1857]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8583]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.8479]], device='cuda:0')
------ ------
delta_t: tensor([[0.7134]], device='cuda:0')
rewards[i]: 0.7121
values[i+1]: tensor([[-0.0092]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0104]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8583]], device='cuda:0')
delta_t: tensor([[0.7134]], device='cuda:0')
------ ------
policy_loss: 191.14149475097656
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8583]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 191.14149475097656
value_loss: 272.1028747558594
loss: 327.19293212890625



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 5.1277e-03,  4.0432e-05,  2.1811e-05,  2.7761e-05,  1.9369e-05],
        [-8.6425e-02, -6.5611e-04, -3.9647e-04, -4.6474e-04, -3.3777e-04],
        [-7.1240e-04, -5.2449e-06, -4.9855e-06, -3.8387e-06, -2.7927e-06],
        [-1.7202e-01, -1.2782e-03, -7.3729e-04, -9.1982e-04, -6.9242e-04],
        [-6.3560e-01, -4.6722e-03, -2.8055e-03, -3.3912e-03, -2.5589e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-3.1620e-05, -6.5399e-05, -6.6505e-06,  6.5523e-05, -2.7463e-05],
        [ 5.2179e-04,  1.0979e-03,  1.1515e-04, -1.0976e-03,  4.5900e-04],
        [ 3.8537e-06,  9.1698e-06,  1.1890e-06, -9.0480e-06,  3.6288e-06],
        [ 1.0483e-03,  2.1740e-03,  2.2063e-04, -2.1766e-03,  9.1719e-04],
        [ 3.8570e-03,  8.0282e-03,  8.2110e-04, -8.0359e-03,  3.3841e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.2050,  0.1949, -0.2044,  0.1633, -0.0997],
        [ 0.1478, -0.1404,  0.1474, -0.1176,  0.0715],
        [ 0.0185, -0.0176,  0.0184, -0.0149,  0.0093],
        [ 0.0112, -0.0106,  0.0111, -0.0089,  0.0054],
        [ 0.0275, -0.0262,  0.0274, -0.0219,  0.0134]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-2.2004e-02, -4.6924e-02, -4.9845e-03,  4.6777e-02, -1.9664e-02],
        [ 1.4003e-02,  2.9313e-02,  2.7257e-03, -2.9433e-02,  1.2829e-02],
        [ 2.1847e-02,  4.5664e-02,  4.4767e-03, -4.5524e-02,  1.9101e-02],
        [-1.7432e-02, -3.7127e-02, -3.9262e-03,  3.7054e-02, -1.5564e-02],
        [ 3.3843e-02,  7.5096e-02,  8.9016e-03, -7.4437e-02,  2.9774e-02],
        [-6.9855e-05, -1.1804e-03, -3.9035e-04,  1.0385e-03, -2.7850e-04],
        [-2.2005e-02, -4.6927e-02, -4.9848e-03,  4.6780e-02, -1.9666e-02],
        [-2.2000e-02, -4.6916e-02, -4.9836e-03,  4.6769e-02, -1.9661e-02],
        [-2.1925e-02, -4.6757e-02, -4.9667e-03,  4.6611e-02, -1.9594e-02],
        [ 1.8824e-02,  4.0439e-02,  4.5231e-03, -4.0334e-02,  1.7606e-02],
        [ 1.6919e-02,  3.5322e-02,  3.6090e-03, -3.5302e-02,  1.5116e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.3293,  2.8348,  0.3011, -2.8259,  1.1879]], device='cuda:0')
--> [loss] 327.19293212890625

---------------------------------- [[#2 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.6925 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.910923041651011 acc: 0.6331
[Epoch 7] loss: 2.4901979766843265 acc: 0.714
[Epoch 11] loss: 0.8798032390939838 acc: 0.7211
[Epoch 15] loss: 0.3852142846006948 acc: 0.706
[Epoch 19] loss: 0.26360457023019757 acc: 0.7123
[Epoch 23] loss: 0.21872549665534435 acc: 0.7069
[Epoch 27] loss: 0.18182729639332085 acc: 0.6979
[Epoch 31] loss: 0.16262847643650477 acc: 0.6986
[Epoch 35] loss: 0.1492084488105934 acc: 0.7033
[Epoch 39] loss: 0.13241085583996742 acc: 0.706
[Epoch 43] loss: 0.12482051880103644 acc: 0.702
[Epoch 47] loss: 0.11919004334460782 acc: 0.6988
[Epoch 51] loss: 0.11134227451241915 acc: 0.6998
[Epoch 55] loss: 0.10173241075073533 acc: 0.7045
[Epoch 59] loss: 0.09908448990501101 acc: 0.7075
[Epoch 63] loss: 0.08548584523776909 acc: 0.7034
[Epoch 67] loss: 0.09492555528502826 acc: 0.7015
[Epoch 71] loss: 0.0818988779666768 acc: 0.7048
--> [test] acc: 0.7117
--> [accuracy] finished 0.7117
new state: tensor([640.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.7117
--> [reward] 0.7117
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.7117 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.942778237487959 acc: 0.6588
[Epoch 7] loss: 2.525048919574684 acc: 0.7087
[Epoch 11] loss: 0.9175125216812734 acc: 0.7129
[Epoch 15] loss: 0.40236706213782664 acc: 0.6987
[Epoch 19] loss: 0.27642017619832016 acc: 0.7063
[Epoch 23] loss: 0.21603797750352213 acc: 0.6914
[Epoch 27] loss: 0.1839052123039046 acc: 0.707
[Epoch 31] loss: 0.16431166796856905 acc: 0.7055
[Epoch 35] loss: 0.15219329148435684 acc: 0.6992
[Epoch 39] loss: 0.13694276097718902 acc: 0.7053
[Epoch 43] loss: 0.13330010478110874 acc: 0.6998
[Epoch 47] loss: 0.11278866163319182 acc: 0.7056
[Epoch 51] loss: 0.10829958916329743 acc: 0.6962
[Epoch 55] loss: 0.11169031376565171 acc: 0.7083
[Epoch 59] loss: 0.08672821825570272 acc: 0.7042
[Epoch 63] loss: 0.10545915358668893 acc: 0.696
[Epoch 67] loss: 0.08710526182647328 acc: 0.7039
[Epoch 71] loss: 0.08581497232809894 acc: 0.6992
--> [test] acc: 0.6978
--> [accuracy] finished 0.6978
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6978
--> [reward] 0.6978
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.6978 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.89426069521843 acc: 0.669
[Epoch 7] loss: 2.464545570485427 acc: 0.7111
[Epoch 11] loss: 0.8978424400872435 acc: 0.6994
[Epoch 15] loss: 0.3797933064625048 acc: 0.7072
[Epoch 19] loss: 0.26208551730150764 acc: 0.7136
[Epoch 23] loss: 0.21461335701339157 acc: 0.7024
[Epoch 27] loss: 0.1801034196963548 acc: 0.7004
[Epoch 31] loss: 0.16707418506960278 acc: 0.6903
[Epoch 35] loss: 0.14479365608538203 acc: 0.7133
[Epoch 39] loss: 0.1424528881413219 acc: 0.7042
[Epoch 43] loss: 0.12554169970247753 acc: 0.7062
[Epoch 47] loss: 0.11695597131434075 acc: 0.6999
[Epoch 51] loss: 0.10425141441118915 acc: 0.7046
[Epoch 55] loss: 0.1074363694173734 acc: 0.6934
[Epoch 59] loss: 0.09168820689275117 acc: 0.6981
[Epoch 63] loss: 0.10061730401244495 acc: 0.6991
[Epoch 67] loss: 0.09080926735696081 acc: 0.7047
[Epoch 71] loss: 0.0919917626523763 acc: 0.701
--> [test] acc: 0.7029
--> [accuracy] finished 0.7029
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.7029
--> [reward] 0.7029
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.7029 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([608.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.006405542421219 acc: 0.6548
[Epoch 7] loss: 2.601421848206264 acc: 0.6986
[Epoch 11] loss: 0.9413623748837835 acc: 0.6808
[Epoch 15] loss: 0.41127906815456156 acc: 0.7
[Epoch 19] loss: 0.2737216785159486 acc: 0.6914
[Epoch 23] loss: 0.22639093028567256 acc: 0.6892
[Epoch 27] loss: 0.19708839364711891 acc: 0.681
[Epoch 31] loss: 0.17321644484510887 acc: 0.6849
[Epoch 35] loss: 0.1549679981806623 acc: 0.685
[Epoch 39] loss: 0.14346087369782007 acc: 0.6853
[Epoch 43] loss: 0.13543189356051138 acc: 0.6856
[Epoch 47] loss: 0.1270765839632877 acc: 0.6748
[Epoch 51] loss: 0.11015129120081968 acc: 0.6905
[Epoch 55] loss: 0.10749039397803624 acc: 0.6863
[Epoch 59] loss: 0.0992502116501722 acc: 0.6885
[Epoch 63] loss: 0.10395286406967681 acc: 0.6889
[Epoch 67] loss: 0.09692261408111724 acc: 0.698
[Epoch 71] loss: 0.08589272270612705 acc: 0.6886
--> [test] acc: 0.6896
--> [accuracy] finished 0.6896
new state: tensor([608.,   4.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6896
--> [reward] 0.6896
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     2.0      |     5.0     | 0.6896 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.033112505360332 acc: 0.6366
[Epoch 7] loss: 2.624532995474003 acc: 0.6843
[Epoch 11] loss: 0.9480109911154756 acc: 0.6913
[Epoch 15] loss: 0.41399304416326 acc: 0.6956
[Epoch 19] loss: 0.2748765391357186 acc: 0.6929
[Epoch 23] loss: 0.22195488398851793 acc: 0.6945
[Epoch 27] loss: 0.19720605120081883 acc: 0.6919
[Epoch 31] loss: 0.17016106836326286 acc: 0.6999
[Epoch 35] loss: 0.15347412784281364 acc: 0.6903
[Epoch 39] loss: 0.14713671442140322 acc: 0.6946
[Epoch 43] loss: 0.12520105634128814 acc: 0.6879
[Epoch 47] loss: 0.12655316050062576 acc: 0.6787
[Epoch 51] loss: 0.10712904203980875 acc: 0.6747
[Epoch 55] loss: 0.11445088803653827 acc: 0.6962
[Epoch 59] loss: 0.10707111650353769 acc: 0.6897
[Epoch 63] loss: 0.09802170877423509 acc: 0.6906
[Epoch 67] loss: 0.09378417364834948 acc: 0.6836
[Epoch 71] loss: 0.08407085891598669 acc: 0.6894
--> [test] acc: 0.6737
--> [accuracy] finished 0.6737
new state: tensor([640.,   4.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6737
--> [reward] 0.6737
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     2.0      |     5.0     | 0.6737 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.664242903442334 acc: 0.6656
[Epoch 7] loss: 2.2405215315806593 acc: 0.7071
[Epoch 11] loss: 0.7367421912453364 acc: 0.726
[Epoch 15] loss: 0.3474451372271304 acc: 0.7159
[Epoch 19] loss: 0.244424051274081 acc: 0.7218
[Epoch 23] loss: 0.19787132095240648 acc: 0.7203
[Epoch 27] loss: 0.1681378479913601 acc: 0.7183
[Epoch 31] loss: 0.15483399222383415 acc: 0.6983
[Epoch 35] loss: 0.13945287063508235 acc: 0.7162
[Epoch 39] loss: 0.12884573087922732 acc: 0.7255
[Epoch 43] loss: 0.12102984181126518 acc: 0.7114
[Epoch 47] loss: 0.10459322045120123 acc: 0.7161
[Epoch 51] loss: 0.10999483959463513 acc: 0.7188
[Epoch 55] loss: 0.09307732988301369 acc: 0.7188
[Epoch 59] loss: 0.0951423319390096 acc: 0.7134
[Epoch 63] loss: 0.0910408998514428 acc: 0.7172
[Epoch 67] loss: 0.08940209108514978 acc: 0.7183
[Epoch 71] loss: 0.07964866595935849 acc: 0.7178
--> [test] acc: 0.7155
--> [accuracy] finished 0.7155
new state: tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
new reward: 0.7155
--> [reward] 0.7155
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     1.0      |     5.0     | 0.7155 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.693694481306978 acc: 0.674
[Epoch 7] loss: 2.3127777090158 acc: 0.73
[Epoch 11] loss: 0.7847063048597416 acc: 0.7239
[Epoch 15] loss: 0.36027000273299187 acc: 0.7221
[Epoch 19] loss: 0.25478984367178603 acc: 0.7208
[Epoch 23] loss: 0.2005653611177107 acc: 0.7257
[Epoch 27] loss: 0.17828993823812783 acc: 0.7251
[Epoch 31] loss: 0.16058731965406242 acc: 0.7208
[Epoch 35] loss: 0.1416371382387055 acc: 0.7195
[Epoch 39] loss: 0.13350457251857956 acc: 0.7257
[Epoch 43] loss: 0.11864736217998273 acc: 0.7142
[Epoch 47] loss: 0.11453757242506842 acc: 0.723
[Epoch 51] loss: 0.10873714900474943 acc: 0.7167
[Epoch 55] loss: 0.09490299143273469 acc: 0.7131
[Epoch 59] loss: 0.09999288158590103 acc: 0.716
[Epoch 63] loss: 0.09865242276755173 acc: 0.7226
[Epoch 67] loss: 0.08492140808438077 acc: 0.7232
[Epoch 71] loss: 0.08533703155525009 acc: 0.7194
--> [test] acc: 0.7081
--> [accuracy] finished 0.7081
new state: tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
new reward: 0.7081
--> [reward] 0.7081
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     1.0      |     5.0     | 0.7081 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   3.,   3.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.671944728592777 acc: 0.6811
[Epoch 7] loss: 2.288410707050577 acc: 0.7382
[Epoch 11] loss: 0.7997445251811724 acc: 0.7319
[Epoch 15] loss: 0.36273262216745283 acc: 0.7311
[Epoch 19] loss: 0.2560878086863729 acc: 0.7373
[Epoch 23] loss: 0.20428098219415874 acc: 0.7341
[Epoch 27] loss: 0.1784991645337561 acc: 0.7371
[Epoch 31] loss: 0.16276899811780782 acc: 0.7294
[Epoch 35] loss: 0.14432777227390833 acc: 0.7368
[Epoch 39] loss: 0.13708044374611733 acc: 0.7289
[Epoch 43] loss: 0.12265066841326634 acc: 0.7309
[Epoch 47] loss: 0.12032050622717651 acc: 0.727
[Epoch 51] loss: 0.11305377177794075 acc: 0.7424
[Epoch 55] loss: 0.10783842493913344 acc: 0.7331
[Epoch 59] loss: 0.09514787870809398 acc: 0.7374
[Epoch 63] loss: 0.09811556455917428 acc: 0.7332
[Epoch 67] loss: 0.09947290384184446 acc: 0.734
[Epoch 71] loss: 0.07939355612099852 acc: 0.7296
--> [test] acc: 0.739
--> [accuracy] finished 0.739
new state: tensor([640.,   3.,   3.,   1.,   5.], device='cuda:0')
new reward: 0.739
--> [reward] 0.739
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     3.0     |     1.0      |     5.0     | 0.739  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   3.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([640.,   3.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.4499220593506115 acc: 0.7074
[Epoch 7] loss: 2.0340991392160013 acc: 0.7624
[Epoch 11] loss: 0.6591388524207465 acc: 0.7524
[Epoch 15] loss: 0.31536448784315446 acc: 0.7486
[Epoch 19] loss: 0.23108722154251146 acc: 0.7559
[Epoch 23] loss: 0.19426399654210985 acc: 0.7534
[Epoch 27] loss: 0.16368358007982334 acc: 0.7553
[Epoch 31] loss: 0.15729242403184057 acc: 0.7522
[Epoch 35] loss: 0.13204527600749355 acc: 0.7477
[Epoch 39] loss: 0.13073325189797547 acc: 0.7519
[Epoch 43] loss: 0.11719539850388112 acc: 0.745
[Epoch 47] loss: 0.10982894713340131 acc: 0.7491
[Epoch 51] loss: 0.10976750833672219 acc: 0.7452
[Epoch 55] loss: 0.10362794116535998 acc: 0.7536
[Epoch 59] loss: 0.08980545180413843 acc: 0.7543
[Epoch 63] loss: 0.0881449627437536 acc: 0.755
[Epoch 67] loss: 0.08950687808272265 acc: 0.7529
[Epoch 71] loss: 0.08338326974944604 acc: 0.7537
--> [test] acc: 0.7402
--> [accuracy] finished 0.7402
new state: tensor([640.,   3.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7402
--> [reward] 0.7402
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     3.0     |     1.0      |     4.0     | 0.7402 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([640.,   3.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.785918877862604 acc: 0.6636
[Epoch 7] loss: 2.5118172244190253 acc: 0.7385
[Epoch 11] loss: 0.971384584951355 acc: 0.7335
[Epoch 15] loss: 0.4211086724763331 acc: 0.724
[Epoch 19] loss: 0.2890074953789373 acc: 0.7281
[Epoch 23] loss: 0.2313716987986356 acc: 0.7252
[Epoch 27] loss: 0.20341323470682515 acc: 0.725
[Epoch 31] loss: 0.18140658845081734 acc: 0.7253
[Epoch 35] loss: 0.17097536328694093 acc: 0.7263
[Epoch 39] loss: 0.15479490305046978 acc: 0.7243
[Epoch 43] loss: 0.13528316853629888 acc: 0.7252
[Epoch 47] loss: 0.14518873931011161 acc: 0.7225
[Epoch 51] loss: 0.13277927624380878 acc: 0.7152
[Epoch 55] loss: 0.11955337160888611 acc: 0.7218
[Epoch 59] loss: 0.11498694288451462 acc: 0.7222
[Epoch 63] loss: 0.10784938323931992 acc: 0.7226
[Epoch 67] loss: 0.11058416041543188 acc: 0.7207
[Epoch 71] loss: 0.1129553380494942 acc: 0.7283
--> [test] acc: 0.7202
--> [accuracy] finished 0.7202
new state: tensor([640.,   3.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7202
--> [reward] 0.7202
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     2.0     |     1.0      |     4.0     | 0.7202 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.766990773970513 acc: 0.6558
[Epoch 7] loss: 2.503636388141481 acc: 0.7329
[Epoch 11] loss: 0.9596278500907561 acc: 0.7297
[Epoch 15] loss: 0.41544740939098396 acc: 0.727
[Epoch 19] loss: 0.2790073946766231 acc: 0.7272
[Epoch 23] loss: 0.23906199942054726 acc: 0.7292
[Epoch 27] loss: 0.20122790873965338 acc: 0.7216
[Epoch 31] loss: 0.1863022650475316 acc: 0.7209
[Epoch 35] loss: 0.17053662936019776 acc: 0.7284
[Epoch 39] loss: 0.15248327479219 acc: 0.7233
[Epoch 43] loss: 0.1414065900749391 acc: 0.7314
[Epoch 47] loss: 0.14059329612830848 acc: 0.7317
[Epoch 51] loss: 0.12359552448162871 acc: 0.729
[Epoch 55] loss: 0.12431790453621218 acc: 0.7189
[Epoch 59] loss: 0.12168662986465636 acc: 0.725
[Epoch 63] loss: 0.11638117892920133 acc: 0.7194
[Epoch 67] loss: 0.10741879582009695 acc: 0.7166
[Epoch 71] loss: 0.10372214743008842 acc: 0.7161
--> [test] acc: 0.7231
--> [accuracy] finished 0.7231
new state: tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7231
--> [reward] 0.7231
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7231 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.747821201143972 acc: 0.653
[Epoch 7] loss: 2.5125234031006505 acc: 0.7219
[Epoch 11] loss: 0.9480917941102439 acc: 0.7211
[Epoch 15] loss: 0.42650522870461804 acc: 0.7321
[Epoch 19] loss: 0.2901754067410403 acc: 0.7291
[Epoch 23] loss: 0.2341454687106716 acc: 0.7233
[Epoch 27] loss: 0.2118284961499293 acc: 0.7156
[Epoch 31] loss: 0.17786788184653082 acc: 0.7183
[Epoch 35] loss: 0.16844131138004229 acc: 0.7255
[Epoch 39] loss: 0.1513222819706306 acc: 0.7255
[Epoch 43] loss: 0.15464495024536654 acc: 0.7205
[Epoch 47] loss: 0.13793550516285782 acc: 0.7238
[Epoch 51] loss: 0.14140283598092948 acc: 0.7215
[Epoch 55] loss: 0.11784862789555865 acc: 0.7161
[Epoch 59] loss: 0.11946588906470229 acc: 0.7238
[Epoch 63] loss: 0.11624833364682773 acc: 0.7208
[Epoch 67] loss: 0.11046616989318156 acc: 0.7135
[Epoch 71] loss: 0.10259824193438843 acc: 0.7237
--> [test] acc: 0.715
--> [accuracy] finished 0.715
new state: tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.715
--> [reward] 0.715
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.715  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.995019875219106 acc: 0.6383
[Epoch 7] loss: 2.8173054099997596 acc: 0.7178
[Epoch 11] loss: 1.2396291081729296 acc: 0.6998
[Epoch 15] loss: 0.5313645039525483 acc: 0.7033
[Epoch 19] loss: 0.34829350190041847 acc: 0.7118
[Epoch 23] loss: 0.273546937874535 acc: 0.7059
[Epoch 27] loss: 0.2382705770361492 acc: 0.7047
[Epoch 31] loss: 0.2164245717622854 acc: 0.7048
[Epoch 35] loss: 0.1911355917487303 acc: 0.7028
[Epoch 39] loss: 0.1752925076476677 acc: 0.7083
[Epoch 43] loss: 0.1750240859092044 acc: 0.7031
[Epoch 47] loss: 0.16566717837486997 acc: 0.7059
[Epoch 51] loss: 0.15281828110704146 acc: 0.7034
[Epoch 55] loss: 0.13764212204052892 acc: 0.7032
[Epoch 59] loss: 0.1376161978159057 acc: 0.706
[Epoch 63] loss: 0.12901490193713086 acc: 0.702
[Epoch 67] loss: 0.13190293133876924 acc: 0.7115
[Epoch 71] loss: 0.12026951855758343 acc: 0.6989
--> [test] acc: 0.7005
--> [accuracy] finished 0.7005
new state: tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7005
--> [reward] 0.7005
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7005 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.4972426312049025 acc: 0.6011
[Epoch 7] loss: 3.6552448431244287 acc: 0.6634
[Epoch 11] loss: 2.1709987801282913 acc: 0.6545
[Epoch 15] loss: 1.0050858248530141 acc: 0.6381
[Epoch 19] loss: 0.5661337383310584 acc: 0.6467
[Epoch 23] loss: 0.4064663601396105 acc: 0.6467
[Epoch 27] loss: 0.3658994645251871 acc: 0.6484
[Epoch 31] loss: 0.29247475689863 acc: 0.6401
[Epoch 35] loss: 0.29001962769862333 acc: 0.6461
[Epoch 39] loss: 0.26305865345741897 acc: 0.6451
[Epoch 43] loss: 0.24208299344872383 acc: 0.6319
[Epoch 47] loss: 0.2288745224018536 acc: 0.6456
[Epoch 51] loss: 0.21316395411053507 acc: 0.6461
[Epoch 55] loss: 0.20486176864046346 acc: 0.6447
[Epoch 59] loss: 0.1942240868628387 acc: 0.6336
[Epoch 63] loss: 0.1927702536668314 acc: 0.6343
[Epoch 67] loss: 0.17387354994297524 acc: 0.6495
[Epoch 71] loss: 0.1740455846331985 acc: 0.6452
--> [test] acc: 0.6428
--> [accuracy] finished 0.6428
new state: tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6428
--> [reward] 0.6428
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([640.,   3.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.354171572286455 acc: 0.6178
[Epoch 7] loss: 3.338236879662175 acc: 0.6414
[Epoch 11] loss: 1.7592531785635692 acc: 0.678
[Epoch 15] loss: 0.7485307044423449 acc: 0.6664
[Epoch 19] loss: 0.4334312814533177 acc: 0.6628
[Epoch 23] loss: 0.34594920014872044 acc: 0.6701
[Epoch 27] loss: 0.29204406352747053 acc: 0.6679
[Epoch 31] loss: 0.261579584244572 acc: 0.6708
[Epoch 35] loss: 0.23262717210166062 acc: 0.6687
[Epoch 39] loss: 0.21124172812003805 acc: 0.6663
[Epoch 43] loss: 0.21245804210872296 acc: 0.6739
[Epoch 47] loss: 0.1879478434905829 acc: 0.6685
[Epoch 51] loss: 0.18807179817353445 acc: 0.6711
[Epoch 55] loss: 0.17704126608731877 acc: 0.6636
[Epoch 59] loss: 0.1645238961276534 acc: 0.6614
[Epoch 63] loss: 0.1533941753911893 acc: 0.6574
[Epoch 67] loss: 0.1528884861368181 acc: 0.664
[Epoch 71] loss: 0.13751202910338217 acc: 0.6605
--> [test] acc: 0.6629
--> [accuracy] finished 0.6629
new state: tensor([640.,   3.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6629
--> [reward] 0.6629
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2200]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4400]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6633]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6583]], device='cuda:0')
------ ------
delta_t: tensor([[0.6633]], device='cuda:0')
rewards[i]: 0.6629
values[i+1]: tensor([[-0.0046]], device='cuda:0')
values[i]: tensor([[-0.0050]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6633]], device='cuda:0')
delta_t: tensor([[0.6633]], device='cuda:0')
------ ------
policy_loss: 1.566747784614563
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6633]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0646]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6892]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2997]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.2946]], device='cuda:0')
------ ------
delta_t: tensor([[0.6430]], device='cuda:0')
rewards[i]: 0.6428
values[i+1]: tensor([[-0.0050]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2997]], device='cuda:0')
delta_t: tensor([[0.6430]], device='cuda:0')
------ ------
policy_loss: 4.659158706665039
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2997]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.0390]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.9488]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9872]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9821]], device='cuda:0')
------ ------
delta_t: tensor([[0.7005]], device='cuda:0')
rewards[i]: 0.7005
values[i+1]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0051]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9872]], device='cuda:0')
delta_t: tensor([[0.7005]], device='cuda:0')
------ ------
policy_loss: 9.40037727355957
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9872]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.6370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.1959]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.6825]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.6773]], device='cuda:0')
------ ------
delta_t: tensor([[0.7152]], device='cuda:0')
rewards[i]: 0.715
values[i+1]: tensor([[-0.0051]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.6825]], device='cuda:0')
delta_t: tensor([[0.7152]], device='cuda:0')
------ ------
policy_loss: 15.808653831481934
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.6825]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[12.3461]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[11.4182]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3791]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3736]], device='cuda:0')
------ ------
delta_t: tensor([[0.7234]], device='cuda:0')
rewards[i]: 0.7231
values[i+1]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0055]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3791]], device='cuda:0')
delta_t: tensor([[0.7234]], device='cuda:0')
------ ------
policy_loss: 23.888351440429688
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3791]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[20.6115]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.5308]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.0658]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.0601]], device='cuda:0')
------ ------
delta_t: tensor([[0.7205]], device='cuda:0')
rewards[i]: 0.7202
values[i+1]: tensor([[-0.0055]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.0658]], device='cuda:0')
delta_t: tensor([[0.7205]], device='cuda:0')
------ ------
policy_loss: 33.61322021484375
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.0658]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[31.9661]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.7092]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7654]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7597]], device='cuda:0')
------ ------
delta_t: tensor([[0.7403]], device='cuda:0')
rewards[i]: 0.7402
values[i+1]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7654]], device='cuda:0')
delta_t: tensor([[0.7403]], device='cuda:0')
------ ------
policy_loss: 45.0175895690918
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7654]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[46.8538]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.7752]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4567]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4511]], device='cuda:0')
------ ------
delta_t: tensor([[0.7389]], device='cuda:0')
rewards[i]: 0.739
values[i+1]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4567]], device='cuda:0')
delta_t: tensor([[0.7389]], device='cuda:0')
------ ------
policy_loss: 58.07859802246094
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4567]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[65.5213]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.3352]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1103]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1047]], device='cuda:0')
------ ------
delta_t: tensor([[0.7082]], device='cuda:0')
rewards[i]: 0.7081
values[i+1]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1103]], device='cuda:0')
delta_t: tensor([[0.7082]], device='cuda:0')
------ ------
policy_loss: 72.70712280273438
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1103]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[88.4007]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.7586]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7645]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7591]], device='cuda:0')
------ ------
delta_t: tensor([[0.7154]], device='cuda:0')
rewards[i]: 0.7155
values[i+1]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0054]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7645]], device='cuda:0')
delta_t: tensor([[0.7154]], device='cuda:0')
------ ------
policy_loss: 88.90335083007812
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7645]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[115.5620]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[54.3226]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.3704]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.3652]], device='cuda:0')
------ ------
delta_t: tensor([[0.6735]], device='cuda:0')
rewards[i]: 0.6737
values[i+1]: tensor([[-0.0054]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.3704]], device='cuda:0')
delta_t: tensor([[0.6735]], device='cuda:0')
------ ------
policy_loss: 106.5509033203125
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.3704]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[147.4539]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[63.7839]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9865]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.9812]], device='cuda:0')
------ ------
delta_t: tensor([[0.6898]], device='cuda:0')
rewards[i]: 0.6896
values[i+1]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0053]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9865]], device='cuda:0')
delta_t: tensor([[0.6898]], device='cuda:0')
------ ------
policy_loss: 125.6772232055664
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9865]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[184.5200]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.1321]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6100]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6043]], device='cuda:0')
------ ------
delta_t: tensor([[0.7034]], device='cuda:0')
rewards[i]: 0.7029
values[i+1]: tensor([[-0.0053]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6100]], device='cuda:0')
delta_t: tensor([[0.7034]], device='cuda:0')
------ ------
policy_loss: 146.30020141601562
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6100]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[227.0454]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.0508]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2223]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2160]], device='cuda:0')
------ ------
delta_t: tensor([[0.6984]], device='cuda:0')
rewards[i]: 0.6978
values[i+1]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0063]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2223]], device='cuda:0')
delta_t: tensor([[0.6984]], device='cuda:0')
------ ------
policy_loss: 168.38961791992188
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2223]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[275.4820]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[96.8732]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8424]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.8356]], device='cuda:0')
------ ------
delta_t: tensor([[0.7123]], device='cuda:0')
rewards[i]: 0.7117
values[i+1]: tensor([[-0.0063]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0069]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8424]], device='cuda:0')
delta_t: tensor([[0.7123]], device='cuda:0')
------ ------
policy_loss: 191.96420288085938
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8424]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 191.96420288085938
value_loss: 275.48199462890625
loss: 329.7052001953125



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 1.1431e-03,  8.0183e-06,  4.5599e-06,  8.3270e-07,  1.0207e-05],
        [-4.6056e-02, -2.9386e-04, -2.0368e-04, -1.1840e-04, -3.7317e-04],
        [-6.2578e-04, -3.6526e-06, -3.1328e-06, -3.0513e-06, -4.6732e-06],
        [-1.2708e-01, -8.0960e-04, -5.5329e-04, -2.9432e-04, -1.0225e-03],
        [-3.7426e-01, -2.4133e-03, -1.5878e-03, -7.0647e-04, -3.0245e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-7.1951e-06, -1.3445e-05, -7.5031e-07,  1.3296e-05, -6.0404e-06],
        [ 2.7937e-04,  5.5240e-04,  4.3243e-05, -5.4768e-04,  2.4175e-04],
        [ 3.6448e-06,  7.6981e-06,  7.8980e-07, -7.6475e-06,  3.2642e-06],
        [ 7.7237e-04,  1.5189e-03,  1.1580e-04, -1.5061e-03,  6.6694e-04],
        [ 2.2896e-03,  4.4491e-03,  3.1887e-04, -4.4105e-03,  1.9655e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.1299,  0.1260, -0.1297,  0.1094, -0.0695],
        [ 0.1272, -0.1231,  0.1270, -0.1062,  0.0669],
        [-0.0027,  0.0024, -0.0027,  0.0016, -0.0006],
        [ 0.0080, -0.0078,  0.0080, -0.0067,  0.0042],
        [-0.0026,  0.0025, -0.0026,  0.0019, -0.0010]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 6.3132e-03,  1.3345e-02,  1.5315e-03, -1.3088e-02,  5.5318e-03],
        [ 3.2411e-02,  6.6633e-02,  6.0540e-03, -6.5871e-02,  2.8295e-02],
        [ 1.2451e-03,  1.9217e-03,  2.5894e-05, -2.0593e-03,  1.0796e-03],
        [-2.2476e-02, -4.5317e-02, -3.8178e-03,  4.4967e-02, -1.9711e-02],
        [-6.0381e-03, -1.2621e-02, -1.0162e-03,  1.2265e-02, -5.1250e-03],
        [ 2.5197e-02,  5.0820e-02,  4.0828e-03, -5.0298e-02,  2.2064e-02],
        [ 3.4750e-02,  6.8953e-02,  5.4009e-03, -6.8545e-02,  3.0332e-02],
        [-2.5027e-02, -5.0216e-02, -4.2716e-03,  4.9907e-02, -2.1954e-02],
        [ 3.6682e-03,  6.8932e-03,  5.5226e-04, -7.0732e-03,  3.3884e-03],
        [-2.4998e-02, -5.0158e-02, -4.2667e-03,  4.9850e-02, -2.1929e-02],
        [-2.5046e-02, -5.0254e-02, -4.2749e-03,  4.9946e-02, -2.1971e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.5119,  3.0337,  0.2581, -3.0150,  1.3263]], device='cuda:0')
--> [loss] 329.7052001953125

---------------------------------- [[#3 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     1.0     |     1.0      |     3.0     | 0.6629 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.521948455544689 acc: 0.6002
[Epoch 7] loss: 3.6402679146708126 acc: 0.6487
[Epoch 11] loss: 2.159822895208283 acc: 0.6578
[Epoch 15] loss: 0.9789673854094332 acc: 0.6455
[Epoch 19] loss: 0.5372468724637233 acc: 0.6445
[Epoch 23] loss: 0.4120760449753774 acc: 0.6423
[Epoch 27] loss: 0.3488112513757194 acc: 0.6409
[Epoch 31] loss: 0.3099295468500737 acc: 0.6432
[Epoch 35] loss: 0.2842530496442295 acc: 0.6425
[Epoch 39] loss: 0.25898040504530645 acc: 0.6407
[Epoch 43] loss: 0.24176692149942489 acc: 0.6522
[Epoch 47] loss: 0.21658375647747913 acc: 0.6496
[Epoch 51] loss: 0.21817897884484827 acc: 0.6384
[Epoch 55] loss: 0.2003105598027863 acc: 0.6464
[Epoch 59] loss: 0.19746732497063782 acc: 0.6413
[Epoch 63] loss: 0.1862935369877178 acc: 0.634
[Epoch 67] loss: 0.17869473855270793 acc: 0.6501
[Epoch 71] loss: 0.17965955767647157 acc: 0.6479
--> [test] acc: 0.6528
--> [accuracy] finished 0.6528
new state: tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6528
--> [reward] 0.6528
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6528 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.0130323319483905 acc: 0.6412
[Epoch 7] loss: 2.8359540542373267 acc: 0.7058
[Epoch 11] loss: 1.2503094772243744 acc: 0.7174
[Epoch 15] loss: 0.5436319133881337 acc: 0.7052
[Epoch 19] loss: 0.34250442057019076 acc: 0.7124
[Epoch 23] loss: 0.274910304881156 acc: 0.7105
[Epoch 27] loss: 0.24193001648201548 acc: 0.6949
[Epoch 31] loss: 0.21697534811075614 acc: 0.7093
[Epoch 35] loss: 0.19280754544479234 acc: 0.7043
[Epoch 39] loss: 0.18353990920345345 acc: 0.7004
[Epoch 43] loss: 0.17631375861099308 acc: 0.7074
[Epoch 47] loss: 0.15076247188488923 acc: 0.7056
[Epoch 51] loss: 0.1556599063922644 acc: 0.7027
[Epoch 55] loss: 0.1424540339253338 acc: 0.7103
[Epoch 59] loss: 0.13143370052099304 acc: 0.6986
[Epoch 63] loss: 0.13324343598902683 acc: 0.7027
[Epoch 67] loss: 0.12599117837100746 acc: 0.7059
[Epoch 71] loss: 0.12402135025366874 acc: 0.6984
--> [test] acc: 0.6989
--> [accuracy] finished 0.6989
new state: tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.6989
--> [reward] 0.6989
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.6989 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.004454057387379 acc: 0.6572
[Epoch 7] loss: 2.8150209897314498 acc: 0.7127
[Epoch 11] loss: 1.2342554273469675 acc: 0.7199
[Epoch 15] loss: 0.5250857463035056 acc: 0.6992
[Epoch 19] loss: 0.3549493051317456 acc: 0.7132
[Epoch 23] loss: 0.27165197997170565 acc: 0.7082
[Epoch 27] loss: 0.2396555127637923 acc: 0.7044
[Epoch 31] loss: 0.21223388329062545 acc: 0.7179
[Epoch 35] loss: 0.19313516386586915 acc: 0.7102
[Epoch 39] loss: 0.17921922132229942 acc: 0.6967
[Epoch 43] loss: 0.17039800462458293 acc: 0.7044
[Epoch 47] loss: 0.16335112105309488 acc: 0.7121
[Epoch 51] loss: 0.14573045481291727 acc: 0.7159
[Epoch 55] loss: 0.15442247970548967 acc: 0.7049
[Epoch 59] loss: 0.1349402817437792 acc: 0.7094
[Epoch 63] loss: 0.12472912146712241 acc: 0.7048
[Epoch 67] loss: 0.128021556916206 acc: 0.7083
[Epoch 71] loss: 0.10878602004237235 acc: 0.7116
--> [test] acc: 0.714
--> [accuracy] finished 0.714
new state: tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.714
--> [reward] 0.714
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.714  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.957730488703989 acc: 0.6446
[Epoch 7] loss: 2.7893069250809264 acc: 0.712
[Epoch 11] loss: 1.2247523437813876 acc: 0.7142
[Epoch 15] loss: 0.5222012119086654 acc: 0.7078
[Epoch 19] loss: 0.33955713926726366 acc: 0.7092
[Epoch 23] loss: 0.2740705750330025 acc: 0.7042
[Epoch 27] loss: 0.24942239274358963 acc: 0.6979
[Epoch 31] loss: 0.19998898780892801 acc: 0.7097
[Epoch 35] loss: 0.19290544369431864 acc: 0.7024
[Epoch 39] loss: 0.18351810815794123 acc: 0.7072
[Epoch 43] loss: 0.15421840165059328 acc: 0.7022
[Epoch 47] loss: 0.1690172796706905 acc: 0.7097
[Epoch 51] loss: 0.15002496299140936 acc: 0.7085
[Epoch 55] loss: 0.13819947760180593 acc: 0.7073
[Epoch 59] loss: 0.14081654538724409 acc: 0.7028
[Epoch 63] loss: 0.11675518690108243 acc: 0.7065
[Epoch 67] loss: 0.12660847786758 acc: 0.7089
[Epoch 71] loss: 0.12324507908581202 acc: 0.7078
--> [test] acc: 0.7019
--> [accuracy] finished 0.7019
new state: tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7019
--> [reward] 0.7019
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7019 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([608.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.672346872137026 acc: 0.67
[Epoch 7] loss: 2.3180205004904275 acc: 0.7366
[Epoch 11] loss: 0.8336662697174665 acc: 0.743
[Epoch 15] loss: 0.3787389014183027 acc: 0.7371
[Epoch 19] loss: 0.2596218121640594 acc: 0.7492
[Epoch 23] loss: 0.22256043030525488 acc: 0.7433
[Epoch 27] loss: 0.19359655160447367 acc: 0.751
[Epoch 31] loss: 0.1696947810282964 acc: 0.7311
[Epoch 35] loss: 0.1638174305867661 acc: 0.7393
[Epoch 39] loss: 0.14395387956034153 acc: 0.7393
[Epoch 43] loss: 0.13708443121146172 acc: 0.7393
[Epoch 47] loss: 0.12561838426258023 acc: 0.744
[Epoch 51] loss: 0.11950078959007511 acc: 0.7398
[Epoch 55] loss: 0.10707222592190403 acc: 0.7381
[Epoch 59] loss: 0.11377799916116858 acc: 0.7463
[Epoch 63] loss: 0.11159615766334757 acc: 0.7338
[Epoch 67] loss: 0.09728336442560268 acc: 0.7452
[Epoch 71] loss: 0.09740179630468779 acc: 0.7376
--> [test] acc: 0.7321
--> [accuracy] finished 0.7321
new state: tensor([608.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7321
--> [reward] 0.7321
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7321 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.654637673626775 acc: 0.6566
[Epoch 7] loss: 2.301668480534078 acc: 0.7435
[Epoch 11] loss: 0.8096842126649283 acc: 0.7443
[Epoch 15] loss: 0.3730992476653565 acc: 0.7421
[Epoch 19] loss: 0.2589963396744388 acc: 0.7539
[Epoch 23] loss: 0.2225431352877594 acc: 0.7496
[Epoch 27] loss: 0.18952850281687267 acc: 0.7401
[Epoch 31] loss: 0.17109420207564902 acc: 0.7485
[Epoch 35] loss: 0.15780133561081136 acc: 0.7431
[Epoch 39] loss: 0.139179393531495 acc: 0.7453
[Epoch 43] loss: 0.13818887643077793 acc: 0.7428
[Epoch 47] loss: 0.12107428805688229 acc: 0.7493
[Epoch 51] loss: 0.117545000197666 acc: 0.7383
[Epoch 55] loss: 0.11409175764445377 acc: 0.7474
[Epoch 59] loss: 0.11453337348727248 acc: 0.7501
[Epoch 63] loss: 0.10238719860117411 acc: 0.741
[Epoch 67] loss: 0.09656580008642004 acc: 0.7403
[Epoch 71] loss: 0.10744226378542837 acc: 0.7468
--> [test] acc: 0.7416
--> [accuracy] finished 0.7416
new state: tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7416
--> [reward] 0.7416
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7416 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.665019155463295 acc: 0.696
[Epoch 7] loss: 2.2967145401422324 acc: 0.7391
[Epoch 11] loss: 0.8168628288532995 acc: 0.7455
[Epoch 15] loss: 0.37030728113458816 acc: 0.7412
[Epoch 19] loss: 0.26178826668949995 acc: 0.7292
[Epoch 23] loss: 0.21501785773269433 acc: 0.7383
[Epoch 27] loss: 0.19394555860115667 acc: 0.7439
[Epoch 31] loss: 0.17505903295515216 acc: 0.7474
[Epoch 35] loss: 0.14701537189104824 acc: 0.747
[Epoch 39] loss: 0.1442967622119295 acc: 0.7415
[Epoch 43] loss: 0.1301902039381473 acc: 0.7426
[Epoch 47] loss: 0.13008389707885162 acc: 0.7448
[Epoch 51] loss: 0.12220334683609245 acc: 0.7368
[Epoch 55] loss: 0.11240112538570943 acc: 0.739
[Epoch 59] loss: 0.10359654853434856 acc: 0.736
[Epoch 63] loss: 0.09681324234298999 acc: 0.7418
[Epoch 67] loss: 0.1071858283244264 acc: 0.7354
[Epoch 71] loss: 0.08610920602565303 acc: 0.738
--> [test] acc: 0.7384
--> [accuracy] finished 0.7384
new state: tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7384
--> [reward] 0.7384
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7384 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.672720519782942 acc: 0.6874
[Epoch 7] loss: 2.317293959429197 acc: 0.7443
[Epoch 11] loss: 0.8605074636981158 acc: 0.7418
[Epoch 15] loss: 0.3782447024970256 acc: 0.7386
[Epoch 19] loss: 0.280488516608982 acc: 0.7387
[Epoch 23] loss: 0.22046780307798663 acc: 0.7422
[Epoch 27] loss: 0.1903103540634827 acc: 0.7458
[Epoch 31] loss: 0.18402644532644535 acc: 0.7397
[Epoch 35] loss: 0.15561994294995618 acc: 0.7394
[Epoch 39] loss: 0.14901669046191304 acc: 0.7391
[Epoch 43] loss: 0.1383569698502093 acc: 0.7415
[Epoch 47] loss: 0.12935745011320782 acc: 0.7419
[Epoch 51] loss: 0.11714446999947242 acc: 0.7293
[Epoch 55] loss: 0.12408723924761576 acc: 0.7392
[Epoch 59] loss: 0.10547335101517817 acc: 0.7506
[Epoch 63] loss: 0.10394282689642 acc: 0.7411
[Epoch 67] loss: 0.10127215653263709 acc: 0.7381
[Epoch 71] loss: 0.0959822306626231 acc: 0.7368
--> [test] acc: 0.7416
--> [accuracy] finished 0.7416
new state: tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7416
--> [reward] 0.7416
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7416 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.539909053946395 acc: 0.7017
[Epoch 7] loss: 2.0726000513795695 acc: 0.7649
[Epoch 11] loss: 0.6889556723425303 acc: 0.7494
[Epoch 15] loss: 0.3280438023269215 acc: 0.7541
[Epoch 19] loss: 0.23631865516200165 acc: 0.7504
[Epoch 23] loss: 0.19680640240773903 acc: 0.7403
[Epoch 27] loss: 0.16999343749912232 acc: 0.7481
[Epoch 31] loss: 0.15093094333672843 acc: 0.749
[Epoch 35] loss: 0.14177367768352828 acc: 0.7474
[Epoch 39] loss: 0.124046939364432 acc: 0.7519
[Epoch 43] loss: 0.11625531222552175 acc: 0.7489
[Epoch 47] loss: 0.11981942020523388 acc: 0.7521
[Epoch 51] loss: 0.10454754439442207 acc: 0.7453
[Epoch 55] loss: 0.10611570442778409 acc: 0.7561
[Epoch 59] loss: 0.09706284848096616 acc: 0.7554
[Epoch 63] loss: 0.0940254906709294 acc: 0.752
[Epoch 67] loss: 0.08368738619443934 acc: 0.7501
[Epoch 71] loss: 0.0856099115429449 acc: 0.7535
--> [test] acc: 0.7521
--> [accuracy] finished 0.7521
new state: tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
new reward: 0.7521
--> [reward] 0.7521
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7521 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.502642883364197 acc: 0.7007
[Epoch 7] loss: 2.106293220959051 acc: 0.7439
[Epoch 11] loss: 0.6900882796358193 acc: 0.755
[Epoch 15] loss: 0.31799164028538157 acc: 0.7578
[Epoch 19] loss: 0.23779706817472834 acc: 0.757
[Epoch 23] loss: 0.1953345190524064 acc: 0.7586
[Epoch 27] loss: 0.16529205834964658 acc: 0.7575
[Epoch 31] loss: 0.15753345282229086 acc: 0.7563
[Epoch 35] loss: 0.1367013333133086 acc: 0.7649
[Epoch 39] loss: 0.12892251445071015 acc: 0.7556
[Epoch 43] loss: 0.12205805018891955 acc: 0.7487
[Epoch 47] loss: 0.11264657920054957 acc: 0.7408
[Epoch 51] loss: 0.11288581510572254 acc: 0.7498
[Epoch 55] loss: 0.10140835196140063 acc: 0.7542
[Epoch 59] loss: 0.09363704356525684 acc: 0.7589
[Epoch 63] loss: 0.09035365099249326 acc: 0.7598
[Epoch 67] loss: 0.08282440757025224 acc: 0.7605
[Epoch 71] loss: 0.09691879777428325 acc: 0.747
--> [test] acc: 0.7607
--> [accuracy] finished 0.7607
new state: tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
new reward: 0.7607
--> [reward] 0.7607
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.665154112879272 acc: 0.6724
[Epoch 7] loss: 2.3240905243646153 acc: 0.7543
[Epoch 11] loss: 0.8701558314797366 acc: 0.7481
[Epoch 15] loss: 0.3901414486539105 acc: 0.7404
[Epoch 19] loss: 0.26889269512689784 acc: 0.7455
[Epoch 23] loss: 0.2201125374197236 acc: 0.7398
[Epoch 27] loss: 0.19434487626023228 acc: 0.7442
[Epoch 31] loss: 0.17077291899782313 acc: 0.7439
[Epoch 35] loss: 0.1611812848030873 acc: 0.737
[Epoch 39] loss: 0.1462932635423825 acc: 0.7375
[Epoch 43] loss: 0.13112277917616316 acc: 0.743
[Epoch 47] loss: 0.12980607169611222 acc: 0.7431
[Epoch 51] loss: 0.10956846854156431 acc: 0.7441
[Epoch 55] loss: 0.12641228320996475 acc: 0.7482
[Epoch 59] loss: 0.11364374749834084 acc: 0.7444
[Epoch 63] loss: 0.1039429113418912 acc: 0.7363
[Epoch 67] loss: 0.10303771510348676 acc: 0.7344
[Epoch 71] loss: 0.09921462755755085 acc: 0.7417
--> [test] acc: 0.7473
--> [accuracy] finished 0.7473
new state: tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7473
--> [reward] 0.7473
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7473 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.734942460761351 acc: 0.6919
[Epoch 7] loss: 2.4525944467853096 acc: 0.753
[Epoch 11] loss: 1.0070814128936556 acc: 0.7513
[Epoch 15] loss: 0.4512679431196826 acc: 0.75
[Epoch 19] loss: 0.3003807630785324 acc: 0.7573
[Epoch 23] loss: 0.2500671328133558 acc: 0.7466
[Epoch 27] loss: 0.20659135708399592 acc: 0.7433
[Epoch 31] loss: 0.19620044540156564 acc: 0.7464
[Epoch 35] loss: 0.17585841322417758 acc: 0.748
[Epoch 39] loss: 0.17476891833798164 acc: 0.7528
[Epoch 43] loss: 0.14471544471838513 acc: 0.7407
[Epoch 47] loss: 0.1476028966717422 acc: 0.7497
[Epoch 51] loss: 0.13224045398181825 acc: 0.7459
[Epoch 55] loss: 0.1315421001494998 acc: 0.7459
[Epoch 59] loss: 0.1330236594866761 acc: 0.7426
[Epoch 63] loss: 0.11637019868249364 acc: 0.7413
[Epoch 67] loss: 0.11964346711461901 acc: 0.7402
[Epoch 71] loss: 0.11508736976400337 acc: 0.7523
--> [test] acc: 0.7465
--> [accuracy] finished 0.7465
new state: tensor([672.,   1.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7465
--> [reward] 0.7465
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     3.0     |     1.0      |     4.0     | 0.7465 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0905,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.715828740993119 acc: 0.6719
[Epoch 7] loss: 2.424448917284036 acc: 0.7541
[Epoch 11] loss: 0.9932670096397552 acc: 0.7483
[Epoch 15] loss: 0.44268471395592096 acc: 0.7465
[Epoch 19] loss: 0.300002017982609 acc: 0.7481
[Epoch 23] loss: 0.2456215713458979 acc: 0.7452
[Epoch 27] loss: 0.2115586141024328 acc: 0.7515
[Epoch 31] loss: 0.18963468453282362 acc: 0.7504
[Epoch 35] loss: 0.1745425784112433 acc: 0.7433
[Epoch 39] loss: 0.1629233728544048 acc: 0.7418
[Epoch 43] loss: 0.15801435195462174 acc: 0.7441
[Epoch 47] loss: 0.14891230063501013 acc: 0.7463
[Epoch 51] loss: 0.1312191859669233 acc: 0.7498
[Epoch 55] loss: 0.12003143595161198 acc: 0.7417
[Epoch 59] loss: 0.12374550958766657 acc: 0.7377
[Epoch 63] loss: 0.12054485613551667 acc: 0.7341
[Epoch 67] loss: 0.11244536042058617 acc: 0.7473
[Epoch 71] loss: 0.10700354360334595 acc: 0.7441
--> [test] acc: 0.7468
--> [accuracy] finished 0.7468
new state: tensor([704.,   1.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7468
--> [reward] 0.7468
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     1.0      |     4.0     | 0.7468 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0905,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([704.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.655573274016075 acc: 0.6612
[Epoch 7] loss: 2.310354094028168 acc: 0.7474
[Epoch 11] loss: 0.837683859774295 acc: 0.7407
[Epoch 15] loss: 0.38233452686168196 acc: 0.7277
[Epoch 19] loss: 0.2741195338913966 acc: 0.7458
[Epoch 23] loss: 0.2223128797093411 acc: 0.7459
[Epoch 27] loss: 0.192829892724095 acc: 0.7431
[Epoch 31] loss: 0.17656475518975417 acc: 0.743
[Epoch 35] loss: 0.1446689342856026 acc: 0.7388
[Epoch 39] loss: 0.15527313072091478 acc: 0.7414
[Epoch 43] loss: 0.12818295253759912 acc: 0.7379
[Epoch 47] loss: 0.12881241356977796 acc: 0.7388
[Epoch 51] loss: 0.11743162715059641 acc: 0.734
[Epoch 55] loss: 0.1196752471785011 acc: 0.7333
[Epoch 59] loss: 0.10975729485156724 acc: 0.7351
[Epoch 63] loss: 0.10781770288143926 acc: 0.7451
[Epoch 67] loss: 0.1060556575329617 acc: 0.7366
[Epoch 71] loss: 0.0886185622197645 acc: 0.7373
--> [test] acc: 0.7456
--> [accuracy] finished 0.7456
new state: tensor([704.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7456
--> [reward] 0.7456
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7456 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0905,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.659922680434058 acc: 0.6652
[Epoch 7] loss: 2.301603253387734 acc: 0.7395
[Epoch 11] loss: 0.8627922878412487 acc: 0.7427
[Epoch 15] loss: 0.37950683431342586 acc: 0.7455
[Epoch 19] loss: 0.26888207968353006 acc: 0.7415
[Epoch 23] loss: 0.21836006813837439 acc: 0.7435
[Epoch 27] loss: 0.18631580278939566 acc: 0.7423
[Epoch 31] loss: 0.18527808871186907 acc: 0.7464
[Epoch 35] loss: 0.15465080692811542 acc: 0.7422
[Epoch 39] loss: 0.14049228525641935 acc: 0.7449
[Epoch 43] loss: 0.13486187802413313 acc: 0.742
[Epoch 47] loss: 0.12986045667325216 acc: 0.7296
[Epoch 51] loss: 0.12528395408745427 acc: 0.7377
[Epoch 55] loss: 0.10595722799487126 acc: 0.7444
[Epoch 59] loss: 0.11700785272252148 acc: 0.7377
[Epoch 63] loss: 0.0989214376959702 acc: 0.7291
[Epoch 67] loss: 0.10428828472102447 acc: 0.7353
[Epoch 71] loss: 0.09127599333607665 acc: 0.7407
--> [test] acc: 0.7319
--> [accuracy] finished 0.7319
new state: tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7319
--> [reward] 0.7319
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2681]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5362]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7323]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7301]], device='cuda:0')
------ ------
delta_t: tensor([[0.7323]], device='cuda:0')
rewards[i]: 0.7319
values[i+1]: tensor([[-0.0019]], device='cuda:0')
values[i]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7323]], device='cuda:0')
delta_t: tensor([[0.7323]], device='cuda:0')
------ ------
policy_loss: 1.7318576574325562
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7323]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.3495]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.1628]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4706]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4684]], device='cuda:0')
------ ------
delta_t: tensor([[0.7457]], device='cuda:0')
rewards[i]: 0.7456
values[i+1]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0023]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4706]], device='cuda:0')
delta_t: tensor([[0.7457]], device='cuda:0')
------ ------
policy_loss: 5.23491096496582
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4706]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.7751]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.8512]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.2025]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.2005]], device='cuda:0')
------ ------
delta_t: tensor([[0.7466]], device='cuda:0')
rewards[i]: 0.7468
values[i+1]: tensor([[-0.0023]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.2025]], device='cuda:0')
delta_t: tensor([[0.7466]], device='cuda:0')
------ ------
policy_loss: 10.491554260253906
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.2025]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[8.0586]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.5670]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9270]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.9250]], device='cuda:0')
------ ------
delta_t: tensor([[0.7464]], device='cuda:0')
rewards[i]: 0.7465
values[i+1]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0020]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9269]], device='cuda:0')
delta_t: tensor([[0.7464]], device='cuda:0')
------ ------
policy_loss: 17.486297607421875
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9269]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[14.7023]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.2874]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6452]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6430]], device='cuda:0')
------ ------
delta_t: tensor([[0.7475]], device='cuda:0')
rewards[i]: 0.7473
values[i+1]: tensor([[-0.0020]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6452]], device='cuda:0')
delta_t: tensor([[0.7475]], device='cuda:0')
------ ------
policy_loss: 26.20235824584961
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6452]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[24.2479]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.0913]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3694]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3673]], device='cuda:0')
------ ------
delta_t: tensor([[0.7606]], device='cuda:0')
rewards[i]: 0.7607
values[i+1]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3694]], device='cuda:0')
delta_t: tensor([[0.7606]], device='cuda:0')
------ ------
policy_loss: 36.655704498291016
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3694]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[37.1386]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[25.7813]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.0775]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0757]], device='cuda:0')
------ ------
delta_t: tensor([[0.7519]], device='cuda:0')
rewards[i]: 0.7521
values[i+1]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0018]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.0775]], device='cuda:0')
delta_t: tensor([[0.7519]], device='cuda:0')
------ ------
policy_loss: 48.80762481689453
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.0775]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[53.7738]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[33.2705]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.7681]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.7666]], device='cuda:0')
------ ------
delta_t: tensor([[0.7413]], device='cuda:0')
rewards[i]: 0.7416
values[i+1]: tensor([[-0.0018]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0015]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.7681]], device='cuda:0')
delta_t: tensor([[0.7413]], device='cuda:0')
------ ------
policy_loss: 62.61493682861328
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.7681]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[74.5635]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[41.5795]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.4482]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4473]], device='cuda:0')
------ ------
delta_t: tensor([[0.7378]], device='cuda:0')
rewards[i]: 0.7384
values[i+1]: tensor([[-0.0015]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0009]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.4482]], device='cuda:0')
delta_t: tensor([[0.7378]], device='cuda:0')
------ ------
policy_loss: 78.05069732666016
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.4482]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[99.9455]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[50.7640]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.1249]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1244]], device='cuda:0')
------ ------
delta_t: tensor([[0.7411]], device='cuda:0')
rewards[i]: 0.7416
values[i+1]: tensor([[-0.0009]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.1249]], device='cuda:0')
delta_t: tensor([[0.7411]], device='cuda:0')
------ ------
policy_loss: 95.1087875366211
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.1249]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[130.2527]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[60.6144]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.7855]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7853]], device='cuda:0')
------ ------
delta_t: tensor([[0.7319]], device='cuda:0')
rewards[i]: 0.7321
values[i+1]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0002]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.7855]], device='cuda:0')
delta_t: tensor([[0.7319]], device='cuda:0')
------ ------
policy_loss: 113.75447845458984
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.7855]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[165.6149]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[70.7244]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.4098]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.4093]], device='cuda:0')
------ ------
delta_t: tensor([[0.7021]], device='cuda:0')
rewards[i]: 0.7019
values[i+1]: tensor([[-0.0002]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.4098]], device='cuda:0')
delta_t: tensor([[0.7021]], device='cuda:0')
------ ------
policy_loss: 133.8955078125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.4098]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[206.4760]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[81.7222]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.0400]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.0392]], device='cuda:0')
------ ------
delta_t: tensor([[0.7143]], device='cuda:0')
rewards[i]: 0.714
values[i+1]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0008]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.0400]], device='cuda:0')
delta_t: tensor([[0.7143]], device='cuda:0')
------ ------
policy_loss: 155.5479736328125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.0400]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[253.0255]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[93.0990]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.6488]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.6477]], device='cuda:0')
------ ------
delta_t: tensor([[0.6992]], device='cuda:0')
rewards[i]: 0.6989
values[i+1]: tensor([[-0.0008]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0010]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.6488]], device='cuda:0')
delta_t: tensor([[0.6992]], device='cuda:0')
------ ------
policy_loss: 178.66175842285156
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.6488]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[305.1060]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[104.1610]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[10.2059]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[10.2041]], device='cuda:0')
------ ------
delta_t: tensor([[0.6536]], device='cuda:0')
rewards[i]: 0.6528
values[i+1]: tensor([[-0.0010]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0019]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[10.2059]], device='cuda:0')
delta_t: tensor([[0.6536]], device='cuda:0')
------ ------
policy_loss: 203.11123657226562
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[10.2059]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 203.11123657226562
value_loss: 305.1059875488281
loss: 355.66424560546875



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 6.2207e-04,  3.0747e-07,  2.8852e-06,  8.7383e-07,  2.5999e-06],
        [-2.6490e-02, -5.8229e-05, -1.0412e-04, -3.9770e-05, -1.2508e-04],
        [-3.9860e-04, -1.6457e-06, -1.3570e-06, -6.4674e-07, -2.0888e-06],
        [-7.0120e-02, -1.3241e-04, -2.7670e-04, -1.0356e-04, -3.2781e-04],
        [-2.0381e-01, -2.4819e-04, -9.4355e-04, -2.9188e-04, -9.4873e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-4.3316e-06, -6.8747e-06, -2.6570e-07,  7.0277e-06, -3.8744e-06],
        [ 1.7374e-04,  3.0357e-04,  1.8850e-05, -3.0739e-04,  1.5449e-04],
        [ 2.4746e-06,  4.7885e-06,  4.0084e-07, -4.8039e-06,  2.1776e-06],
        [ 4.6076e-04,  7.9531e-04,  4.7366e-05, -8.0629e-04,  4.1088e-04],
        [ 1.3414e-03,  2.2504e-03,  1.1208e-04, -2.2856e-03,  1.2059e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.0724,  0.0702, -0.0722,  0.0613, -0.0395],
        [ 0.0820, -0.0795,  0.0818, -0.0690,  0.0447],
        [-0.0112,  0.0108, -0.0112,  0.0092, -0.0061],
        [ 0.0047, -0.0045,  0.0047, -0.0039,  0.0025],
        [-0.0030,  0.0029, -0.0030,  0.0024, -0.0017]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 8.5068e-03,  1.4618e-02,  1.2382e-03, -1.4929e-02,  7.1208e-03],
        [ 2.8111e-02,  5.2068e-02,  3.0028e-03, -5.2327e-02,  2.5589e-02],
        [ 2.0943e-02,  3.9020e-02,  3.6286e-03, -3.9242e-02,  1.7908e-02],
        [-2.2877e-02, -4.1886e-02, -2.9288e-03,  4.2171e-02, -2.0175e-02],
        [-1.5208e-02, -2.7187e-02, -1.9897e-03,  2.7517e-02, -1.3209e-02],
        [ 5.5296e-02,  1.0043e-01,  7.2251e-03, -1.0125e-01,  4.8535e-02],
        [ 2.5425e-03,  3.6521e-03,  7.5465e-05, -3.8389e-03,  1.9440e-03],
        [-2.8630e-02, -5.2673e-02, -3.7634e-03,  5.3021e-02, -2.5388e-02],
        [-2.8488e-02, -5.2412e-02, -3.7447e-03,  5.2758e-02, -2.5262e-02],
        [ 8.4690e-03,  1.7111e-02,  1.0246e-03, -1.6964e-02,  8.3574e-03],
        [-2.8665e-02, -5.2738e-02, -3.7681e-03,  5.3086e-02, -2.5420e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.7301,  3.1830,  0.2274, -3.2040,  1.5342]], device='cuda:0')
--> [loss] 355.66424560546875

---------------------------------- [[#4 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7319 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.690732022685468 acc: 0.6798
[Epoch 7] loss: 2.347874245832643 acc: 0.7259
[Epoch 11] loss: 0.8456142932710136 acc: 0.7497
[Epoch 15] loss: 0.371260959902764 acc: 0.742
[Epoch 19] loss: 0.2774812088269369 acc: 0.7328
[Epoch 23] loss: 0.2193584046719591 acc: 0.7354
[Epoch 27] loss: 0.1967791675225548 acc: 0.7356
[Epoch 31] loss: 0.1772747493975455 acc: 0.749
[Epoch 35] loss: 0.15221690258089823 acc: 0.7414
[Epoch 39] loss: 0.14892706933105007 acc: 0.7372
[Epoch 43] loss: 0.1350481877522662 acc: 0.7326
[Epoch 47] loss: 0.1307537631561046 acc: 0.7396
[Epoch 51] loss: 0.12038449545872792 acc: 0.7339
[Epoch 55] loss: 0.11123338461284289 acc: 0.7408
[Epoch 59] loss: 0.1103945505021371 acc: 0.7279
[Epoch 63] loss: 0.10810164468275989 acc: 0.7369
[Epoch 67] loss: 0.09613600863909583 acc: 0.7374
[Epoch 71] loss: 0.10432001755482344 acc: 0.7415
--> [test] acc: 0.7286
--> [accuracy] finished 0.7286
new state: tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7286
--> [reward] 0.7286
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7286 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.642811306297322 acc: 0.6779
[Epoch 7] loss: 2.3145444161446806 acc: 0.7462
[Epoch 11] loss: 0.8421199244859121 acc: 0.7356
[Epoch 15] loss: 0.3676533185021804 acc: 0.7477
[Epoch 19] loss: 0.26607966582860104 acc: 0.7398
[Epoch 23] loss: 0.22167059545502868 acc: 0.7372
[Epoch 27] loss: 0.1871532948039796 acc: 0.7439
[Epoch 31] loss: 0.17426310932439992 acc: 0.7416
[Epoch 35] loss: 0.15468126093335163 acc: 0.7402
[Epoch 39] loss: 0.13900491304795645 acc: 0.7348
[Epoch 43] loss: 0.1414421207278662 acc: 0.7455
[Epoch 47] loss: 0.11568707299402074 acc: 0.7299
[Epoch 51] loss: 0.125131837116159 acc: 0.7418
[Epoch 55] loss: 0.11776453491105028 acc: 0.7356
[Epoch 59] loss: 0.11569564716766595 acc: 0.7473
[Epoch 63] loss: 0.10580759256592263 acc: 0.7479
[Epoch 67] loss: 0.0970734471724371 acc: 0.7308
[Epoch 71] loss: 0.09208631376042734 acc: 0.7343
--> [test] acc: 0.7412
--> [accuracy] finished 0.7412
new state: tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7412
--> [reward] 0.7412
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7412 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.712706317224771 acc: 0.694
[Epoch 7] loss: 2.3515597125682075 acc: 0.7454
[Epoch 11] loss: 0.8538829710124932 acc: 0.7496
[Epoch 15] loss: 0.384428054651679 acc: 0.7454
[Epoch 19] loss: 0.26882700843122 acc: 0.7485
[Epoch 23] loss: 0.20738069219764826 acc: 0.7481
[Epoch 27] loss: 0.19424067856148458 acc: 0.7424
[Epoch 31] loss: 0.1738099316206506 acc: 0.738
[Epoch 35] loss: 0.1644489611677654 acc: 0.7428
[Epoch 39] loss: 0.14078103411047127 acc: 0.7448
[Epoch 43] loss: 0.13320647610846878 acc: 0.7331
[Epoch 47] loss: 0.1186469659927394 acc: 0.7405
[Epoch 51] loss: 0.1265214135551163 acc: 0.7321
[Epoch 55] loss: 0.11309113958850503 acc: 0.7343
[Epoch 59] loss: 0.11643302476937499 acc: 0.7422
[Epoch 63] loss: 0.10820208299372708 acc: 0.7361
[Epoch 67] loss: 0.09160489680917214 acc: 0.7394
[Epoch 71] loss: 0.09136839571070698 acc: 0.7391
--> [test] acc: 0.7376
--> [accuracy] finished 0.7376
new state: tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7376
--> [reward] 0.7376
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7376 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.702549916703988 acc: 0.6722
[Epoch 7] loss: 2.3314274650095674 acc: 0.7375
[Epoch 11] loss: 0.8488458632860724 acc: 0.7469
[Epoch 15] loss: 0.37954136222134083 acc: 0.739
[Epoch 19] loss: 0.2639307199722475 acc: 0.7466
[Epoch 23] loss: 0.22180964483800905 acc: 0.742
[Epoch 27] loss: 0.20389899311830167 acc: 0.7407
[Epoch 31] loss: 0.16946409784300287 acc: 0.7363
[Epoch 35] loss: 0.15000958812406376 acc: 0.7327
[Epoch 39] loss: 0.14786060386554092 acc: 0.7398
[Epoch 43] loss: 0.13815574199342362 acc: 0.734
[Epoch 47] loss: 0.12066845543434858 acc: 0.7408
[Epoch 51] loss: 0.12296610865313226 acc: 0.7405
[Epoch 55] loss: 0.12295269132937159 acc: 0.7345
[Epoch 59] loss: 0.10659880131808207 acc: 0.7228
[Epoch 63] loss: 0.09738021112961785 acc: 0.737
[Epoch 67] loss: 0.1030060446444813 acc: 0.7482
[Epoch 71] loss: 0.09837262243832774 acc: 0.7362
--> [test] acc: 0.7372
--> [accuracy] finished 0.7372
new state: tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7372
--> [reward] 0.7372
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7372 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.010525335588723 acc: 0.6433
[Epoch 7] loss: 2.7938359459037976 acc: 0.7144
[Epoch 11] loss: 1.2251671639740314 acc: 0.7153
[Epoch 15] loss: 0.5093101391001887 acc: 0.7117
[Epoch 19] loss: 0.3431173663067124 acc: 0.7161
[Epoch 23] loss: 0.27733794821526314 acc: 0.7073
[Epoch 27] loss: 0.22202529663177173 acc: 0.7107
[Epoch 31] loss: 0.21234955755121948 acc: 0.7117
[Epoch 35] loss: 0.2017810773021063 acc: 0.7083
[Epoch 39] loss: 0.17003659890009487 acc: 0.7019
[Epoch 43] loss: 0.16774731011861158 acc: 0.7032
[Epoch 47] loss: 0.15625461658505757 acc: 0.7052
[Epoch 51] loss: 0.14826830382199238 acc: 0.7126
[Epoch 55] loss: 0.14006435204549786 acc: 0.7081
[Epoch 59] loss: 0.11933450768356357 acc: 0.7133
[Epoch 63] loss: 0.1376034629571697 acc: 0.7109
[Epoch 67] loss: 0.11657909046301185 acc: 0.6874
[Epoch 71] loss: 0.12014064994459624 acc: 0.7064
--> [test] acc: 0.7036
--> [accuracy] finished 0.7036
new state: tensor([672.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7036
--> [reward] 0.7036
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7036 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.95356953098341 acc: 0.6222
[Epoch 7] loss: 2.83975276937875 acc: 0.7177
[Epoch 11] loss: 1.3672391434802729 acc: 0.722
[Epoch 15] loss: 0.59356468810183 acc: 0.7053
[Epoch 19] loss: 0.3644509961366501 acc: 0.7175
[Epoch 23] loss: 0.30449570763779954 acc: 0.7117
[Epoch 27] loss: 0.2535348367160353 acc: 0.7152
[Epoch 31] loss: 0.23637206787410217 acc: 0.7139
[Epoch 35] loss: 0.2108156555058325 acc: 0.7049
[Epoch 39] loss: 0.1929962528557481 acc: 0.715
[Epoch 43] loss: 0.18798277998471732 acc: 0.7112
[Epoch 47] loss: 0.17736710696254887 acc: 0.7197
[Epoch 51] loss: 0.15994467363094964 acc: 0.7118
[Epoch 55] loss: 0.15886972870444283 acc: 0.7129
[Epoch 59] loss: 0.15392808446748768 acc: 0.7054
[Epoch 63] loss: 0.14104011661344495 acc: 0.712
[Epoch 67] loss: 0.13566825234109675 acc: 0.7078
[Epoch 71] loss: 0.12964202332570304 acc: 0.709
--> [test] acc: 0.7158
--> [accuracy] finished 0.7158
new state: tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7158
--> [reward] 0.7158
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7158 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.043360188184187 acc: 0.6429
[Epoch 7] loss: 2.913709549952651 acc: 0.7192
[Epoch 11] loss: 1.4240134492173524 acc: 0.7249
[Epoch 15] loss: 0.6136903813123094 acc: 0.7119
[Epoch 19] loss: 0.3910135810842256 acc: 0.709
[Epoch 23] loss: 0.3126743905832205 acc: 0.7089
[Epoch 27] loss: 0.26033268141491184 acc: 0.7035
[Epoch 31] loss: 0.23721554958263932 acc: 0.7107
[Epoch 35] loss: 0.21339803193564838 acc: 0.7085
[Epoch 39] loss: 0.20424049999207602 acc: 0.6979
[Epoch 43] loss: 0.1910912750879555 acc: 0.7058
[Epoch 47] loss: 0.1816828474676346 acc: 0.7085
[Epoch 51] loss: 0.16439436768572255 acc: 0.7068
[Epoch 55] loss: 0.17196317988059595 acc: 0.7093
[Epoch 59] loss: 0.15464988837966606 acc: 0.7064
[Epoch 63] loss: 0.14006023898082393 acc: 0.706
[Epoch 67] loss: 0.13987894114785734 acc: 0.7009
[Epoch 71] loss: 0.1496221312593259 acc: 0.7069
--> [test] acc: 0.7125
--> [accuracy] finished 0.7125
new state: tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7125
--> [reward] 0.7125
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7125 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([640.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.002170142920121 acc: 0.6272
[Epoch 7] loss: 2.866017287573241 acc: 0.71
[Epoch 11] loss: 1.4115707342658201 acc: 0.7232
[Epoch 15] loss: 0.62339413249889 acc: 0.7072
[Epoch 19] loss: 0.38207268365718366 acc: 0.7098
[Epoch 23] loss: 0.30763000807703456 acc: 0.6968
[Epoch 27] loss: 0.27641080519246397 acc: 0.7154
[Epoch 31] loss: 0.23395294753258186 acc: 0.7172
[Epoch 35] loss: 0.22304952077691437 acc: 0.6973
[Epoch 39] loss: 0.2055718441734381 acc: 0.7141
[Epoch 43] loss: 0.18034380354473126 acc: 0.7062
[Epoch 47] loss: 0.1763024034462107 acc: 0.7035
[Epoch 51] loss: 0.18274700906737457 acc: 0.706
[Epoch 55] loss: 0.16338355320355738 acc: 0.7092
[Epoch 59] loss: 0.14687287639540708 acc: 0.7075
[Epoch 63] loss: 0.14888742191912344 acc: 0.7103
[Epoch 67] loss: 0.13436137587356065 acc: 0.7125
[Epoch 71] loss: 0.14020907446441938 acc: 0.7011
--> [test] acc: 0.7083
--> [accuracy] finished 0.7083
new state: tensor([640.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7083
--> [reward] 0.7083
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7083 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.046630183144298 acc: 0.6493
[Epoch 7] loss: 2.928027847005278 acc: 0.7232
[Epoch 11] loss: 1.4823036705479598 acc: 0.7221
[Epoch 15] loss: 0.660242694884996 acc: 0.7183
[Epoch 19] loss: 0.39568860052853744 acc: 0.7038
[Epoch 23] loss: 0.31922819941302244 acc: 0.7114
[Epoch 27] loss: 0.256468353273771 acc: 0.6999
[Epoch 31] loss: 0.25283805352380817 acc: 0.7095
[Epoch 35] loss: 0.22073129973972164 acc: 0.7063
[Epoch 39] loss: 0.2002074165183984 acc: 0.6989
[Epoch 43] loss: 0.19245492626705665 acc: 0.6967
[Epoch 47] loss: 0.17987188151406358 acc: 0.7052
[Epoch 51] loss: 0.17250947379495216 acc: 0.7078
[Epoch 55] loss: 0.1607824995056214 acc: 0.706
[Epoch 59] loss: 0.15180359024446113 acc: 0.7154
[Epoch 63] loss: 0.1577177304724503 acc: 0.706
[Epoch 67] loss: 0.1342493475121839 acc: 0.7118
[Epoch 71] loss: 0.13775336643790498 acc: 0.7188
--> [test] acc: 0.7192
--> [accuracy] finished 0.7192
new state: tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7192
--> [reward] 0.7192
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7192 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.037982940064062 acc: 0.6515
[Epoch 7] loss: 2.9101187599741896 acc: 0.7172
[Epoch 11] loss: 1.4687374603108068 acc: 0.7234
[Epoch 15] loss: 0.634318943923849 acc: 0.7078
[Epoch 19] loss: 0.3927925794868899 acc: 0.7105
[Epoch 23] loss: 0.30992098532312207 acc: 0.7084
[Epoch 27] loss: 0.26990515302123547 acc: 0.7116
[Epoch 31] loss: 0.24306390580513973 acc: 0.7088
[Epoch 35] loss: 0.21466257789498552 acc: 0.7052
[Epoch 39] loss: 0.21387923384071006 acc: 0.7043
[Epoch 43] loss: 0.1928454030071721 acc: 0.7117
[Epoch 47] loss: 0.17339675948071434 acc: 0.7073
[Epoch 51] loss: 0.1712778107920552 acc: 0.7132
[Epoch 55] loss: 0.16294215622005503 acc: 0.7023
[Epoch 59] loss: 0.16400808145356416 acc: 0.7166
[Epoch 63] loss: 0.14646219943895403 acc: 0.706
[Epoch 67] loss: 0.13873166391861808 acc: 0.7128
[Epoch 71] loss: 0.13965294662091282 acc: 0.6985
--> [test] acc: 0.7075
--> [accuracy] finished 0.7075
new state: tensor([576.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7075
--> [reward] 0.7075
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7075 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.033843046716412 acc: 0.6476
[Epoch 7] loss: 2.8799588059068033 acc: 0.7041
[Epoch 11] loss: 1.4124729855705405 acc: 0.7258
[Epoch 15] loss: 0.6198955716855843 acc: 0.7174
[Epoch 19] loss: 0.38161892080893905 acc: 0.7142
[Epoch 23] loss: 0.31743584936746705 acc: 0.7163
[Epoch 27] loss: 0.2634655866924378 acc: 0.7147
[Epoch 31] loss: 0.239525489218514 acc: 0.7069
[Epoch 35] loss: 0.21486120081156532 acc: 0.7027
[Epoch 39] loss: 0.21478696118759186 acc: 0.7128
[Epoch 43] loss: 0.1899709386531921 acc: 0.7167
[Epoch 47] loss: 0.18134960240643958 acc: 0.7066
[Epoch 51] loss: 0.16731854485909994 acc: 0.7141
[Epoch 55] loss: 0.1615856013348912 acc: 0.7025
[Epoch 59] loss: 0.1571663061056829 acc: 0.7066
[Epoch 63] loss: 0.15118450224768762 acc: 0.7058
[Epoch 67] loss: 0.14344432008216906 acc: 0.72
[Epoch 71] loss: 0.13501263903977964 acc: 0.7094
--> [test] acc: 0.7058
--> [accuracy] finished 0.7058
new state: tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7058
--> [reward] 0.7058
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7058 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([608.,   1.,   2.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.584577994578329 acc: 0.5665
[Epoch 7] loss: 3.554325601405195 acc: 0.6678
[Epoch 11] loss: 2.157847829906227 acc: 0.6678
[Epoch 15] loss: 1.0932768844925533 acc: 0.6648
[Epoch 19] loss: 0.6064162578986353 acc: 0.6624
[Epoch 23] loss: 0.4475471344835999 acc: 0.6648
[Epoch 27] loss: 0.35509656213433544 acc: 0.658
[Epoch 31] loss: 0.3236684671167256 acc: 0.6629
[Epoch 35] loss: 0.29135187092901726 acc: 0.6591
[Epoch 39] loss: 0.26954896495108255 acc: 0.6618
[Epoch 43] loss: 0.2501781047833964 acc: 0.6608
[Epoch 47] loss: 0.22433649846936202 acc: 0.6648
[Epoch 51] loss: 0.22669170741849315 acc: 0.6531
[Epoch 55] loss: 0.21469113617763877 acc: 0.6625
[Epoch 59] loss: 0.19627516831883499 acc: 0.6592
[Epoch 63] loss: 0.18758644420258186 acc: 0.657
[Epoch 67] loss: 0.18424902435587456 acc: 0.6612
[Epoch 71] loss: 0.18764065035864178 acc: 0.6571
--> [test] acc: 0.6583
--> [accuracy] finished 0.6583
new state: tensor([608.,   1.,   2.,   2.,   3.], device='cuda:0')
new reward: 0.6583
--> [reward] 0.6583
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     2.0      |     3.0     | 0.6583 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.996437841669068 acc: 0.541
[Epoch 7] loss: 4.293972866462015 acc: 0.6111
[Epoch 11] loss: 3.1062627741900246 acc: 0.614
[Epoch 15] loss: 1.9548368407865924 acc: 0.6145
[Epoch 19] loss: 1.0797750012344107 acc: 0.6002
[Epoch 23] loss: 0.7180868046610709 acc: 0.6029
[Epoch 27] loss: 0.5421729776960658 acc: 0.6006
[Epoch 31] loss: 0.45406014683282436 acc: 0.5992
[Epoch 35] loss: 0.41992679361701774 acc: 0.5964
[Epoch 39] loss: 0.3972607317030468 acc: 0.5972
[Epoch 43] loss: 0.3511335825800057 acc: 0.5993
[Epoch 47] loss: 0.34527689847818877 acc: 0.586
[Epoch 51] loss: 0.3243957251653342 acc: 0.592
[Epoch 55] loss: 0.3046981274231296 acc: 0.5905
[Epoch 59] loss: 0.28777602799427326 acc: 0.5873
[Epoch 63] loss: 0.28344770089325394 acc: 0.6001
[Epoch 67] loss: 0.25791950736080516 acc: 0.5959
[Epoch 71] loss: 0.27462602395783453 acc: 0.5998
--> [test] acc: 0.5949
--> [accuracy] finished 0.5949
new state: tensor([608.,   1.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.5949
--> [reward] 0.5949
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.5949 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.5949 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.00284519189459 acc: 0.5306
[Epoch 7] loss: 4.305907856015598 acc: 0.599
[Epoch 11] loss: 3.1426067028356637 acc: 0.6022
[Epoch 15] loss: 1.9957551636430613 acc: 0.6091
[Epoch 19] loss: 1.126257802919506 acc: 0.6059
[Epoch 23] loss: 0.7321600812580198 acc: 0.6082
[Epoch 27] loss: 0.5564134867571275 acc: 0.6001
[Epoch 31] loss: 0.4910272464723043 acc: 0.5961
[Epoch 35] loss: 0.43266771734237214 acc: 0.5989
[Epoch 39] loss: 0.38789483290546767 acc: 0.6066
[Epoch 43] loss: 0.3630431480324634 acc: 0.6031
[Epoch 47] loss: 0.3535106493388791 acc: 0.603
[Epoch 51] loss: 0.341060309566062 acc: 0.591
[Epoch 55] loss: 0.31755788478038044 acc: 0.5953
[Epoch 59] loss: 0.30220061315275976 acc: 0.6026
[Epoch 63] loss: 0.27474453586541936 acc: 0.5923
[Epoch 67] loss: 0.2695336834476341 acc: 0.5963
[Epoch 71] loss: 0.27302980087006756 acc: 0.5977
--> [test] acc: 0.595
--> [accuracy] finished 0.595
new state: tensor([576.,   1.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.595
--> [reward] 0.595
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.595  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.5949 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.786986326317653 acc: 0.4515
[Epoch 7] loss: 5.555535671656089 acc: 0.4924
[Epoch 11] loss: 4.768977300132937 acc: 0.5089
[Epoch 15] loss: 3.88165950729414 acc: 0.501
[Epoch 19] loss: 2.803389046808033 acc: 0.4978
[Epoch 23] loss: 1.8148109830172776 acc: 0.4812
[Epoch 27] loss: 1.204904246989571 acc: 0.4814
[Epoch 31] loss: 0.8964689170651119 acc: 0.4743
[Epoch 35] loss: 0.7737253157569625 acc: 0.4695
[Epoch 39] loss: 0.7012364807945993 acc: 0.4679
[Epoch 43] loss: 0.6290193191679466 acc: 0.4648
[Epoch 47] loss: 0.5625171306664529 acc: 0.4717
[Epoch 51] loss: 0.535659817430903 acc: 0.4724
[Epoch 55] loss: 0.53176000314143 acc: 0.4688
[Epoch 59] loss: 0.4774633881962284 acc: 0.4803
[Epoch 63] loss: 0.47543794322577887 acc: 0.4706
[Epoch 67] loss: 0.4413645465898773 acc: 0.4696
[Epoch 71] loss: 0.42903715556920946 acc: 0.471
--> [test] acc: 0.473
--> [accuracy] finished 0.473
new state: tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.473
--> [reward] 0.473
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1120]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.2239]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.4732]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.4774]], device='cuda:0')
------ ------
delta_t: tensor([[0.4732]], device='cuda:0')
rewards[i]: 0.473
values[i+1]: tensor([[0.0044]], device='cuda:0')
values[i]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.4732]], device='cuda:0')
delta_t: tensor([[0.4732]], device='cuda:0')
------ ------
policy_loss: 1.1106997728347778
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.4732]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.6777]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.1315]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.0637]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.0676]], device='cuda:0')
------ ------
delta_t: tensor([[0.5953]], device='cuda:0')
rewards[i]: 0.595
values[i+1]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0039]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.0637]], device='cuda:0')
delta_t: tensor([[0.5953]], device='cuda:0')
------ ------
policy_loss: 3.6373684406280518
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.0637]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.0357]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.7160]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.6480]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.6518]], device='cuda:0')
------ ------
delta_t: tensor([[0.5949]], device='cuda:0')
rewards[i]: 0.5949
values[i+1]: tensor([[0.0039]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.6480]], device='cuda:0')
delta_t: tensor([[0.5949]], device='cuda:0')
------ ------
policy_loss: 7.5648369789123535
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.6480]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[4.6568]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.2421]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.2896]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.2936]], device='cuda:0')
------ ------
delta_t: tensor([[0.6580]], device='cuda:0')
rewards[i]: 0.6583
values[i+1]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0040]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.2896]], device='cuda:0')
delta_t: tensor([[0.6580]], device='cuda:0')
------ ------
policy_loss: 13.030998229980469
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.2896]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[9.0740]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.8344]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9723]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.9765]], device='cuda:0')
------ ------
delta_t: tensor([[0.7056]], device='cuda:0')
rewards[i]: 0.7058
values[i+1]: tensor([[0.0040]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9723]], device='cuda:0')
delta_t: tensor([[0.7056]], device='cuda:0')
------ ------
policy_loss: 20.13275909423828
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9723]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[15.7367]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.3254]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6504]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6542]], device='cuda:0')
------ ------
delta_t: tensor([[0.7079]], device='cuda:0')
rewards[i]: 0.7075
values[i+1]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6504]], device='cuda:0')
delta_t: tensor([[0.7079]], device='cuda:0')
------ ------
policy_loss: 28.861785888671875
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6504]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[25.1261]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[18.7790]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3335]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3369]], device='cuda:0')
------ ------
delta_t: tensor([[0.7196]], device='cuda:0')
rewards[i]: 0.7192
values[i+1]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0034]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3335]], device='cuda:0')
delta_t: tensor([[0.7196]], device='cuda:0')
------ ------
policy_loss: 39.228694915771484
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3335]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.6194]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.9865]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9986]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0018]], device='cuda:0')
------ ------
delta_t: tensor([[0.7085]], device='cuda:0')
rewards[i]: 0.7083
values[i+1]: tensor([[0.0034]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9986]], device='cuda:0')
delta_t: tensor([[0.7085]], device='cuda:0')
------ ------
policy_loss: 51.190589904785156
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9986]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[53.6437]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.0487]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.6612]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.6643]], device='cuda:0')
------ ------
delta_t: tensor([[0.7125]], device='cuda:0')
rewards[i]: 0.7125
values[i+1]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.6612]], device='cuda:0')
delta_t: tensor([[0.7125]], device='cuda:0')
------ ------
policy_loss: 64.74161529541016
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.6612]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[73.6164]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[39.9453]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3202]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.3234]], device='cuda:0')
------ ------
delta_t: tensor([[0.7157]], device='cuda:0')
rewards[i]: 0.7158
values[i+1]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0032]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3202]], device='cuda:0')
delta_t: tensor([[0.7157]], device='cuda:0')
------ ------
policy_loss: 79.87303924560547
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3202]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[97.8417]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[48.4507]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9607]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.9638]], device='cuda:0')
------ ------
delta_t: tensor([[0.7036]], device='cuda:0')
rewards[i]: 0.7036
values[i+1]: tensor([[0.0032]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9607]], device='cuda:0')
delta_t: tensor([[0.7036]], device='cuda:0')
------ ------
policy_loss: 96.53839111328125
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9607]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[126.9357]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[58.1879]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.6281]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.6314]], device='cuda:0')
------ ------
delta_t: tensor([[0.7371]], device='cuda:0')
rewards[i]: 0.7372
values[i+1]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0033]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.6281]], device='cuda:0')
delta_t: tensor([[0.7371]], device='cuda:0')
------ ------
policy_loss: 114.80205535888672
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.6281]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[161.2961]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[68.7209]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.2898]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.2926]], device='cuda:0')
------ ------
delta_t: tensor([[0.7380]], device='cuda:0')
rewards[i]: 0.7376
values[i+1]: tensor([[0.0033]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0028]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.2898]], device='cuda:0')
delta_t: tensor([[0.7380]], device='cuda:0')
------ ------
policy_loss: 134.6552276611328
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.2898]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[201.3370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[80.0818]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.9488]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.9509]], device='cuda:0')
------ ------
delta_t: tensor([[0.7419]], device='cuda:0')
rewards[i]: 0.7412
values[i+1]: tensor([[0.0028]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.9488]], device='cuda:0')
delta_t: tensor([[0.7419]], device='cuda:0')
------ ------
policy_loss: 156.0889892578125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.9488]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[247.3126]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[91.9511]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.5891]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.5900]], device='cuda:0')
------ ------
delta_t: tensor([[0.7298]], device='cuda:0')
rewards[i]: 0.7286
values[i+1]: tensor([[0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.5891]], device='cuda:0')
delta_t: tensor([[0.7298]], device='cuda:0')
------ ------
policy_loss: 179.06297302246094
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.5891]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 179.06297302246094
value_loss: 247.31260681152344
loss: 302.7192687988281



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 3.5814e-04,  5.2963e-07,  7.0788e-08,  9.3972e-07,  7.4588e-07],
        [-1.4437e-02, -3.3147e-05, -3.4235e-05, -2.8389e-05, -5.0834e-05],
        [-3.7821e-04, -1.0506e-06, -1.8301e-06, -5.2270e-07, -2.0792e-06],
        [-1.7608e-02, -3.7487e-05,  1.6994e-05, -4.6006e-05, -8.3832e-06],
        [-1.4062e-02, -2.6626e-05,  2.9360e-04, -9.3481e-05,  2.7149e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-3.0953e-06, -4.4871e-06,  7.2966e-08,  4.5902e-06, -2.6840e-06],
        [ 1.0746e-04,  1.7246e-04,  4.0539e-06, -1.7495e-04,  9.4092e-05],
        [ 2.3159e-06,  4.3137e-06,  3.0837e-07, -4.3282e-06,  2.0756e-06],
        [ 1.6343e-04,  2.2295e-04, -7.7047e-06, -2.2929e-04,  1.3918e-04],
        [ 2.9630e-04,  2.4386e-04, -7.0033e-05, -2.6546e-04,  2.3510e-04]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.0126,  0.0116, -0.0126,  0.0089, -0.0048],
        [ 0.0537, -0.0520,  0.0536, -0.0450,  0.0289],
        [-0.0360,  0.0353, -0.0360,  0.0314, -0.0208],
        [ 0.0033, -0.0032,  0.0033, -0.0028,  0.0018],
        [-0.0084,  0.0083, -0.0084,  0.0075, -0.0051]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0636,  0.1103,  0.0056, -0.1113,  0.0565],
        [ 0.0139,  0.0235,  0.0006, -0.0237,  0.0122],
        [ 0.0207,  0.0356,  0.0015, -0.0360,  0.0185],
        [-0.0262, -0.0457, -0.0022,  0.0461, -0.0233],
        [ 0.0065,  0.0111,  0.0002, -0.0111,  0.0059],
        [-0.0264, -0.0460, -0.0022,  0.0464, -0.0234],
        [-0.0264, -0.0461, -0.0023,  0.0464, -0.0235],
        [-0.0164, -0.0288, -0.0017,  0.0290, -0.0144],
        [ 0.0113,  0.0220,  0.0023, -0.0219,  0.0100],
        [-0.0264, -0.0460, -0.0022,  0.0464, -0.0235],
        [ 0.0059,  0.0102,  0.0006, -0.0103,  0.0050]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.5959,  2.7824,  0.1361, -2.8037,  1.4181]], device='cuda:0')
--> [loss] 302.7192687988281

---------------------------------- [[#5 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.473  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  576.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.473  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.782348939829775 acc: 0.4512
[Epoch 7] loss: 5.522789072197721 acc: 0.485
[Epoch 11] loss: 4.756141616407867 acc: 0.5108
[Epoch 15] loss: 3.889405967786794 acc: 0.5017
[Epoch 19] loss: 2.836134026498746 acc: 0.4951
[Epoch 23] loss: 1.8460596091378376 acc: 0.4879
[Epoch 27] loss: 1.2254645365202212 acc: 0.479
[Epoch 31] loss: 0.9105931710442314 acc: 0.4677
[Epoch 35] loss: 0.8009530525568807 acc: 0.4746
[Epoch 39] loss: 0.6719927520245847 acc: 0.4802
[Epoch 43] loss: 0.6139450664239009 acc: 0.4788
[Epoch 47] loss: 0.5891216066058563 acc: 0.4722
[Epoch 51] loss: 0.5330987203380336 acc: 0.4776
[Epoch 55] loss: 0.5252831459302655 acc: 0.4719
[Epoch 59] loss: 0.4869985906955074 acc: 0.4722
[Epoch 63] loss: 0.4746449986696624 acc: 0.4755
[Epoch 67] loss: 0.4251879109546085 acc: 0.481
[Epoch 71] loss: 0.4075181236314347 acc: 0.4724
--> [test] acc: 0.47
--> [accuracy] finished 0.47
new state: tensor([608.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.47
--> [reward] 0.47
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.767315307236694 acc: 0.4542
[Epoch 7] loss: 5.525891588167156 acc: 0.4846
[Epoch 11] loss: 4.739256105459559 acc: 0.5071
[Epoch 15] loss: 3.8552043430335687 acc: 0.5115
[Epoch 19] loss: 2.7934473147020316 acc: 0.499
[Epoch 23] loss: 1.8031510815519811 acc: 0.4883
[Epoch 27] loss: 1.2208171903019975 acc: 0.4823
[Epoch 31] loss: 0.9210781355000213 acc: 0.4763
[Epoch 35] loss: 0.762432501596563 acc: 0.4864
[Epoch 39] loss: 0.6779675031238047 acc: 0.4793
[Epoch 43] loss: 0.6193185784589604 acc: 0.4805
[Epoch 47] loss: 0.5904313334075691 acc: 0.4807
[Epoch 51] loss: 0.5250345553221453 acc: 0.4843
[Epoch 55] loss: 0.5175635577191401 acc: 0.4782
[Epoch 59] loss: 0.46632854899158105 acc: 0.4804
[Epoch 63] loss: 0.46849338907052945 acc: 0.4855
[Epoch 67] loss: 0.43690463773372684 acc: 0.4807
[Epoch 71] loss: 0.42323684819814417 acc: 0.4808
--> [test] acc: 0.4865
--> [accuracy] finished 0.4865
new state: tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.4865
--> [reward] 0.4865
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.4865 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([576.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.604592756237215 acc: 0.4733
[Epoch 7] loss: 5.316730294386139 acc: 0.51
[Epoch 11] loss: 4.523846875066343 acc: 0.529
[Epoch 15] loss: 3.6291557486404846 acc: 0.5272
[Epoch 19] loss: 2.565233098119116 acc: 0.5214
[Epoch 23] loss: 1.6293012548971664 acc: 0.5147
[Epoch 27] loss: 1.0581911821347063 acc: 0.5041
[Epoch 31] loss: 0.7975388894052914 acc: 0.5045
[Epoch 35] loss: 0.6679290298305814 acc: 0.5092
[Epoch 39] loss: 0.5865731053887997 acc: 0.4981
[Epoch 43] loss: 0.5599521324371971 acc: 0.493
[Epoch 47] loss: 0.4910942538238852 acc: 0.4935
[Epoch 51] loss: 0.4722949174635322 acc: 0.493
[Epoch 55] loss: 0.46556858635028764 acc: 0.5039
[Epoch 59] loss: 0.4378478538406932 acc: 0.4948
[Epoch 63] loss: 0.38675251950407424 acc: 0.4962
[Epoch 67] loss: 0.4028446754069089 acc: 0.5026
[Epoch 71] loss: 0.35973877196088244 acc: 0.4973
--> [test] acc: 0.492
--> [accuracy] finished 0.492
new state: tensor([576.,   1.,   1.,   4.,   3.], device='cuda:0')
new reward: 0.492
--> [reward] 0.492
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     4.0      |     3.0     | 0.492  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([576.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.17065831797812 acc: 0.5213
[Epoch 7] loss: 4.65122285325204 acc: 0.561
[Epoch 11] loss: 3.5740483080029795 acc: 0.5684
[Epoch 15] loss: 2.3713122025856275 acc: 0.5719
[Epoch 19] loss: 1.3554117164152968 acc: 0.5641
[Epoch 23] loss: 0.8082708270691544 acc: 0.5729
[Epoch 27] loss: 0.597662641564408 acc: 0.5507
[Epoch 31] loss: 0.5028113255377316 acc: 0.5443
[Epoch 35] loss: 0.4525453643706601 acc: 0.5573
[Epoch 39] loss: 0.39694879208322226 acc: 0.5605
[Epoch 43] loss: 0.37384725161506543 acc: 0.5504
[Epoch 47] loss: 0.34072145697234385 acc: 0.5479
[Epoch 51] loss: 0.32641215797971046 acc: 0.5582
[Epoch 55] loss: 0.3041428430148822 acc: 0.5586
[Epoch 59] loss: 0.31113502335594134 acc: 0.5626
[Epoch 63] loss: 0.27123473564405803 acc: 0.5583
[Epoch 67] loss: 0.2783392966555817 acc: 0.5514
[Epoch 71] loss: 0.25435142077939094 acc: 0.5417
--> [test] acc: 0.5511
--> [accuracy] finished 0.5511
new state: tensor([576.,   1.,   2.,   4.,   3.], device='cuda:0')
new reward: 0.5511
--> [reward] 0.5511
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     2.0     |     4.0      |     3.0     | 0.5511 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.175258413909951 acc: 0.5156
[Epoch 7] loss: 4.633222080100223 acc: 0.566
[Epoch 11] loss: 3.525598922958764 acc: 0.5759
[Epoch 15] loss: 2.3219339121180727 acc: 0.5793
[Epoch 19] loss: 1.309375061403455 acc: 0.5695
[Epoch 23] loss: 0.8134351793648033 acc: 0.5706
[Epoch 27] loss: 0.5943743646659357 acc: 0.5644
[Epoch 31] loss: 0.4796634684686008 acc: 0.5617
[Epoch 35] loss: 0.44963047292340746 acc: 0.5611
[Epoch 39] loss: 0.3945100123415251 acc: 0.5742
[Epoch 43] loss: 0.37784702182673585 acc: 0.5636
[Epoch 47] loss: 0.3318785456773799 acc: 0.5554
[Epoch 51] loss: 0.324086525406012 acc: 0.5648
[Epoch 55] loss: 0.30493665186454877 acc: 0.5629
[Epoch 59] loss: 0.2905986997920572 acc: 0.5601
[Epoch 63] loss: 0.27420339990130926 acc: 0.5483
[Epoch 67] loss: 0.2713740357648 acc: 0.5608
[Epoch 71] loss: 0.2587196327617292 acc: 0.5595
--> [test] acc: 0.5614
--> [accuracy] finished 0.5614
new state: tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
new reward: 0.5614
--> [reward] 0.5614
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     4.0      |     3.0     | 0.5614 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.617557332643767 acc: 0.4494
[Epoch 7] loss: 5.328535925854197 acc: 0.501
[Epoch 11] loss: 4.514997204398865 acc: 0.5338
[Epoch 15] loss: 3.598308163378245 acc: 0.5271
[Epoch 19] loss: 2.5157762001390043 acc: 0.5071
[Epoch 23] loss: 1.585253702328943 acc: 0.5108
[Epoch 27] loss: 1.0388237938970861 acc: 0.5047
[Epoch 31] loss: 0.8092473158446114 acc: 0.5003
[Epoch 35] loss: 0.6680520991497028 acc: 0.4951
[Epoch 39] loss: 0.5755104333105142 acc: 0.5006
[Epoch 43] loss: 0.5406301099082926 acc: 0.497
[Epoch 47] loss: 0.49945487225871255 acc: 0.4946
[Epoch 51] loss: 0.4697900869481056 acc: 0.5046
[Epoch 55] loss: 0.4397151985389116 acc: 0.4953
[Epoch 59] loss: 0.4157286243408423 acc: 0.4966
[Epoch 63] loss: 0.41210803936909685 acc: 0.4942
[Epoch 67] loss: 0.38956086373056675 acc: 0.494
[Epoch 71] loss: 0.36307193011598055 acc: 0.4939
--> [test] acc: 0.4885
--> [accuracy] finished 0.4885
new state: tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
new reward: 0.4885
--> [reward] 0.4885
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     4.0      |     3.0     | 0.4885 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.620229959487915 acc: 0.4655
[Epoch 7] loss: 5.354968489435932 acc: 0.4864
[Epoch 11] loss: 4.563249225659138 acc: 0.5284
[Epoch 15] loss: 3.673091523025347 acc: 0.523
[Epoch 19] loss: 2.611559397271832 acc: 0.5137
[Epoch 23] loss: 1.6580186044925924 acc: 0.5027
[Epoch 27] loss: 1.0927138149433429 acc: 0.4999
[Epoch 31] loss: 0.8307930771594919 acc: 0.4987
[Epoch 35] loss: 0.6749473040556664 acc: 0.4933
[Epoch 39] loss: 0.6107741864707769 acc: 0.4986
[Epoch 43] loss: 0.5686799835537553 acc: 0.5038
[Epoch 47] loss: 0.5084670059992682 acc: 0.4906
[Epoch 51] loss: 0.4664265258318704 acc: 0.4974
[Epoch 55] loss: 0.4702893339450021 acc: 0.4973
[Epoch 59] loss: 0.4378377212702161 acc: 0.5009
[Epoch 63] loss: 0.3970893554108413 acc: 0.4969
[Epoch 67] loss: 0.40737854344698854 acc: 0.4963
[Epoch 71] loss: 0.37183836324359565 acc: 0.4984
--> [test] acc: 0.4957
--> [accuracy] finished 0.4957
new state: tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
new reward: 0.4957
--> [reward] 0.4957
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     4.0      |     3.0     | 0.4957 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.184338469029693 acc: 0.5228
[Epoch 7] loss: 4.661200559047787 acc: 0.5691
[Epoch 11] loss: 3.5725252413383837 acc: 0.5826
[Epoch 15] loss: 2.3733065481228595 acc: 0.572
[Epoch 19] loss: 1.3559106331690194 acc: 0.5687
[Epoch 23] loss: 0.8239406276580012 acc: 0.5619
[Epoch 27] loss: 0.6088954132726735 acc: 0.5564
[Epoch 31] loss: 0.48575789797717656 acc: 0.5489
[Epoch 35] loss: 0.45908540981771695 acc: 0.5524
[Epoch 39] loss: 0.4048061226292149 acc: 0.5551
[Epoch 43] loss: 0.3625584861068317 acc: 0.5441
[Epoch 47] loss: 0.3572385471833446 acc: 0.5551
[Epoch 51] loss: 0.3211692613537621 acc: 0.5463
[Epoch 55] loss: 0.3179630040407867 acc: 0.5575
[Epoch 59] loss: 0.2826250305975718 acc: 0.5532
[Epoch 63] loss: 0.283194412677394 acc: 0.5566
[Epoch 67] loss: 0.26608135894684076 acc: 0.5531
[Epoch 71] loss: 0.2628342082558433 acc: 0.5509
--> [test] acc: 0.5576
--> [accuracy] finished 0.5576
new state: tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
new reward: 0.5576
--> [reward] 0.5576
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     4.0      |     3.0     | 0.5576 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([608.,   1.,   2.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.163372520290677 acc: 0.5134
[Epoch 7] loss: 4.612843569892142 acc: 0.5674
[Epoch 11] loss: 3.511278250180852 acc: 0.5749
[Epoch 15] loss: 2.3329719892700616 acc: 0.5695
[Epoch 19] loss: 1.3390700410470329 acc: 0.5541
[Epoch 23] loss: 0.8157524804077337 acc: 0.561
[Epoch 27] loss: 0.5826170858510239 acc: 0.5548
[Epoch 31] loss: 0.501774722514936 acc: 0.5573
[Epoch 35] loss: 0.4239573775340453 acc: 0.5529
[Epoch 39] loss: 0.4094854089886407 acc: 0.5564
[Epoch 43] loss: 0.3569072990694924 acc: 0.5532
[Epoch 47] loss: 0.34409554813371596 acc: 0.5597
[Epoch 51] loss: 0.32109829754380464 acc: 0.5466
[Epoch 55] loss: 0.30400199944015277 acc: 0.5566
[Epoch 59] loss: 0.27481155647941485 acc: 0.5558
[Epoch 63] loss: 0.28072444444565137 acc: 0.5483
[Epoch 67] loss: 0.2762657178284796 acc: 0.5532
[Epoch 71] loss: 0.2523292132184062 acc: 0.5624
--> [test] acc: 0.5582
--> [accuracy] finished 0.5582
new state: tensor([608.,   1.,   2.,   5.,   3.], device='cuda:0')
new reward: 0.5582
--> [reward] 0.5582
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     5.0      |     3.0     | 0.5582 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.583845904111253 acc: 0.4604
[Epoch 7] loss: 5.304729516853762 acc: 0.513
[Epoch 11] loss: 4.47713719182612 acc: 0.5342
[Epoch 15] loss: 3.5639812521007666 acc: 0.533
[Epoch 19] loss: 2.484400340586977 acc: 0.5206
[Epoch 23] loss: 1.53445739750667 acc: 0.5065
[Epoch 27] loss: 1.019157812189873 acc: 0.503
[Epoch 31] loss: 0.750956097794006 acc: 0.5081
[Epoch 35] loss: 0.6671288346543032 acc: 0.496
[Epoch 39] loss: 0.56592169579814 acc: 0.4981
[Epoch 43] loss: 0.5340083095833392 acc: 0.4979
[Epoch 47] loss: 0.49299292199199307 acc: 0.504
[Epoch 51] loss: 0.4755816554407711 acc: 0.5003
[Epoch 55] loss: 0.44428616673078225 acc: 0.4959
[Epoch 59] loss: 0.4087604536815449 acc: 0.4947
[Epoch 63] loss: 0.39375903649622446 acc: 0.4983
[Epoch 67] loss: 0.4077275305643411 acc: 0.5011
[Epoch 71] loss: 0.3616620509354088 acc: 0.4944
--> [test] acc: 0.496
--> [accuracy] finished 0.496
new state: tensor([608.,   1.,   1.,   5.,   3.], device='cuda:0')
new reward: 0.496
--> [reward] 0.496
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     5.0      |     3.0     | 0.496  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.586121994211241 acc: 0.4502
[Epoch 7] loss: 5.315617375666528 acc: 0.5184
[Epoch 11] loss: 4.522042452221942 acc: 0.5368
[Epoch 15] loss: 3.619409608109223 acc: 0.5257
[Epoch 19] loss: 2.532197492049478 acc: 0.5204
[Epoch 23] loss: 1.5866581882585955 acc: 0.5153
[Epoch 27] loss: 1.0545508040453466 acc: 0.5079
[Epoch 31] loss: 0.7787334527296331 acc: 0.5098
[Epoch 35] loss: 0.6572216442688499 acc: 0.5091
[Epoch 39] loss: 0.6098129745296506 acc: 0.505
[Epoch 43] loss: 0.5337251997540903 acc: 0.5009
[Epoch 47] loss: 0.524089408576336 acc: 0.507
[Epoch 51] loss: 0.47516495608688925 acc: 0.5066
[Epoch 55] loss: 0.44245623894359754 acc: 0.5013
[Epoch 59] loss: 0.4432979786668516 acc: 0.5069
[Epoch 63] loss: 0.3916479419807301 acc: 0.5058
[Epoch 67] loss: 0.41522715735437393 acc: 0.4919
[Epoch 71] loss: 0.3832679478275349 acc: 0.4955
--> [test] acc: 0.5009
--> [accuracy] finished 0.5009
new state: tensor([608.,   1.,   1.,   5.,   3.], device='cuda:0')
new reward: 0.5009
--> [reward] 0.5009
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     5.0      |     3.0     | 0.5009 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([608.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.698639522428098 acc: 0.4278
[Epoch 7] loss: 5.482460557347369 acc: 0.513
[Epoch 11] loss: 4.695969789076949 acc: 0.5205
[Epoch 15] loss: 3.833108609137328 acc: 0.5265
[Epoch 19] loss: 2.7652487725857884 acc: 0.5177
[Epoch 23] loss: 1.8078616749676293 acc: 0.4925
[Epoch 27] loss: 1.194377136943133 acc: 0.498
[Epoch 31] loss: 0.9112595938470053 acc: 0.4897
[Epoch 35] loss: 0.7341363082075363 acc: 0.4946
[Epoch 39] loss: 0.6622597971535704 acc: 0.4855
[Epoch 43] loss: 0.5968411304580662 acc: 0.4903
[Epoch 47] loss: 0.5477035035572165 acc: 0.4774
[Epoch 51] loss: 0.5316750586147199 acc: 0.4886
[Epoch 55] loss: 0.4788785642727523 acc: 0.4868
[Epoch 59] loss: 0.46340683715708575 acc: 0.4927
[Epoch 63] loss: 0.4544228823650676 acc: 0.4893
[Epoch 67] loss: 0.40175841965467274 acc: 0.4846
[Epoch 71] loss: 0.422695151487332 acc: 0.4932
--> [test] acc: 0.4849
--> [accuracy] finished 0.4849
new state: tensor([608.,   1.,   1.,   6.,   3.], device='cuda:0')
new reward: 0.4849
--> [reward] 0.4849
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     6.0      |     3.0     | 0.4849 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.882111310044213 acc: 0.4484
[Epoch 7] loss: 5.779572784748224 acc: 0.464
[Epoch 11] loss: 5.080996339735777 acc: 0.4866
[Epoch 15] loss: 4.270334981591501 acc: 0.4925
[Epoch 19] loss: 3.239920766228605 acc: 0.4877
[Epoch 23] loss: 2.172994530185714 acc: 0.4683
[Epoch 27] loss: 1.4642002703359975 acc: 0.4735
[Epoch 31] loss: 1.0784731536265224 acc: 0.4643
[Epoch 35] loss: 0.892244271848284 acc: 0.4664
[Epoch 39] loss: 0.7885313571509346 acc: 0.4679
[Epoch 43] loss: 0.7064343744988941 acc: 0.4651
[Epoch 47] loss: 0.6357455921199773 acc: 0.4579
[Epoch 51] loss: 0.6081400172846854 acc: 0.4579
[Epoch 55] loss: 0.5789534411133479 acc: 0.4598
[Epoch 59] loss: 0.5656780499317076 acc: 0.4631
[Epoch 63] loss: 0.5188921156155941 acc: 0.4593
[Epoch 67] loss: 0.48446241437512283 acc: 0.4578
[Epoch 71] loss: 0.47202855410992794 acc: 0.4561
--> [test] acc: 0.4472
--> [accuracy] finished 0.4472
new state: tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
new reward: 0.4472
--> [reward] 0.4472
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.920585108839947 acc: 0.4396
[Epoch 7] loss: 5.834184017937506 acc: 0.4735
[Epoch 11] loss: 5.154234408417626 acc: 0.4833
[Epoch 15] loss: 4.367224793452436 acc: 0.485
[Epoch 19] loss: 3.341507293379215 acc: 0.4797
[Epoch 23] loss: 2.3140314912704554 acc: 0.4748
[Epoch 27] loss: 1.5735158166845742 acc: 0.464
[Epoch 31] loss: 1.1635005548024726 acc: 0.4644
[Epoch 35] loss: 0.9217827319527221 acc: 0.4655
[Epoch 39] loss: 0.8357042542770695 acc: 0.4554
[Epoch 43] loss: 0.7271464003721619 acc: 0.4573
[Epoch 47] loss: 0.663584451598432 acc: 0.4608
[Epoch 51] loss: 0.6594466354050066 acc: 0.4531
[Epoch 55] loss: 0.6128073513812726 acc: 0.4663
[Epoch 59] loss: 0.5510401860298708 acc: 0.4581
[Epoch 63] loss: 0.5273474854562441 acc: 0.4581
[Epoch 67] loss: 0.5234887305256504 acc: 0.4492
[Epoch 71] loss: 0.493068116328791 acc: 0.4413
--> [test] acc: 0.4548
--> [accuracy] finished 0.4548
new state: tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
new reward: 0.4548
--> [reward] 0.4548
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4548 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.935130783359108 acc: 0.43
[Epoch 7] loss: 5.8099807897187254 acc: 0.4805
[Epoch 11] loss: 5.135290552435628 acc: 0.4986
[Epoch 15] loss: 4.3516204787032375 acc: 0.4814
[Epoch 19] loss: 3.3432324929615422 acc: 0.4779
[Epoch 23] loss: 2.312325096038906 acc: 0.4723
[Epoch 27] loss: 1.5685239239117068 acc: 0.4648
[Epoch 31] loss: 1.1774732379428565 acc: 0.464
[Epoch 35] loss: 0.9461616563522602 acc: 0.463
[Epoch 39] loss: 0.8262753682997068 acc: 0.4621
[Epoch 43] loss: 0.7590788665996946 acc: 0.4575
[Epoch 47] loss: 0.6585096279087731 acc: 0.4541
[Epoch 51] loss: 0.6572671331932096 acc: 0.4565
[Epoch 55] loss: 0.5844427703610618 acc: 0.4602
[Epoch 59] loss: 0.5605072774174039 acc: 0.4627
[Epoch 63] loss: 0.5201329958560826 acc: 0.4579
[Epoch 67] loss: 0.5341078713107521 acc: 0.4689
[Epoch 71] loss: 0.4839373460692137 acc: 0.4622
--> [test] acc: 0.4546
--> [accuracy] finished 0.4546
new state: tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
new reward: 0.4546
--> [reward] 0.4546
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1033]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.2066]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.4545]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.4631]], device='cuda:0')
------ ------
delta_t: tensor([[0.4545]], device='cuda:0')
rewards[i]: 0.4546
values[i+1]: tensor([[0.0086]], device='cuda:0')
values[i]: tensor([[0.0086]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.4545]], device='cuda:0')
delta_t: tensor([[0.4545]], device='cuda:0')
------ ------
policy_loss: 1.0659879446029663
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.4545]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.5126]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.8186]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.9048]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.9133]], device='cuda:0')
------ ------
delta_t: tensor([[0.4548]], device='cuda:0')
rewards[i]: 0.4548
values[i+1]: tensor([[0.0086]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.9048]], device='cuda:0')
delta_t: tensor([[0.4548]], device='cuda:0')
------ ------
policy_loss: 3.2112982273101807
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.9048]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[1.4143]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.8035]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.3429]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3514]], device='cuda:0')
------ ------
delta_t: tensor([[0.4472]], device='cuda:0')
rewards[i]: 0.4472
values[i+1]: tensor([[0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.3429]], device='cuda:0')
delta_t: tensor([[0.4472]], device='cuda:0')
------ ------
policy_loss: 6.40763521194458
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.3429]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[3.0604]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.2922]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8144]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.8227]], device='cuda:0')
------ ------
delta_t: tensor([[0.4849]], device='cuda:0')
rewards[i]: 0.4849
values[i+1]: tensor([[0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0083]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8144]], device='cuda:0')
delta_t: tensor([[0.4849]], device='cuda:0')
------ ------
policy_loss: 10.734621047973633
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8144]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[5.6991]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.2773]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.2972]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.3054]], device='cuda:0')
------ ------
delta_t: tensor([[0.5009]], device='cuda:0')
rewards[i]: 0.5009
values[i+1]: tensor([[0.0083]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0082]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.2972]], device='cuda:0')
delta_t: tensor([[0.5009]], device='cuda:0')
------ ------
policy_loss: 16.218563079833984
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.2972]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[9.5365]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.6750]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.7704]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7784]], device='cuda:0')
------ ------
delta_t: tensor([[0.4961]], device='cuda:0')
rewards[i]: 0.496
values[i+1]: tensor([[0.0082]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.7704]], device='cuda:0')
delta_t: tensor([[0.4961]], device='cuda:0')
------ ------
policy_loss: 22.83692741394043
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.7704]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[14.9841]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.8951]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3008]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3088]], device='cuda:0')
------ ------
delta_t: tensor([[0.5581]], device='cuda:0')
rewards[i]: 0.5582
values[i+1]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3008]], device='cuda:0')
delta_t: tensor([[0.5581]], device='cuda:0')
------ ------
policy_loss: 30.72811508178711
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3008]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[22.3002]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.6321]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.8252]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.8333]], device='cuda:0')
------ ------
delta_t: tensor([[0.5574]], device='cuda:0')
rewards[i]: 0.5576
values[i+1]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.8252]], device='cuda:0')
delta_t: tensor([[0.5574]], device='cuda:0')
------ ------
policy_loss: 39.8765869140625
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.8252]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[31.4707]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[18.3411]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2827]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2907]], device='cuda:0')
------ ------
delta_t: tensor([[0.4957]], device='cuda:0')
rewards[i]: 0.4957
values[i+1]: tensor([[0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2827]], device='cuda:0')
delta_t: tensor([[0.4957]], device='cuda:0')
------ ------
policy_loss: 50.1215934753418
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2827]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[42.6488]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.3561]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7282]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7362]], device='cuda:0')
------ ------
delta_t: tensor([[0.4884]], device='cuda:0')
rewards[i]: 0.4885
values[i+1]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7282]], device='cuda:0')
delta_t: tensor([[0.4884]], device='cuda:0')
------ ------
policy_loss: 61.43415451049805
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7282]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[56.3875]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[27.4775]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2419]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.2503]], device='cuda:0')
------ ------
delta_t: tensor([[0.5610]], device='cuda:0')
rewards[i]: 0.5614
values[i+1]: tensor([[0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2419]], device='cuda:0')
delta_t: tensor([[0.5610]], device='cuda:0')
------ ------
policy_loss: 73.97664642333984
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2419]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[72.8637]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.9524]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.7404]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.7489]], device='cuda:0')
------ ------
delta_t: tensor([[0.5509]], device='cuda:0')
rewards[i]: 0.5511
values[i+1]: tensor([[0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.7404]], device='cuda:0')
delta_t: tensor([[0.5509]], device='cuda:0')
------ ------
policy_loss: 87.7176513671875
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.7404]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[91.9309]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[38.1344]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1753]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1834]], device='cuda:0')
------ ------
delta_t: tensor([[0.4923]], device='cuda:0')
rewards[i]: 0.492
values[i+1]: tensor([[0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1753]], device='cuda:0')
delta_t: tensor([[0.4923]], device='cuda:0')
------ ------
policy_loss: 102.50188446044922
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1753]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[113.7139]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[43.5660]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.6005]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.6081]], device='cuda:0')
------ ------
delta_t: tensor([[0.4869]], device='cuda:0')
rewards[i]: 0.4865
values[i+1]: tensor([[0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0076]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.6005]], device='cuda:0')
delta_t: tensor([[0.4869]], device='cuda:0')
------ ------
policy_loss: 118.30425262451172
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.6005]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[138.2486]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[49.0693]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.0050]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.0120]], device='cuda:0')
------ ------
delta_t: tensor([[0.4705]], device='cuda:0')
rewards[i]: 0.47
values[i+1]: tensor([[0.0076]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0070]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.0050]], device='cuda:0')
delta_t: tensor([[0.4705]], device='cuda:0')
------ ------
policy_loss: 135.0733184814453
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.0050]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 135.0733184814453
value_loss: 138.24859619140625
loss: 204.19761657714844



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 8.4670e-05,  1.3617e-07, -6.1016e-07,  6.7623e-06,  5.0890e-07],
        [-1.0896e-02, -1.8211e-05, -1.1749e-05, -1.5630e-04, -5.6542e-05],
        [-3.4145e-04, -5.7888e-07, -9.4596e-07, -8.8481e-07, -1.7152e-06],
        [-1.9137e-02, -3.1537e-05, -1.3392e-06, -3.8413e-04, -1.0157e-04],
        [-4.0777e-02, -6.4746e-05,  4.5788e-05, -1.3007e-03, -2.2321e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-5.1410e-07, -8.6166e-08,  5.1941e-07, -1.7005e-08, -5.8574e-07],
        [ 7.2825e-05,  1.0979e-04, -6.1609e-06, -1.0859e-04,  6.8141e-05],
        [ 2.3150e-06,  3.9778e-06,  1.4524e-07, -4.0081e-06,  2.0880e-06],
        [ 1.2748e-04,  1.7932e-04, -1.9977e-05, -1.7532e-04,  1.2156e-04],
        [ 2.6802e-04,  3.1838e-04, -8.3980e-05, -3.0190e-04,  2.6518e-04]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.0172,  0.0164, -0.0172,  0.0139, -0.0069],
        [ 0.0270, -0.0259,  0.0269, -0.0219,  0.0128],
        [-0.0067,  0.0065, -0.0067,  0.0055, -0.0040],
        [ 0.0047, -0.0045,  0.0047, -0.0038,  0.0023],
        [-0.0078,  0.0075, -0.0078,  0.0064, -0.0042]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 5.5676e-03,  9.1082e-03,  4.6355e-04, -9.1805e-03,  4.9310e-03],
        [ 2.5418e-02,  4.1890e-02,  5.7422e-04, -4.2035e-02,  2.3034e-02],
        [-1.8729e-02, -3.0669e-02, -1.3948e-04,  3.0721e-02, -1.7110e-02],
        [-1.8588e-02, -3.0439e-02, -1.3843e-04,  3.0490e-02, -1.6982e-02],
        [ 2.0189e-02,  3.3279e-02, -2.0620e-04, -3.3258e-02,  1.8711e-02],
        [ 1.6125e-02,  2.6030e-02, -9.5966e-05, -2.6057e-02,  1.4701e-02],
        [-1.8747e-02, -3.0698e-02, -1.3961e-04,  3.0750e-02, -1.7126e-02],
        [ 2.9259e-02,  4.7660e-02,  8.8875e-05, -4.7761e-02,  2.6673e-02],
        [-1.8614e-02, -3.0481e-02, -1.3863e-04,  3.0533e-02, -1.7006e-02],
        [-1.8709e-02, -3.0636e-02, -1.3933e-04,  3.0688e-02, -1.7092e-02],
        [-3.1716e-03, -5.0432e-03, -1.2900e-04,  5.1110e-03, -2.7344e-03]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.1305,  1.8511,  0.0084, -1.8542,  1.0328]], device='cuda:0')
--> [loss] 204.19761657714844

---------------------------------- [[#6 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4546 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.8631611922208 acc: 0.4562
[Epoch 7] loss: 5.750382996276212 acc: 0.4884
[Epoch 11] loss: 5.056587485858547 acc: 0.5073
[Epoch 15] loss: 4.273211015002502 acc: 0.5049
[Epoch 19] loss: 3.280866405238276 acc: 0.4916
[Epoch 23] loss: 2.2458209101958655 acc: 0.4817
[Epoch 27] loss: 1.5182760266772926 acc: 0.4733
[Epoch 31] loss: 1.1060675728465894 acc: 0.474
[Epoch 35] loss: 0.9266813231817902 acc: 0.4716
[Epoch 39] loss: 0.8057793251255436 acc: 0.4655
[Epoch 43] loss: 0.7031477466035072 acc: 0.4724
[Epoch 47] loss: 0.6733811765270846 acc: 0.4648
[Epoch 51] loss: 0.59497914019772 acc: 0.4607
[Epoch 55] loss: 0.5951796192771105 acc: 0.4641
[Epoch 59] loss: 0.5659501365364512 acc: 0.4662
[Epoch 63] loss: 0.537140129605675 acc: 0.459
[Epoch 67] loss: 0.5191467997839536 acc: 0.4644
[Epoch 71] loss: 0.4822738157094592 acc: 0.4576
--> [test] acc: 0.4624
--> [accuracy] finished 0.4624
new state: tensor([608.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.4624
--> [reward] 0.4624
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.4624 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.888871643549341 acc: 0.4434
[Epoch 7] loss: 5.742075014602193 acc: 0.4802
[Epoch 11] loss: 5.044732684522029 acc: 0.4897
[Epoch 15] loss: 4.25478205214376 acc: 0.4961
[Epoch 19] loss: 3.2214788936288157 acc: 0.4911
[Epoch 23] loss: 2.1933392643776086 acc: 0.4782
[Epoch 27] loss: 1.4791988771208717 acc: 0.4756
[Epoch 31] loss: 1.1142172139623892 acc: 0.4696
[Epoch 35] loss: 0.8911655954826061 acc: 0.4762
[Epoch 39] loss: 0.7871732801732505 acc: 0.4692
[Epoch 43] loss: 0.7365898488522948 acc: 0.4718
[Epoch 47] loss: 0.6699785425916047 acc: 0.4616
[Epoch 51] loss: 0.6079103565391373 acc: 0.4627
[Epoch 55] loss: 0.5741157180169965 acc: 0.4627
[Epoch 59] loss: 0.5601048572965519 acc: 0.4656
[Epoch 63] loss: 0.511570773983036 acc: 0.4649
[Epoch 67] loss: 0.5083260089206649 acc: 0.4684
[Epoch 71] loss: 0.4723911713737318 acc: 0.4609
--> [test] acc: 0.4636
--> [accuracy] finished 0.4636
new state: tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.4636
--> [reward] 0.4636
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.4636 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([640.,   1.,   1.,   6.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.656098876767756 acc: 0.4551
[Epoch 7] loss: 5.373415568905413 acc: 0.4964
[Epoch 11] loss: 4.589426056808218 acc: 0.5179
[Epoch 15] loss: 3.7280158100225735 acc: 0.5288
[Epoch 19] loss: 2.6777202887150944 acc: 0.5154
[Epoch 23] loss: 1.704934439352711 acc: 0.5043
[Epoch 27] loss: 1.1251169275063688 acc: 0.5013
[Epoch 31] loss: 0.8524204813267874 acc: 0.4891
[Epoch 35] loss: 0.6951856407363092 acc: 0.4956
[Epoch 39] loss: 0.6178615825045901 acc: 0.4913
[Epoch 43] loss: 0.5747797885657195 acc: 0.4881
[Epoch 47] loss: 0.5097365306066278 acc: 0.4927
[Epoch 51] loss: 0.5098321239137665 acc: 0.4876
[Epoch 55] loss: 0.467205791450713 acc: 0.4972
[Epoch 59] loss: 0.4517113994759367 acc: 0.489
[Epoch 63] loss: 0.43615751463891295 acc: 0.4905
[Epoch 67] loss: 0.39954369367264647 acc: 0.4862
[Epoch 71] loss: 0.41555466436211713 acc: 0.4938
--> [test] acc: 0.4874
--> [accuracy] finished 0.4874
new state: tensor([640.,   1.,   1.,   6.,   4.], device='cuda:0')
new reward: 0.4874
--> [reward] 0.4874
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     4.0     | 0.4874 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.694054134361579 acc: 0.4621
[Epoch 7] loss: 5.443129232777354 acc: 0.5053
[Epoch 11] loss: 4.6565509808947665 acc: 0.5226
[Epoch 15] loss: 3.7765241390299003 acc: 0.5136
[Epoch 19] loss: 2.71391096634938 acc: 0.515
[Epoch 23] loss: 1.74699267546844 acc: 0.4992
[Epoch 27] loss: 1.1786957017296111 acc: 0.4984
[Epoch 31] loss: 0.894591728897046 acc: 0.5005
[Epoch 35] loss: 0.7375909001721294 acc: 0.4928
[Epoch 39] loss: 0.6697691749600346 acc: 0.4948
[Epoch 43] loss: 0.5873353562086744 acc: 0.4898
[Epoch 47] loss: 0.5542025550999834 acc: 0.4913
[Epoch 51] loss: 0.5258374823843275 acc: 0.4951
[Epoch 55] loss: 0.5027827207484971 acc: 0.4925
[Epoch 59] loss: 0.46743078611295696 acc: 0.491
[Epoch 63] loss: 0.45869922666522245 acc: 0.4864
[Epoch 67] loss: 0.41946746202190516 acc: 0.4835
[Epoch 71] loss: 0.416346999899963 acc: 0.485
--> [test] acc: 0.4984
--> [accuracy] finished 0.4984
new state: tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
new reward: 0.4984
--> [reward] 0.4984
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     3.0     | 0.4984 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.691201070080632 acc: 0.4614
[Epoch 7] loss: 5.455377680871188 acc: 0.5095
[Epoch 11] loss: 4.676981474280052 acc: 0.51
[Epoch 15] loss: 3.823762943982468 acc: 0.5216
[Epoch 19] loss: 2.796965697461077 acc: 0.5182
[Epoch 23] loss: 1.8206424303067006 acc: 0.4974
[Epoch 27] loss: 1.2249298550360037 acc: 0.4962
[Epoch 31] loss: 0.8991546663062652 acc: 0.4894
[Epoch 35] loss: 0.7700523938936041 acc: 0.4802
[Epoch 39] loss: 0.6619704887747307 acc: 0.4856
[Epoch 43] loss: 0.5856775748388617 acc: 0.4828
[Epoch 47] loss: 0.566679735799008 acc: 0.4813
[Epoch 51] loss: 0.5108175793534044 acc: 0.4942
[Epoch 55] loss: 0.4949331601124133 acc: 0.4792
[Epoch 59] loss: 0.47733142337930934 acc: 0.4868
[Epoch 63] loss: 0.4490664652727373 acc: 0.4871
[Epoch 67] loss: 0.43140246567156765 acc: 0.4877
[Epoch 71] loss: 0.4030286523887454 acc: 0.4865
--> [test] acc: 0.4805
--> [accuracy] finished 0.4805
new state: tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
new reward: 0.4805
--> [reward] 0.4805
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     3.0     | 0.4805 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.655068225263025 acc: 0.4649
[Epoch 7] loss: 5.416128288449534 acc: 0.5069
[Epoch 11] loss: 4.600760034587987 acc: 0.5208
[Epoch 15] loss: 3.7205143866636563 acc: 0.515
[Epoch 19] loss: 2.649983329617459 acc: 0.5118
[Epoch 23] loss: 1.68321967917635 acc: 0.5041
[Epoch 27] loss: 1.1268108844223534 acc: 0.492
[Epoch 31] loss: 0.8522011603007231 acc: 0.4975
[Epoch 35] loss: 0.7297884278151843 acc: 0.4862
[Epoch 39] loss: 0.6338639854622619 acc: 0.4975
[Epoch 43] loss: 0.5882069106973574 acc: 0.4837
[Epoch 47] loss: 0.5499117089878491 acc: 0.4904
[Epoch 51] loss: 0.4926862177174643 acc: 0.4781
[Epoch 55] loss: 0.4769984919892248 acc: 0.4934
[Epoch 59] loss: 0.46588861782227636 acc: 0.4894
[Epoch 63] loss: 0.4431835020061039 acc: 0.4843
[Epoch 67] loss: 0.39205461785749857 acc: 0.4808
[Epoch 71] loss: 0.41600111167392007 acc: 0.4865
--> [test] acc: 0.4939
--> [accuracy] finished 0.4939
new state: tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
new reward: 0.4939
--> [reward] 0.4939
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     3.0     | 0.4939 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([672.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.698513067896713 acc: 0.4665
[Epoch 7] loss: 5.4442436890224055 acc: 0.5112
[Epoch 11] loss: 4.6600083051740055 acc: 0.5188
[Epoch 15] loss: 3.795580949624786 acc: 0.5142
[Epoch 19] loss: 2.7489767611941414 acc: 0.5184
[Epoch 23] loss: 1.7832221548499354 acc: 0.5089
[Epoch 27] loss: 1.1954274805610443 acc: 0.4902
[Epoch 31] loss: 0.9188593462814608 acc: 0.4941
[Epoch 35] loss: 0.7680047007034654 acc: 0.5017
[Epoch 39] loss: 0.6604592321950304 acc: 0.4926
[Epoch 43] loss: 0.6013185706208733 acc: 0.4885
[Epoch 47] loss: 0.5544043896372056 acc: 0.486
[Epoch 51] loss: 0.5290434378558946 acc: 0.4854
[Epoch 55] loss: 0.5009925722208856 acc: 0.4822
[Epoch 59] loss: 0.4626843813863938 acc: 0.4832
[Epoch 63] loss: 0.4457337784454646 acc: 0.4828
[Epoch 67] loss: 0.4484573632164303 acc: 0.4977
[Epoch 71] loss: 0.40717367396530363 acc: 0.4767
--> [test] acc: 0.4837
--> [accuracy] finished 0.4837
new state: tensor([672.,   1.,   1.,   6.,   3.], device='cuda:0')
new reward: 0.4837
--> [reward] 0.4837
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     6.0      |     3.0     | 0.4837 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([672.,   1.,   2.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.261165930487006 acc: 0.4898
[Epoch 7] loss: 4.827601725792946 acc: 0.5642
[Epoch 11] loss: 3.7500294010962367 acc: 0.5784
[Epoch 15] loss: 2.526527200056159 acc: 0.559
[Epoch 19] loss: 1.459070905395176 acc: 0.5619
[Epoch 23] loss: 0.8912620119121678 acc: 0.5484
[Epoch 27] loss: 0.6390792886581262 acc: 0.5468
[Epoch 31] loss: 0.5373876060042387 acc: 0.5371
[Epoch 35] loss: 0.47461471302182323 acc: 0.5543
[Epoch 39] loss: 0.43721975660537515 acc: 0.55
[Epoch 43] loss: 0.3821963421938479 acc: 0.5507
[Epoch 47] loss: 0.37440169112199484 acc: 0.5394
[Epoch 51] loss: 0.3287372189214277 acc: 0.5519
[Epoch 55] loss: 0.336722641423478 acc: 0.5448
[Epoch 59] loss: 0.2946724842023819 acc: 0.5489
[Epoch 63] loss: 0.30231219145190685 acc: 0.5491
[Epoch 67] loss: 0.29609074078195385 acc: 0.5494
[Epoch 71] loss: 0.2673028684542764 acc: 0.5443
--> [test] acc: 0.5491
--> [accuracy] finished 0.5491
new state: tensor([672.,   1.,   2.,   6.,   3.], device='cuda:0')
new reward: 0.5491
--> [reward] 0.5491
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     3.0     | 0.5491 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.887008446866594 acc: 0.5425
[Epoch 7] loss: 4.250819516456341 acc: 0.6116
[Epoch 11] loss: 2.9481218040294355 acc: 0.6208
[Epoch 15] loss: 1.6268931094490353 acc: 0.6077
[Epoch 19] loss: 0.8628862521627827 acc: 0.6069
[Epoch 23] loss: 0.5920023329012916 acc: 0.5981
[Epoch 27] loss: 0.45745594075897617 acc: 0.5945
[Epoch 31] loss: 0.4001894539407909 acc: 0.5925
[Epoch 35] loss: 0.362221986248308 acc: 0.596
[Epoch 39] loss: 0.3274009287276346 acc: 0.5932
[Epoch 43] loss: 0.31514303694429147 acc: 0.5978
[Epoch 47] loss: 0.2938011372795381 acc: 0.6014
[Epoch 51] loss: 0.2649627640162168 acc: 0.5997
[Epoch 55] loss: 0.2662477363710818 acc: 0.5992
[Epoch 59] loss: 0.23505903977443418 acc: 0.5977
[Epoch 63] loss: 0.25906135316085444 acc: 0.5912
[Epoch 67] loss: 0.22327884349047833 acc: 0.5958
[Epoch 71] loss: 0.22076665412615556 acc: 0.5917
--> [test] acc: 0.5998
--> [accuracy] finished 0.5998
new state: tensor([672.,   1.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.5998
--> [reward] 0.5998
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     2.0     | 0.5998 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.88174160697576 acc: 0.4963
[Epoch 7] loss: 4.2588351100607 acc: 0.6133
[Epoch 11] loss: 2.9487016124036307 acc: 0.6114
[Epoch 15] loss: 1.624195940544843 acc: 0.6014
[Epoch 19] loss: 0.8433379407238473 acc: 0.6088
[Epoch 23] loss: 0.5778896037270042 acc: 0.5971
[Epoch 27] loss: 0.45806471333669885 acc: 0.6084
[Epoch 31] loss: 0.40754547344563563 acc: 0.6043
[Epoch 35] loss: 0.35052849422863985 acc: 0.5945
[Epoch 39] loss: 0.32554212223042917 acc: 0.5998
[Epoch 43] loss: 0.3042722683890587 acc: 0.5921
[Epoch 47] loss: 0.2833930544979165 acc: 0.5954
[Epoch 51] loss: 0.28063138783373454 acc: 0.5948
[Epoch 55] loss: 0.2574314648223579 acc: 0.5837
[Epoch 59] loss: 0.24725681166771962 acc: 0.597
[Epoch 63] loss: 0.22770450806931194 acc: 0.5924
[Epoch 67] loss: 0.2231813554516267 acc: 0.6006
[Epoch 71] loss: 0.2066300818720437 acc: 0.6056
--> [test] acc: 0.5962
--> [accuracy] finished 0.5962
new state: tensor([672.,   1.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.5962
--> [reward] 0.5962
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     2.0     | 0.5962 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.816872868391559 acc: 0.5548
[Epoch 7] loss: 4.157175721872188 acc: 0.6161
[Epoch 11] loss: 2.777848769026949 acc: 0.6236
[Epoch 15] loss: 1.444007821476368 acc: 0.6101
[Epoch 19] loss: 0.747781771871135 acc: 0.6057
[Epoch 23] loss: 0.5237954569708966 acc: 0.601
[Epoch 27] loss: 0.42733844541146626 acc: 0.6022
[Epoch 31] loss: 0.36467054979327845 acc: 0.6028
[Epoch 35] loss: 0.3387949578511669 acc: 0.6053
[Epoch 39] loss: 0.2917407924056892 acc: 0.5969
[Epoch 43] loss: 0.29049286986832196 acc: 0.6118
[Epoch 47] loss: 0.27551919634184796 acc: 0.5969
[Epoch 51] loss: 0.25691749141706377 acc: 0.6012
[Epoch 55] loss: 0.23467149284413402 acc: 0.6075
[Epoch 59] loss: 0.232479681351396 acc: 0.6007
[Epoch 63] loss: 0.22160949459289916 acc: 0.603
[Epoch 67] loss: 0.19583863380800962 acc: 0.6084
[Epoch 71] loss: 0.20798906601508102 acc: 0.5995
--> [test] acc: 0.5907
--> [accuracy] finished 0.5907
new state: tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
new reward: 0.5907
--> [reward] 0.5907
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     1.0     | 0.5907 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.8783257870418035 acc: 0.5519
[Epoch 7] loss: 4.213473301257014 acc: 0.6087
[Epoch 11] loss: 2.802012672052359 acc: 0.6218
[Epoch 15] loss: 1.4189306682409228 acc: 0.6025
[Epoch 19] loss: 0.7559795826959336 acc: 0.6129
[Epoch 23] loss: 0.5219056501108057 acc: 0.5983
[Epoch 27] loss: 0.41743875977099704 acc: 0.6016
[Epoch 31] loss: 0.36975453361449645 acc: 0.6006
[Epoch 35] loss: 0.32819416140303814 acc: 0.5909
[Epoch 39] loss: 0.31833811905568515 acc: 0.6035
[Epoch 43] loss: 0.2766936584721174 acc: 0.5927
[Epoch 47] loss: 0.2713724385442026 acc: 0.593
[Epoch 51] loss: 0.24675602726685955 acc: 0.6015
[Epoch 55] loss: 0.25136580568907396 acc: 0.5922
[Epoch 59] loss: 0.22293250144833265 acc: 0.5918
[Epoch 63] loss: 0.21586681010625552 acc: 0.5949
[Epoch 67] loss: 0.2190361039503418 acc: 0.5957
[Epoch 71] loss: 0.20198786233211188 acc: 0.5955
--> [test] acc: 0.5997
--> [accuracy] finished 0.5997
new state: tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
new reward: 0.5997
--> [reward] 0.5997
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     1.0     | 0.5997 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.830807070597968 acc: 0.5454
[Epoch 7] loss: 4.151452266651651 acc: 0.6178
[Epoch 11] loss: 2.792480642685805 acc: 0.6123
[Epoch 15] loss: 1.434234545930572 acc: 0.5981
[Epoch 19] loss: 0.7635481739631089 acc: 0.6002
[Epoch 23] loss: 0.515849443528887 acc: 0.6047
[Epoch 27] loss: 0.43324710632128943 acc: 0.5984
[Epoch 31] loss: 0.3633274988859625 acc: 0.594
[Epoch 35] loss: 0.3342089110442325 acc: 0.5894
[Epoch 39] loss: 0.3080582961968868 acc: 0.5989
[Epoch 43] loss: 0.2913135024897583 acc: 0.5862
[Epoch 47] loss: 0.2685667149367673 acc: 0.5902
[Epoch 51] loss: 0.2513177582012761 acc: 0.5973
[Epoch 55] loss: 0.23305519847580425 acc: 0.5982
[Epoch 59] loss: 0.23075074352481215 acc: 0.5946
[Epoch 63] loss: 0.2228313887687138 acc: 0.5904
[Epoch 67] loss: 0.20458429185387766 acc: 0.5897
[Epoch 71] loss: 0.22298713399292638 acc: 0.5968
--> [test] acc: 0.5953
--> [accuracy] finished 0.5953
new state: tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
new reward: 0.5953
--> [reward] 0.5953
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     1.0     | 0.5953 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.862823150956722 acc: 0.5528
[Epoch 7] loss: 4.213179808138581 acc: 0.613
[Epoch 11] loss: 2.9696741002752347 acc: 0.6237
[Epoch 15] loss: 1.7191949746264217 acc: 0.6101
[Epoch 19] loss: 0.9325841777121929 acc: 0.6102
[Epoch 23] loss: 0.6028534312163244 acc: 0.6027
[Epoch 27] loss: 0.5063425029396934 acc: 0.601
[Epoch 31] loss: 0.427071574294125 acc: 0.5988
[Epoch 35] loss: 0.3917062233780961 acc: 0.5952
[Epoch 39] loss: 0.33940436143447145 acc: 0.5974
[Epoch 43] loss: 0.32954597547936165 acc: 0.5955
[Epoch 47] loss: 0.3163802526281465 acc: 0.5945
[Epoch 51] loss: 0.2984554611332238 acc: 0.5907
[Epoch 55] loss: 0.26446579748054827 acc: 0.5934
[Epoch 59] loss: 0.2717827812904287 acc: 0.5993
[Epoch 63] loss: 0.2613252849768266 acc: 0.5902
[Epoch 67] loss: 0.2533052560809018 acc: 0.5966
[Epoch 71] loss: 0.23800487632689346 acc: 0.593
--> [test] acc: 0.5951
--> [accuracy] finished 0.5951
new state: tensor([672.,   1.,   1.,   6.,   1.], device='cuda:0')
new reward: 0.5951
--> [reward] 0.5951
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     6.0      |     1.0     | 0.5951 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0916, 0.0910, 0.0902, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3972, -2.3978, -2.3986, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.8809228787946575 acc: 0.5461
[Epoch 7] loss: 4.208580695149844 acc: 0.6139
[Epoch 11] loss: 2.83401124945382 acc: 0.611
[Epoch 15] loss: 1.477031756468746 acc: 0.6025
[Epoch 19] loss: 0.7770976936516097 acc: 0.6007
[Epoch 23] loss: 0.5181091434805823 acc: 0.6059
[Epoch 27] loss: 0.4350765538461449 acc: 0.6065
[Epoch 31] loss: 0.37739972447704934 acc: 0.6062
[Epoch 35] loss: 0.3334225200145217 acc: 0.5976
[Epoch 39] loss: 0.314735155331109 acc: 0.5956
[Epoch 43] loss: 0.28431519428672997 acc: 0.6036
[Epoch 47] loss: 0.27658851696726156 acc: 0.5947
[Epoch 51] loss: 0.2630532877352994 acc: 0.6014
[Epoch 55] loss: 0.22831262776489986 acc: 0.6022
[Epoch 59] loss: 0.24467470842744687 acc: 0.5987
[Epoch 63] loss: 0.21797204145547144 acc: 0.5937
[Epoch 67] loss: 0.2244353716022304 acc: 0.6012
[Epoch 71] loss: 0.1957913629860734 acc: 0.6035
--> [test] acc: 0.5947
--> [accuracy] finished 0.5947
new state: tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
new reward: 0.5947
--> [reward] 0.5947
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1767]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3535]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.5945]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6073]], device='cuda:0')
------ ------
delta_t: tensor([[0.5945]], device='cuda:0')
rewards[i]: 0.5947
values[i+1]: tensor([[0.0127]], device='cuda:0')
values[i]: tensor([[0.0128]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.5945]], device='cuda:0')
delta_t: tensor([[0.5945]], device='cuda:0')
------ ------
policy_loss: 1.4016063213348389
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.5945]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.8774]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.4014]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.1838]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.1963]], device='cuda:0')
------ ------
delta_t: tensor([[0.5952]], device='cuda:0')
rewards[i]: 0.5951
values[i+1]: tensor([[0.0128]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0125]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.1838]], device='cuda:0')
delta_t: tensor([[0.5952]], device='cuda:0')
------ ------
policy_loss: 4.215927600860596
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.1838]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.4390]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.1230]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.7672]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.7797]], device='cuda:0')
------ ------
delta_t: tensor([[0.5952]], device='cuda:0')
rewards[i]: 0.5953
values[i+1]: tensor([[0.0125]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0124]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.7672]], device='cuda:0')
delta_t: tensor([[0.5952]], device='cuda:0')
------ ------
policy_loss: 8.42944049835205
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.7672]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.1983]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.5188]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.3492]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.3616]], device='cuda:0')
------ ------
delta_t: tensor([[0.5997]], device='cuda:0')
rewards[i]: 0.5997
values[i+1]: tensor([[0.0124]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0124]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.3492]], device='cuda:0')
delta_t: tensor([[0.5997]], device='cuda:0')
------ ------
policy_loss: 14.038451194763184
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.3492]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[9.4510]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.5053]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9164]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.9286]], device='cuda:0')
------ ------
delta_t: tensor([[0.5907]], device='cuda:0')
rewards[i]: 0.5907
values[i+1]: tensor([[0.0124]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0123]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9164]], device='cuda:0')
delta_t: tensor([[0.5907]], device='cuda:0')
------ ------
policy_loss: 21.009267807006836
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9164]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[15.5179]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.1337]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.4834]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.4956]], device='cuda:0')
------ ------
delta_t: tensor([[0.5961]], device='cuda:0')
rewards[i]: 0.5962
values[i+1]: tensor([[0.0123]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0122]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.4833]], device='cuda:0')
delta_t: tensor([[0.5961]], device='cuda:0')
------ ------
policy_loss: 29.337772369384766
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.4833]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[23.7118]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.3878]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.0482]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.0604]], device='cuda:0')
------ ------
delta_t: tensor([[0.5997]], device='cuda:0')
rewards[i]: 0.5998
values[i+1]: tensor([[0.0122]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0122]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.0482]], device='cuda:0')
delta_t: tensor([[0.5997]], device='cuda:0')
------ ------
policy_loss: 39.02313232421875
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.0482]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[34.0922]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[20.7608]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.5564]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.5689]], device='cuda:0')
------ ------
delta_t: tensor([[0.5487]], device='cuda:0')
rewards[i]: 0.5491
values[i+1]: tensor([[0.0122]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0125]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.5564]], device='cuda:0')
delta_t: tensor([[0.5487]], device='cuda:0')
------ ------
policy_loss: 49.92477035522461
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.5564]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[46.5639]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.9433]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9943]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0069]], device='cuda:0')
------ ------
delta_t: tensor([[0.4835]], device='cuda:0')
rewards[i]: 0.4837
values[i+1]: tensor([[0.0125]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0126]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9943]], device='cuda:0')
delta_t: tensor([[0.4835]], device='cuda:0')
------ ------
policy_loss: 61.8731575012207
log_probs[i]: tensor([[-2.3972]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9943]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[61.3514]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.5750]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4383]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4507]], device='cuda:0')
------ ------
delta_t: tensor([[0.4939]], device='cuda:0')
rewards[i]: 0.4939
values[i+1]: tensor([[0.0126]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0125]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4383]], device='cuda:0')
delta_t: tensor([[0.4939]], device='cuda:0')
------ ------
policy_loss: 74.88928985595703
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4383]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[78.5473]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.3918]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8645]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.8767]], device='cuda:0')
------ ------
delta_t: tensor([[0.4805]], device='cuda:0')
rewards[i]: 0.4805
values[i+1]: tensor([[0.0125]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0123]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8645]], device='cuda:0')
delta_t: tensor([[0.4805]], device='cuda:0')
------ ------
policy_loss: 88.92729187011719
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8645]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[98.4192]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[39.7438]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3043]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.3164]], device='cuda:0')
------ ------
delta_t: tensor([[0.4985]], device='cuda:0')
rewards[i]: 0.4984
values[i+1]: tensor([[0.0123]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0121]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3043]], device='cuda:0')
delta_t: tensor([[0.4985]], device='cuda:0')
------ ------
policy_loss: 104.02369689941406
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3043]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[121.0559]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.2734]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7286]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7406]], device='cuda:0')
------ ------
delta_t: tensor([[0.4873]], device='cuda:0')
rewards[i]: 0.4874
values[i+1]: tensor([[0.0121]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0121]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7285]], device='cuda:0')
delta_t: tensor([[0.4873]], device='cuda:0')
------ ------
policy_loss: 120.13348388671875
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7285]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[146.4380]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[50.7643]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.1249]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1368]], device='cuda:0')
------ ------
delta_t: tensor([[0.4636]], device='cuda:0')
rewards[i]: 0.4636
values[i+1]: tensor([[0.0121]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0119]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.1249]], device='cuda:0')
delta_t: tensor([[0.4636]], device='cuda:0')
------ ------
policy_loss: 137.18934631347656
log_probs[i]: tensor([[-2.3972]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.1249]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[174.6881]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.5002]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.5167]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5278]], device='cuda:0')
------ ------
delta_t: tensor([[0.4630]], device='cuda:0')
rewards[i]: 0.4624
values[i+1]: tensor([[0.0119]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0112]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.5167]], device='cuda:0')
delta_t: tensor([[0.4630]], device='cuda:0')
------ ------
policy_loss: 155.19032287597656
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.5167]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 155.19032287597656
value_loss: 174.6881103515625
loss: 242.5343780517578



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.2787e-03, -2.1523e-06, -2.1879e-06, -1.2339e-05, -6.8392e-06],
        [ 7.8397e-03,  1.4511e-05,  1.1133e-05,  7.1704e-05,  4.7990e-05],
        [-6.0433e-04, -9.8257e-07, -1.0787e-06, -6.0329e-06, -3.0981e-06],
        [ 3.2497e-02,  5.7819e-05,  4.9940e-05,  3.0894e-04,  1.8863e-04],
        [ 1.2876e-01,  2.3142e-04,  1.6902e-04,  1.2624e-03,  8.1850e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 8.5274e-06,  1.4153e-05,  6.1980e-08, -1.4156e-05,  7.8257e-06],
        [-5.1669e-05, -9.3250e-05, -2.8364e-06,  9.2810e-05, -4.8540e-05],
        [ 4.0496e-06,  6.5167e-06, -3.8099e-08, -6.5299e-06,  3.6850e-06],
        [-2.1546e-04, -3.7508e-04, -7.2872e-06,  3.7407e-04, -2.0031e-04],
        [-8.5319e-04, -1.4894e-03, -2.9213e-05,  1.4834e-03, -7.9399e-04]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.0343, -0.0326,  0.0342, -0.0268,  0.0156],
        [ 0.0090, -0.0090,  0.0090, -0.0084,  0.0054],
        [-0.0292,  0.0280, -0.0291,  0.0238, -0.0142],
        [ 0.0067, -0.0065,  0.0067, -0.0057,  0.0035],
        [-0.0209,  0.0201, -0.0208,  0.0172, -0.0103]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-2.3458e-02, -3.6240e-02,  7.1640e-04,  3.6416e-02, -2.1144e-02],
        [ 2.3877e-02,  3.6783e-02, -9.1453e-04, -3.6771e-02,  2.1494e-02],
        [ 4.5596e-02,  7.0121e-02, -1.5260e-03, -7.0654e-02,  4.1095e-02],
        [-2.3208e-02, -3.5854e-02,  7.0877e-04,  3.6028e-02, -2.0918e-02],
        [-1.8537e-02, -2.8738e-02,  6.0123e-04,  2.8788e-02, -1.6731e-02],
        [-2.2815e-03, -3.8162e-03,  9.2403e-05,  3.7189e-03, -2.1751e-03],
        [ 2.6659e-03,  4.2430e-03, -1.6126e-04, -4.0338e-03,  2.4612e-03],
        [-2.3377e-02, -3.6115e-02,  7.1391e-04,  3.6290e-02, -2.1070e-02],
        [ 2.9742e-02,  4.6087e-02, -9.3834e-04, -4.6286e-02,  2.6837e-02],
        [ 5.0421e-03,  8.4671e-03,  1.6671e-04, -8.4269e-03,  4.6548e-03],
        [-1.6060e-02, -2.4937e-02,  5.4068e-04,  2.4932e-02, -1.4503e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.4154,  2.1867, -0.0432, -2.1973,  1.2758]], device='cuda:0')
--> [loss] 242.5343780517578

---------------------------------- [[#7 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     6.0      |     1.0     | 0.5947 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0903,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.889192325074959 acc: 0.5518
[Epoch 7] loss: 4.24271591453601 acc: 0.6199
[Epoch 11] loss: 2.9935800951460134 acc: 0.6138
[Epoch 15] loss: 1.7422976884085808 acc: 0.6075
[Epoch 19] loss: 0.9297182365032413 acc: 0.5899
[Epoch 23] loss: 0.6023797422857083 acc: 0.605
[Epoch 27] loss: 0.4942154346362633 acc: 0.6052
[Epoch 31] loss: 0.42907188988297873 acc: 0.6022
[Epoch 35] loss: 0.3946437217804896 acc: 0.6005
[Epoch 39] loss: 0.33992164584872364 acc: 0.5939
[Epoch 43] loss: 0.33811555070864496 acc: 0.5969
[Epoch 47] loss: 0.2952673433448576 acc: 0.5943
[Epoch 51] loss: 0.27878737259927727 acc: 0.5954
[Epoch 55] loss: 0.2779891606839493 acc: 0.6031
[Epoch 59] loss: 0.267836449346255 acc: 0.5959
[Epoch 63] loss: 0.24551274541579662 acc: 0.5949
[Epoch 67] loss: 0.24283116789834808 acc: 0.5925
[Epoch 71] loss: 0.22236016378297335 acc: 0.5969
--> [test] acc: 0.59
--> [accuracy] finished 0.59
new state: tensor([672.,   1.,   1.,   6.,   1.], device='cuda:0')
new reward: 0.59
--> [reward] 0.59
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     6.0      |     1.0     |  0.59  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0903,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([640.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.908796386950461 acc: 0.5335
[Epoch 7] loss: 4.249897736722551 acc: 0.6136
[Epoch 11] loss: 2.9849439708473127 acc: 0.6177
[Epoch 15] loss: 1.6931231386597505 acc: 0.6128
[Epoch 19] loss: 0.8871120545260437 acc: 0.5984
[Epoch 23] loss: 0.611183934523474 acc: 0.6008
[Epoch 27] loss: 0.48725839951039884 acc: 0.6036
[Epoch 31] loss: 0.4327683976787092 acc: 0.5984
[Epoch 35] loss: 0.38050476762244617 acc: 0.5955
[Epoch 39] loss: 0.3506701193092501 acc: 0.593
[Epoch 43] loss: 0.3311532646026986 acc: 0.594
[Epoch 47] loss: 0.31921103426857905 acc: 0.599
[Epoch 51] loss: 0.2919658432140603 acc: 0.6006
[Epoch 55] loss: 0.28555933417766677 acc: 0.5868
[Epoch 59] loss: 0.25758222975861045 acc: 0.5896
[Epoch 63] loss: 0.26460406611032805 acc: 0.5997
[Epoch 67] loss: 0.22557158192233814 acc: 0.5983
[Epoch 71] loss: 0.2363054201511852 acc: 0.5966
--> [test] acc: 0.5945
--> [accuracy] finished 0.5945
new state: tensor([640.,   1.,   1.,   6.,   1.], device='cuda:0')
new reward: 0.5945
--> [reward] 0.5945
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     1.0     | 0.5945 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0903,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([640.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.859843962637664 acc: 0.5465
[Epoch 7] loss: 4.20364377763875 acc: 0.6248
[Epoch 11] loss: 2.960761061486076 acc: 0.6111
[Epoch 15] loss: 1.6989152795061127 acc: 0.613
[Epoch 19] loss: 0.911710269101288 acc: 0.611
[Epoch 23] loss: 0.5919269785818542 acc: 0.5993
[Epoch 27] loss: 0.5008079674228302 acc: 0.6025
[Epoch 31] loss: 0.4271349208286542 acc: 0.5981
[Epoch 35] loss: 0.3736660925152204 acc: 0.597
[Epoch 39] loss: 0.34672242399695735 acc: 0.5996
[Epoch 43] loss: 0.33406515503802414 acc: 0.6092
[Epoch 47] loss: 0.3051396640186744 acc: 0.5941
[Epoch 51] loss: 0.282132489254217 acc: 0.6062
[Epoch 55] loss: 0.2780014009207792 acc: 0.6055
[Epoch 59] loss: 0.2507455825312611 acc: 0.5969
[Epoch 63] loss: 0.2736078954844848 acc: 0.6063
[Epoch 67] loss: 0.22253317483093427 acc: 0.6087
[Epoch 71] loss: 0.24093109649865677 acc: 0.5937
--> [test] acc: 0.5989
--> [accuracy] finished 0.5989
new state: tensor([640.,   1.,   1.,   6.,   1.], device='cuda:0')
new reward: 0.5989
--> [reward] 0.5989
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     1.0     | 0.5989 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0903,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([640.,   1.,   1.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.17534747940805 acc: 0.5172
[Epoch 7] loss: 4.693400009361374 acc: 0.5846
[Epoch 11] loss: 3.644783673993767 acc: 0.5847
[Epoch 15] loss: 2.501141432224942 acc: 0.5827
[Epoch 19] loss: 1.469279840721956 acc: 0.5703
[Epoch 23] loss: 0.8836873862940027 acc: 0.5658
[Epoch 27] loss: 0.6681033859739218 acc: 0.5569
[Epoch 31] loss: 0.5563272086050733 acc: 0.5646
[Epoch 35] loss: 0.47585292568292153 acc: 0.5581
[Epoch 39] loss: 0.46116313025536365 acc: 0.5545
[Epoch 43] loss: 0.4180887211947833 acc: 0.5554
[Epoch 47] loss: 0.3902962587040175 acc: 0.5685
[Epoch 51] loss: 0.3665883461813754 acc: 0.5572
[Epoch 55] loss: 0.35220332382737524 acc: 0.5636
[Epoch 59] loss: 0.32525867013894305 acc: 0.5661
[Epoch 63] loss: 0.3173403039222102 acc: 0.5596
[Epoch 67] loss: 0.3123846610846079 acc: 0.5541
[Epoch 71] loss: 0.2872870734004337 acc: 0.5606
--> [test] acc: 0.5611
--> [accuracy] finished 0.5611
new state: tensor([640.,   1.,   1.,   6.,   2.], device='cuda:0')
new reward: 0.5611
--> [reward] 0.5611
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     6.0      |     2.0     | 0.5611 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0903,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   1.,   1.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.151700076544681 acc: 0.5168
[Epoch 7] loss: 4.669950861912554 acc: 0.5742
[Epoch 11] loss: 3.6337889559433587 acc: 0.5864
[Epoch 15] loss: 2.4866311319953645 acc: 0.5766
[Epoch 19] loss: 1.4228948336809188 acc: 0.5747
[Epoch 23] loss: 0.8688580254878839 acc: 0.5684
[Epoch 27] loss: 0.657290438556915 acc: 0.5644
[Epoch 31] loss: 0.5466916159109767 acc: 0.5589
[Epoch 35] loss: 0.49027986242376326 acc: 0.5688
[Epoch 39] loss: 0.4407500241861662 acc: 0.558
[Epoch 43] loss: 0.39994974884078327 acc: 0.5623
[Epoch 47] loss: 0.37842063503601897 acc: 0.5514
[Epoch 51] loss: 0.3768520140401123 acc: 0.5651
[Epoch 55] loss: 0.3453826890529498 acc: 0.565
[Epoch 59] loss: 0.32780823859688646 acc: 0.5528
[Epoch 63] loss: 0.30918071417571485 acc: 0.5644
[Epoch 67] loss: 0.3058590003934777 acc: 0.5549
[Epoch 71] loss: 0.2782237041548676 acc: 0.5539
--> [test] acc: 0.5564
--> [accuracy] finished 0.5564
new state: tensor([608.,   1.,   1.,   6.,   2.], device='cuda:0')
new reward: 0.5564
--> [reward] 0.5564
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     6.0      |     2.0     | 0.5564 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([608.,   1.,   1.,   7.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.456130138748442 acc: 0.4842
[Epoch 7] loss: 5.1504273010641715 acc: 0.5307
[Epoch 11] loss: 4.196851115092597 acc: 0.5483
[Epoch 15] loss: 3.0929971908212015 acc: 0.544
[Epoch 19] loss: 1.904695173930329 acc: 0.526
[Epoch 23] loss: 1.1593573851429897 acc: 0.5269
[Epoch 27] loss: 0.8520272290405563 acc: 0.5243
[Epoch 31] loss: 0.6792414494220863 acc: 0.5199
[Epoch 35] loss: 0.6051513006686783 acc: 0.518
[Epoch 39] loss: 0.5607725215261169 acc: 0.5158
[Epoch 43] loss: 0.510023099672802 acc: 0.5181
[Epoch 47] loss: 0.4781608638470359 acc: 0.5125
[Epoch 51] loss: 0.44034957721391144 acc: 0.512
[Epoch 55] loss: 0.43036705526568547 acc: 0.5246
[Epoch 59] loss: 0.38909760229003704 acc: 0.5194
[Epoch 63] loss: 0.3752912652138097 acc: 0.5149
[Epoch 67] loss: 0.3768622643478653 acc: 0.5168
[Epoch 71] loss: 0.3337885080967718 acc: 0.5241
--> [test] acc: 0.5166
--> [accuracy] finished 0.5166
new state: tensor([608.,   1.,   1.,   7.,   2.], device='cuda:0')
new reward: 0.5166
--> [reward] 0.5166
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     2.0     | 0.5166 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   1.,   1.,   7.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.513838168910093 acc: 0.4707
[Epoch 7] loss: 5.184929826985234 acc: 0.5293
[Epoch 11] loss: 4.21130604969571 acc: 0.5493
[Epoch 15] loss: 3.0870696714771984 acc: 0.5459
[Epoch 19] loss: 1.9249031808598878 acc: 0.5323
[Epoch 23] loss: 1.1671490807210088 acc: 0.5091
[Epoch 27] loss: 0.8511784967520962 acc: 0.5157
[Epoch 31] loss: 0.6962964546383189 acc: 0.5153
[Epoch 35] loss: 0.6079335656074231 acc: 0.5249
[Epoch 39] loss: 0.5498314793209743 acc: 0.5145
[Epoch 43] loss: 0.5067686763022791 acc: 0.5163
[Epoch 47] loss: 0.4829821734477187 acc: 0.512
[Epoch 51] loss: 0.4527594262014722 acc: 0.5217
[Epoch 55] loss: 0.4232487357662195 acc: 0.5242
[Epoch 59] loss: 0.3992660499280295 acc: 0.5148
[Epoch 63] loss: 0.39410651737200025 acc: 0.5132
[Epoch 67] loss: 0.3709663505453969 acc: 0.5232
[Epoch 71] loss: 0.339842165196243 acc: 0.5179
--> [test] acc: 0.5228
--> [accuracy] finished 0.5228
new state: tensor([640.,   1.,   1.,   7.,   2.], device='cuda:0')
new reward: 0.5228
--> [reward] 0.5228
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     2.0     | 0.5228 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   2.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.314244720941919 acc: 0.5138
[Epoch 7] loss: 4.81583737595307 acc: 0.5676
[Epoch 11] loss: 3.673581844248125 acc: 0.5691
[Epoch 15] loss: 2.363190154666486 acc: 0.5639
[Epoch 19] loss: 1.3424064010915244 acc: 0.5665
[Epoch 23] loss: 0.8500823922683973 acc: 0.5615
[Epoch 27] loss: 0.6728806025717798 acc: 0.5555
[Epoch 31] loss: 0.5447467159327415 acc: 0.5476
[Epoch 35] loss: 0.5011586773654689 acc: 0.5595
[Epoch 39] loss: 0.447422480915704 acc: 0.551
[Epoch 43] loss: 0.428174195751605 acc: 0.5466
[Epoch 47] loss: 0.38289778408549174 acc: 0.5529
[Epoch 51] loss: 0.36298205794009103 acc: 0.5501
[Epoch 55] loss: 0.353331641296444 acc: 0.5446
[Epoch 59] loss: 0.3282056776997264 acc: 0.556
[Epoch 63] loss: 0.3241271183342504 acc: 0.5439
[Epoch 67] loss: 0.3088891307306488 acc: 0.5411
[Epoch 71] loss: 0.29637482240939 acc: 0.5464
--> [test] acc: 0.5551
--> [accuracy] finished 0.5551
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5551
--> [reward] 0.5551
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5551 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.301237073700751 acc: 0.4985
[Epoch 7] loss: 4.790872321714221 acc: 0.5647
[Epoch 11] loss: 3.6101250637827627 acc: 0.5802
[Epoch 15] loss: 2.263095015150202 acc: 0.5674
[Epoch 19] loss: 1.252034022070258 acc: 0.5621
[Epoch 23] loss: 0.8057261256077101 acc: 0.5564
[Epoch 27] loss: 0.6246933260803942 acc: 0.5571
[Epoch 31] loss: 0.5316152487788588 acc: 0.5524
[Epoch 35] loss: 0.4692853928431678 acc: 0.5611
[Epoch 39] loss: 0.4297502659156423 acc: 0.557
[Epoch 43] loss: 0.40437572442538217 acc: 0.548
[Epoch 47] loss: 0.39575684367371794 acc: 0.5536
[Epoch 51] loss: 0.3495406169858773 acc: 0.5458
[Epoch 55] loss: 0.33491384434273175 acc: 0.5481
[Epoch 59] loss: 0.3114056887433809 acc: 0.5525
[Epoch 63] loss: 0.3054927085023707 acc: 0.5515
[Epoch 67] loss: 0.29666682684798834 acc: 0.5438
[Epoch 71] loss: 0.2916063919515752 acc: 0.551
--> [test] acc: 0.5447
--> [accuracy] finished 0.5447
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5447
--> [reward] 0.5447
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5447 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.352201734967244 acc: 0.5106
[Epoch 7] loss: 4.8414515086147185 acc: 0.5421
[Epoch 11] loss: 3.7058707532065602 acc: 0.5761
[Epoch 15] loss: 2.4488602399521167 acc: 0.5696
[Epoch 19] loss: 1.4198152081435904 acc: 0.5602
[Epoch 23] loss: 0.8828577007097966 acc: 0.5607
[Epoch 27] loss: 0.6727275513588925 acc: 0.5586
[Epoch 31] loss: 0.5732059868533745 acc: 0.5485
[Epoch 35] loss: 0.5111773905279996 acc: 0.5568
[Epoch 39] loss: 0.4470667498629264 acc: 0.5499
[Epoch 43] loss: 0.4053033185917932 acc: 0.5515
[Epoch 47] loss: 0.4169427721244295 acc: 0.5515
[Epoch 51] loss: 0.3713262974763351 acc: 0.545
[Epoch 55] loss: 0.3608714360743761 acc: 0.5405
[Epoch 59] loss: 0.3260912392883921 acc: 0.5389
[Epoch 63] loss: 0.33469940467363657 acc: 0.5453
[Epoch 67] loss: 0.2947657763114785 acc: 0.5456
[Epoch 71] loss: 0.2949866041865038 acc: 0.5404
--> [test] acc: 0.5436
--> [accuracy] finished 0.5436
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5436
--> [reward] 0.5436
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5436 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.292245434373236 acc: 0.4991
[Epoch 7] loss: 4.786339436955465 acc: 0.5612
[Epoch 11] loss: 3.6160393153005246 acc: 0.5747
[Epoch 15] loss: 2.300866054451984 acc: 0.5793
[Epoch 19] loss: 1.3001640391014422 acc: 0.5549
[Epoch 23] loss: 0.8285549557803537 acc: 0.5481
[Epoch 27] loss: 0.6425002626026682 acc: 0.5532
[Epoch 31] loss: 0.5285947343949086 acc: 0.547
[Epoch 35] loss: 0.48394014488648424 acc: 0.5518
[Epoch 39] loss: 0.44029280399937004 acc: 0.5423
[Epoch 43] loss: 0.41025641928081547 acc: 0.5467
[Epoch 47] loss: 0.3857136717461564 acc: 0.5562
[Epoch 51] loss: 0.3766986091580728 acc: 0.5523
[Epoch 55] loss: 0.333449102659493 acc: 0.5491
[Epoch 59] loss: 0.34477362594783995 acc: 0.5517
[Epoch 63] loss: 0.2993254437018424 acc: 0.5545
[Epoch 67] loss: 0.2981697018841839 acc: 0.5544
[Epoch 71] loss: 0.28695275916067686 acc: 0.5528
--> [test] acc: 0.5505
--> [accuracy] finished 0.5505
new state: tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5505
--> [reward] 0.5505
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5505 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.272478146931094 acc: 0.5122
[Epoch 7] loss: 4.762422434966583 acc: 0.5689
[Epoch 11] loss: 3.565351581177138 acc: 0.5713
[Epoch 15] loss: 2.2154206214353556 acc: 0.5633
[Epoch 19] loss: 1.223607244717953 acc: 0.5594
[Epoch 23] loss: 0.8110779705064376 acc: 0.5538
[Epoch 27] loss: 0.6107391946808532 acc: 0.5637
[Epoch 31] loss: 0.5356919600025696 acc: 0.5651
[Epoch 35] loss: 0.48938042684779753 acc: 0.5519
[Epoch 39] loss: 0.431143499756008 acc: 0.5514
[Epoch 43] loss: 0.4083323740564725 acc: 0.546
[Epoch 47] loss: 0.3893074340918256 acc: 0.5491
[Epoch 51] loss: 0.345627739229966 acc: 0.5529
[Epoch 55] loss: 0.3481406685908127 acc: 0.5494
[Epoch 59] loss: 0.32347454199486453 acc: 0.5487
[Epoch 63] loss: 0.31431423296051486 acc: 0.5509
[Epoch 67] loss: 0.2940426059758 acc: 0.5506
[Epoch 71] loss: 0.28531376932226027 acc: 0.5491
--> [test] acc: 0.5586
--> [accuracy] finished 0.5586
new state: tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5586
--> [reward] 0.5586
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5586 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.307396530800158 acc: 0.5069
[Epoch 7] loss: 4.849140114948877 acc: 0.5599
[Epoch 11] loss: 3.668493475907904 acc: 0.5766
[Epoch 15] loss: 2.348352939652665 acc: 0.5668
[Epoch 19] loss: 1.3193133294086932 acc: 0.5592
[Epoch 23] loss: 0.8372536154506761 acc: 0.5582
[Epoch 27] loss: 0.6629014329036789 acc: 0.5545
[Epoch 31] loss: 0.5316894710673701 acc: 0.5587
[Epoch 35] loss: 0.4785227914295538 acc: 0.553
[Epoch 39] loss: 0.43888338618075756 acc: 0.5537
[Epoch 43] loss: 0.4117693281506219 acc: 0.5457
[Epoch 47] loss: 0.38070869778790284 acc: 0.5515
[Epoch 51] loss: 0.37472153453113477 acc: 0.5431
[Epoch 55] loss: 0.34266706864299523 acc: 0.5481
[Epoch 59] loss: 0.330238612649767 acc: 0.5567
[Epoch 63] loss: 0.3048295481321033 acc: 0.5427
[Epoch 67] loss: 0.31117804814844635 acc: 0.5468
[Epoch 71] loss: 0.2729528588492928 acc: 0.5496
--> [test] acc: 0.5425
--> [accuracy] finished 0.5425
new state: tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5425
--> [reward] 0.5425
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5425 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.315707212823736 acc: 0.5094
[Epoch 7] loss: 4.829069156171111 acc: 0.5752
[Epoch 11] loss: 3.670929034500171 acc: 0.574
[Epoch 15] loss: 2.353485902480762 acc: 0.5636
[Epoch 19] loss: 1.3038746976600888 acc: 0.5532
[Epoch 23] loss: 0.869539124772067 acc: 0.5526
[Epoch 27] loss: 0.6403924279162646 acc: 0.5493
[Epoch 31] loss: 0.5544724809506055 acc: 0.5549
[Epoch 35] loss: 0.4954787143708571 acc: 0.5448
[Epoch 39] loss: 0.4359544455323873 acc: 0.5404
[Epoch 43] loss: 0.39685528295452865 acc: 0.5534
[Epoch 47] loss: 0.40237071193383095 acc: 0.5451
[Epoch 51] loss: 0.3662586089228387 acc: 0.5531
[Epoch 55] loss: 0.35080207630639415 acc: 0.5493
[Epoch 59] loss: 0.3230222981879273 acc: 0.5427
[Epoch 63] loss: 0.3165858078419286 acc: 0.549
[Epoch 67] loss: 0.29389608281252483 acc: 0.5512
[Epoch 71] loss: 0.29895570958295214 acc: 0.5503
--> [test] acc: 0.5391
--> [accuracy] finished 0.5391
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5391
--> [reward] 0.5391
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5391 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0911, 0.0917, 0.0910, 0.0901, 0.0912, 0.0910, 0.0910, 0.0908, 0.0904,
         0.0908, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3977, -2.3971, -2.3978, -2.3987, -2.3976, -2.3978, -2.3978, -2.3980,
         -2.3985, -2.3980, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.272134909544454 acc: 0.4938
[Epoch 7] loss: 4.8074411307759295 acc: 0.5584
[Epoch 11] loss: 3.6354365671991995 acc: 0.566
[Epoch 15] loss: 2.285688139822172 acc: 0.5693
[Epoch 19] loss: 1.2887002732366553 acc: 0.5536
[Epoch 23] loss: 0.8185403704776636 acc: 0.5492
[Epoch 27] loss: 0.659726799201325 acc: 0.5505
[Epoch 31] loss: 0.5435347023284267 acc: 0.549
[Epoch 35] loss: 0.49171225094448423 acc: 0.5451
[Epoch 39] loss: 0.4444590688342481 acc: 0.5383
[Epoch 43] loss: 0.416178392589359 acc: 0.546
[Epoch 47] loss: 0.3837803008125337 acc: 0.5451
[Epoch 51] loss: 0.3609074453401672 acc: 0.5491
[Epoch 55] loss: 0.35129970327839066 acc: 0.549
[Epoch 59] loss: 0.31484965885253363 acc: 0.5439
[Epoch 63] loss: 0.3159528809487629 acc: 0.5489
[Epoch 67] loss: 0.2892693939745007 acc: 0.5522
[Epoch 71] loss: 0.30336394100247516 acc: 0.5517
--> [test] acc: 0.5478
--> [accuracy] finished 0.5478
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5478
--> [reward] 0.5478
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1499]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.2997]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.5475]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.5648]], device='cuda:0')
------ ------
delta_t: tensor([[0.5475]], device='cuda:0')
rewards[i]: 0.5478
values[i+1]: tensor([[0.0172]], device='cuda:0')
values[i]: tensor([[0.0174]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.5475]], device='cuda:0')
delta_t: tensor([[0.5475]], device='cuda:0')
------ ------
policy_loss: 1.2888472080230713
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.5475]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.7338]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.1678]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.0807]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.0983]], device='cuda:0')
------ ------
delta_t: tensor([[0.5387]], device='cuda:0')
rewards[i]: 0.5391
values[i+1]: tensor([[0.0174]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0176]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.0807]], device='cuda:0')
delta_t: tensor([[0.5387]], device='cuda:0')
------ ------
policy_loss: 3.855292320251465
log_probs[i]: tensor([[-2.3971]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.0807]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.0333]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.5990]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.6121]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.6298]], device='cuda:0')
------ ------
delta_t: tensor([[0.5423]], device='cuda:0')
rewards[i]: 0.5425
values[i+1]: tensor([[0.0176]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0177]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.6121]], device='cuda:0')
delta_t: tensor([[0.5423]], device='cuda:0')
------ ------
policy_loss: 7.6965250968933105
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.6121]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[4.3544]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.6423]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1546]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1721]], device='cuda:0')
------ ------
delta_t: tensor([[0.5586]], device='cuda:0')
rewards[i]: 0.5586
values[i+1]: tensor([[0.0177]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0175]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1546]], device='cuda:0')
delta_t: tensor([[0.5586]], device='cuda:0')
------ ------
policy_loss: 12.838980674743652
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1546]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[7.9552]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.2017]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.6836]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7009]], device='cuda:0')
------ ------
delta_t: tensor([[0.5505]], device='cuda:0')
rewards[i]: 0.5505
values[i+1]: tensor([[0.0175]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0173]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.6836]], device='cuda:0')
delta_t: tensor([[0.5505]], device='cuda:0')
------ ------
policy_loss: 19.2493896484375
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.6836]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[13.0759]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.2414]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.2002]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2175]], device='cuda:0')
------ ------
delta_t: tensor([[0.5435]], device='cuda:0')
rewards[i]: 0.5436
values[i+1]: tensor([[0.0173]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0172]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.2002]], device='cuda:0')
delta_t: tensor([[0.5435]], device='cuda:0')
------ ------
policy_loss: 26.8996639251709
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.2002]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[19.9682]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.7845]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.7127]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7300]], device='cuda:0')
------ ------
delta_t: tensor([[0.5445]], device='cuda:0')
rewards[i]: 0.5447
values[i+1]: tensor([[0.0172]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0172]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.7127]], device='cuda:0')
delta_t: tensor([[0.5445]], device='cuda:0')
------ ------
policy_loss: 35.78141403198242
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.7127]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[28.9162]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.8960]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2304]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2478]], device='cuda:0')
------ ------
delta_t: tensor([[0.5548]], device='cuda:0')
rewards[i]: 0.5551
values[i+1]: tensor([[0.0172]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0174]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2304]], device='cuda:0')
delta_t: tensor([[0.5548]], device='cuda:0')
------ ------
policy_loss: 45.90376281738281
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2304]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[40.0102]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.1880]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7104]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7281]], device='cuda:0')
------ ------
delta_t: tensor([[0.5224]], device='cuda:0')
rewards[i]: 0.5228
values[i+1]: tensor([[0.0174]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0177]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7104]], device='cuda:0')
delta_t: tensor([[0.5224]], device='cuda:0')
------ ------
policy_loss: 57.171077728271484
log_probs[i]: tensor([[-2.3971]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7104]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[53.4254]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[26.8304]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.1798]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.1974]], device='cuda:0')
------ ------
delta_t: tensor([[0.5165]], device='cuda:0')
rewards[i]: 0.5166
values[i+1]: tensor([[0.0177]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0176]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.1798]], device='cuda:0')
delta_t: tensor([[0.5165]], device='cuda:0')
------ ------
policy_loss: 69.56847381591797
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.1798]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[69.5814]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.3120]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.6844]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.7019]], device='cuda:0')
------ ------
delta_t: tensor([[0.5564]], device='cuda:0')
rewards[i]: 0.5564
values[i+1]: tensor([[0.0176]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0175]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.6844]], device='cuda:0')
delta_t: tensor([[0.5564]], device='cuda:0')
------ ------
policy_loss: 83.17378234863281
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.6844]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[88.7303]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[38.2978]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1885]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2059]], device='cuda:0')
------ ------
delta_t: tensor([[0.5610]], device='cuda:0')
rewards[i]: 0.5611
values[i+1]: tensor([[0.0175]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0174]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1885]], device='cuda:0')
delta_t: tensor([[0.5610]], device='cuda:0')
------ ------
policy_loss: 97.9900894165039
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1885]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[111.3484]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.2362]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7258]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7428]], device='cuda:0')
------ ------
delta_t: tensor([[0.5991]], device='cuda:0')
rewards[i]: 0.5989
values[i+1]: tensor([[0.0174]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0170]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7258]], device='cuda:0')
delta_t: tensor([[0.5991]], device='cuda:0')
------ ------
policy_loss: 114.09159851074219
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7258]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[137.6554]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[52.6140]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2535]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2699]], device='cuda:0')
------ ------
delta_t: tensor([[0.5950]], device='cuda:0')
rewards[i]: 0.5945
values[i+1]: tensor([[0.0170]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0163]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2535]], device='cuda:0')
delta_t: tensor([[0.5950]], device='cuda:0')
------ ------
policy_loss: 131.45928955078125
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2535]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[167.8560]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[60.4013]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.7718]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7872]], device='cuda:0')
------ ------
delta_t: tensor([[0.5908]], device='cuda:0')
rewards[i]: 0.59
values[i+1]: tensor([[0.0163]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0153]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.7718]], device='cuda:0')
delta_t: tensor([[0.5908]], device='cuda:0')
------ ------
policy_loss: 150.06874084472656
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.7718]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 150.06874084472656
value_loss: 167.85601806640625
loss: 233.9967498779297



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-7.6118e-04, -1.0978e-06, -2.3387e-06, -6.0342e-06, -1.6464e-06],
        [ 1.1893e-02,  1.6688e-05,  4.2126e-05,  8.7333e-05,  2.4525e-05],
        [-5.1976e-04, -7.8951e-07, -1.1811e-06, -4.7974e-06, -1.0883e-06],
        [ 7.0223e-02,  1.0276e-04,  2.0347e-04,  5.8764e-04,  1.4196e-04],
        [ 2.6042e-01,  3.7760e-04,  8.0089e-04,  2.1191e-03,  5.4225e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 5.0191e-06,  7.9588e-06,  2.5007e-08, -8.0578e-06,  4.5065e-06],
        [-7.5751e-05, -1.2437e-04, -2.2122e-06,  1.2573e-04, -6.8333e-05],
        [ 3.6369e-06,  5.4258e-06, -1.4143e-07, -5.5074e-06,  3.2480e-06],
        [-4.6961e-04, -7.3299e-04,  3.9197e-06,  7.4256e-04, -4.2160e-04],
        [-1.7206e-03, -2.7196e-03,  1.0672e-07,  2.7532e-03, -1.5469e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.0772, -0.0755,  0.0771, -0.0673,  0.0426],
        [-0.0126,  0.0125, -0.0126,  0.0114, -0.0075],
        [-0.0512,  0.0499, -0.0511,  0.0442, -0.0277],
        [ 0.0051, -0.0049,  0.0051, -0.0043,  0.0026],
        [-0.0185,  0.0180, -0.0184,  0.0160, -0.0100]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0428,  0.0613, -0.0024, -0.0623,  0.0379],
        [-0.0005, -0.0011, -0.0005,  0.0012, -0.0004],
        [-0.0240, -0.0343,  0.0017,  0.0349, -0.0214],
        [-0.0086, -0.0123,  0.0004,  0.0126, -0.0075],
        [ 0.0449,  0.0653, -0.0018, -0.0665,  0.0398],
        [-0.0240, -0.0343,  0.0017,  0.0348, -0.0214],
        [-0.0240, -0.0343,  0.0017,  0.0348, -0.0214],
        [ 0.0129,  0.0180, -0.0015, -0.0182,  0.0116],
        [-0.0066, -0.0096,  0.0001,  0.0098, -0.0058],
        [ 0.0025,  0.0031, -0.0001, -0.0032,  0.0020],
        [-0.0152, -0.0218,  0.0009,  0.0221, -0.0135]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.4510,  2.0700, -0.1015, -2.1040,  1.2918]], device='cuda:0')
--> [loss] 233.9967498779297

---------------------------------- [[#8 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5478 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.303787468034593 acc: 0.5173
[Epoch 7] loss: 4.7631656378126515 acc: 0.5766
[Epoch 11] loss: 3.5817413327029293 acc: 0.5822
[Epoch 15] loss: 2.250056001383935 acc: 0.5659
[Epoch 19] loss: 1.2582850563327979 acc: 0.5546
[Epoch 23] loss: 0.8062123344339374 acc: 0.5576
[Epoch 27] loss: 0.6289324781493001 acc: 0.5601
[Epoch 31] loss: 0.5395741119380574 acc: 0.5586
[Epoch 35] loss: 0.4811123381928562 acc: 0.5614
[Epoch 39] loss: 0.4388955064699092 acc: 0.56
[Epoch 43] loss: 0.4188175921273582 acc: 0.554
[Epoch 47] loss: 0.369290175190305 acc: 0.5482
[Epoch 51] loss: 0.364108421996979 acc: 0.5549
[Epoch 55] loss: 0.328541746098653 acc: 0.5557
[Epoch 59] loss: 0.33423416095945385 acc: 0.5437
[Epoch 63] loss: 0.3132781264469828 acc: 0.5492
[Epoch 67] loss: 0.3060343964168292 acc: 0.5564
[Epoch 71] loss: 0.29493450614221184 acc: 0.5488
--> [test] acc: 0.5537
--> [accuracy] finished 0.5537
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5537
--> [reward] 0.5537
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5537 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.328358805393014 acc: 0.5161
[Epoch 7] loss: 4.830475897130454 acc: 0.5564
[Epoch 11] loss: 3.6615969256671796 acc: 0.5844
[Epoch 15] loss: 2.3665386959719843 acc: 0.5639
[Epoch 19] loss: 1.3239568931519832 acc: 0.5547
[Epoch 23] loss: 0.8558421182300886 acc: 0.5517
[Epoch 27] loss: 0.6493947389714249 acc: 0.5521
[Epoch 31] loss: 0.5550217388717034 acc: 0.5523
[Epoch 35] loss: 0.5006971515219687 acc: 0.5506
[Epoch 39] loss: 0.44076855097661544 acc: 0.5484
[Epoch 43] loss: 0.40849537458366064 acc: 0.5463
[Epoch 47] loss: 0.38773607496467544 acc: 0.5494
[Epoch 51] loss: 0.36230365982960405 acc: 0.5518
[Epoch 55] loss: 0.3398543507017938 acc: 0.5435
[Epoch 59] loss: 0.32564060209924 acc: 0.5476
[Epoch 63] loss: 0.3181447919715396 acc: 0.553
[Epoch 67] loss: 0.297453375205712 acc: 0.5466
[Epoch 71] loss: 0.28444896865150204 acc: 0.5439
--> [test] acc: 0.5468
--> [accuracy] finished 0.5468
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5468
--> [reward] 0.5468
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5468 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.304582522653253 acc: 0.5133
[Epoch 7] loss: 4.795645177974116 acc: 0.5563
[Epoch 11] loss: 3.642553984661541 acc: 0.5806
[Epoch 15] loss: 2.379278178105269 acc: 0.5666
[Epoch 19] loss: 1.3577962776126764 acc: 0.561
[Epoch 23] loss: 0.8820690334776936 acc: 0.5603
[Epoch 27] loss: 0.6758107655703107 acc: 0.5518
[Epoch 31] loss: 0.5857676233800934 acc: 0.5562
[Epoch 35] loss: 0.4967278516791818 acc: 0.5516
[Epoch 39] loss: 0.46963320806374786 acc: 0.5489
[Epoch 43] loss: 0.42664987825409834 acc: 0.5515
[Epoch 47] loss: 0.4009287237663708 acc: 0.545
[Epoch 51] loss: 0.3942142417416205 acc: 0.5434
[Epoch 55] loss: 0.34792480569885437 acc: 0.5503
[Epoch 59] loss: 0.35112182593778196 acc: 0.5382
[Epoch 63] loss: 0.33075890498464483 acc: 0.5471
[Epoch 67] loss: 0.30049214689561243 acc: 0.5459
[Epoch 71] loss: 0.3099327042789372 acc: 0.5415
--> [test] acc: 0.5471
--> [accuracy] finished 0.5471
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.5471
--> [reward] 0.5471
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.5471 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.281835177365472 acc: 0.4865
[Epoch 7] loss: 4.793955033087669 acc: 0.559
[Epoch 11] loss: 3.610811259435571 acc: 0.569
[Epoch 15] loss: 2.2945987883278782 acc: 0.5574
[Epoch 19] loss: 1.292095453309281 acc: 0.5699
[Epoch 23] loss: 0.8371077186101691 acc: 0.561
[Epoch 27] loss: 0.6361571380968594 acc: 0.5427
[Epoch 31] loss: 0.5601695604917719 acc: 0.551
[Epoch 35] loss: 0.48544793735351177 acc: 0.5469
[Epoch 39] loss: 0.44306320155187107 acc: 0.5455
[Epoch 43] loss: 0.40241830002116347 acc: 0.553
[Epoch 47] loss: 0.404109766997416 acc: 0.5554
[Epoch 51] loss: 0.3516275299108013 acc: 0.5507
[Epoch 55] loss: 0.3500053581598279 acc: 0.5469
[Epoch 59] loss: 0.3234963229955043 acc: 0.5488
[Epoch 63] loss: 0.31024747811581777 acc: 0.55
[Epoch 67] loss: 0.30596512540593707 acc: 0.5456
[Epoch 71] loss: 0.29152456046703756 acc: 0.5472
--> [test] acc: 0.546
--> [accuracy] finished 0.546
new state: tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
new reward: 0.546
--> [reward] 0.546
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     1.0     | 0.546  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.277905882471967 acc: 0.4947
[Epoch 7] loss: 4.791367435546787 acc: 0.5607
[Epoch 11] loss: 3.5372498750381762 acc: 0.5789
[Epoch 15] loss: 2.1074941896873973 acc: 0.5575
[Epoch 19] loss: 1.1501813941942456 acc: 0.5623
[Epoch 23] loss: 0.7493521229499747 acc: 0.5577
[Epoch 27] loss: 0.5787963424154254 acc: 0.5473
[Epoch 31] loss: 0.5080538812162512 acc: 0.5555
[Epoch 35] loss: 0.4522822303197268 acc: 0.5585
[Epoch 39] loss: 0.413060021615299 acc: 0.5519
[Epoch 43] loss: 0.3685538478033698 acc: 0.548
[Epoch 47] loss: 0.3540831771595856 acc: 0.5445
[Epoch 51] loss: 0.33134468908771836 acc: 0.549
[Epoch 55] loss: 0.32300757989287376 acc: 0.5453
[Epoch 59] loss: 0.2969010304945433 acc: 0.5558
[Epoch 63] loss: 0.2817505454022885 acc: 0.5472
[Epoch 67] loss: 0.28817264396873543 acc: 0.5462
[Epoch 71] loss: 0.26813545524407073 acc: 0.5438
--> [test] acc: 0.5445
--> [accuracy] finished 0.5445
new state: tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.5445
--> [reward] 0.5445
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.5445 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.290399881884875 acc: 0.5092
[Epoch 7] loss: 4.768457656626201 acc: 0.5626
[Epoch 11] loss: 3.5474578956203997 acc: 0.5733
[Epoch 15] loss: 2.131248169123669 acc: 0.5628
[Epoch 19] loss: 1.1789087666879834 acc: 0.5622
[Epoch 23] loss: 0.7625967095536954 acc: 0.5487
[Epoch 27] loss: 0.6079737334595541 acc: 0.5484
[Epoch 31] loss: 0.5096083190101568 acc: 0.5566
[Epoch 35] loss: 0.47535111576490235 acc: 0.5419
[Epoch 39] loss: 0.40893867197910994 acc: 0.5465
[Epoch 43] loss: 0.38333726704330245 acc: 0.5461
[Epoch 47] loss: 0.36324373644340757 acc: 0.5453
[Epoch 51] loss: 0.33471833032501097 acc: 0.5412
[Epoch 55] loss: 0.3345486774607121 acc: 0.5437
[Epoch 59] loss: 0.29430544363987415 acc: 0.5447
[Epoch 63] loss: 0.3005274450072966 acc: 0.5463
[Epoch 67] loss: 0.27788466210369867 acc: 0.5457
[Epoch 71] loss: 0.2708090008939128 acc: 0.5364
--> [test] acc: 0.5415
--> [accuracy] finished 0.5415
new state: tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.5415
--> [reward] 0.5415
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.5415 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.30150322292162 acc: 0.4984
[Epoch 7] loss: 4.799735695352335 acc: 0.5613
[Epoch 11] loss: 3.556228637542871 acc: 0.5739
[Epoch 15] loss: 2.1014297922020373 acc: 0.5606
[Epoch 19] loss: 1.1075368846583244 acc: 0.5484
[Epoch 23] loss: 0.7169880148242501 acc: 0.5496
[Epoch 27] loss: 0.5854137133082846 acc: 0.5521
[Epoch 31] loss: 0.4850010970640746 acc: 0.5482
[Epoch 35] loss: 0.4521841111800174 acc: 0.55
[Epoch 39] loss: 0.39356707371152044 acc: 0.5477
[Epoch 43] loss: 0.3653780788449985 acc: 0.5548
[Epoch 47] loss: 0.35367262914367115 acc: 0.5484
[Epoch 51] loss: 0.33574158826704753 acc: 0.5496
[Epoch 55] loss: 0.2982293983487903 acc: 0.5521
[Epoch 59] loss: 0.2936231940746536 acc: 0.547
[Epoch 63] loss: 0.28413166154814345 acc: 0.5491
[Epoch 67] loss: 0.26378430222706567 acc: 0.5427
[Epoch 71] loss: 0.25535529820710573 acc: 0.552
--> [test] acc: 0.5444
--> [accuracy] finished 0.5444
new state: tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.5444
--> [reward] 0.5444
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.5444 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.299272004905564 acc: 0.5061
[Epoch 7] loss: 4.789890780168421 acc: 0.5587
[Epoch 11] loss: 3.591399418880872 acc: 0.5716
[Epoch 15] loss: 2.182012852919681 acc: 0.5723
[Epoch 19] loss: 1.190814943321983 acc: 0.5534
[Epoch 23] loss: 0.7792273629504396 acc: 0.5489
[Epoch 27] loss: 0.6101282449615428 acc: 0.5442
[Epoch 31] loss: 0.5037071190517196 acc: 0.5545
[Epoch 35] loss: 0.4739281367415281 acc: 0.5582
[Epoch 39] loss: 0.4169195087226417 acc: 0.5464
[Epoch 43] loss: 0.383263275620368 acc: 0.5487
[Epoch 47] loss: 0.3634799929345241 acc: 0.5453
[Epoch 51] loss: 0.3309078018283448 acc: 0.547
[Epoch 55] loss: 0.33936335942696044 acc: 0.5483
[Epoch 59] loss: 0.2992142682080455 acc: 0.552
[Epoch 63] loss: 0.29187318826299113 acc: 0.5507
[Epoch 67] loss: 0.27522622811062564 acc: 0.5479
[Epoch 71] loss: 0.2797373666301789 acc: 0.5509
--> [test] acc: 0.552
--> [accuracy] finished 0.552
new state: tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.552
--> [reward] 0.552
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.552  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.293350720039719 acc: 0.4976
[Epoch 7] loss: 4.808898787364326 acc: 0.5658
[Epoch 11] loss: 3.582464246493776 acc: 0.5837
[Epoch 15] loss: 2.172779852281446 acc: 0.5716
[Epoch 19] loss: 1.180523301710558 acc: 0.5463
[Epoch 23] loss: 0.7724832957300841 acc: 0.541
[Epoch 27] loss: 0.590628463622478 acc: 0.554
[Epoch 31] loss: 0.5080105603798805 acc: 0.5427
[Epoch 35] loss: 0.47359616796264564 acc: 0.5535
[Epoch 39] loss: 0.40561791336464 acc: 0.5446
[Epoch 43] loss: 0.37579688597994537 acc: 0.541
[Epoch 47] loss: 0.34961438833323816 acc: 0.5481
[Epoch 51] loss: 0.3452022313819174 acc: 0.5479
[Epoch 55] loss: 0.3131794950274555 acc: 0.5519
[Epoch 59] loss: 0.30185706571906884 acc: 0.5398
[Epoch 63] loss: 0.3001367722511711 acc: 0.5364
[Epoch 67] loss: 0.27323052148475213 acc: 0.5439
[Epoch 71] loss: 0.2723211712058624 acc: 0.545
--> [test] acc: 0.5407
--> [accuracy] finished 0.5407
new state: tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.5407
--> [reward] 0.5407
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.5407 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.271790234329146 acc: 0.5067
[Epoch 7] loss: 4.806177629839124 acc: 0.5579
[Epoch 11] loss: 3.5729638833524016 acc: 0.5723
[Epoch 15] loss: 2.1226835661684462 acc: 0.5608
[Epoch 19] loss: 1.1492612978534016 acc: 0.5535
[Epoch 23] loss: 0.756273597564615 acc: 0.5506
[Epoch 27] loss: 0.6061226956813079 acc: 0.5532
[Epoch 31] loss: 0.5015203260466494 acc: 0.5449
[Epoch 35] loss: 0.46728677157779486 acc: 0.5486
[Epoch 39] loss: 0.4077015153091887 acc: 0.5509
[Epoch 43] loss: 0.37879247138338623 acc: 0.5375
[Epoch 47] loss: 0.3512212598524854 acc: 0.5452
[Epoch 51] loss: 0.3396168831721558 acc: 0.5398
[Epoch 55] loss: 0.33814048206271685 acc: 0.5431
[Epoch 59] loss: 0.29730494191531864 acc: 0.5411
[Epoch 63] loss: 0.29077259766871627 acc: 0.5453
[Epoch 67] loss: 0.27400209369552336 acc: 0.5387
[Epoch 71] loss: 0.2674958143869172 acc: 0.5398
--> [test] acc: 0.5399
--> [accuracy] finished 0.5399
new state: tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.5399
--> [reward] 0.5399
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.5399 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.25221599032507 acc: 0.5091
[Epoch 7] loss: 4.761232178839271 acc: 0.5541
[Epoch 11] loss: 3.529237801614015 acc: 0.5839
[Epoch 15] loss: 2.1001645764883827 acc: 0.5654
[Epoch 19] loss: 1.1431047383629147 acc: 0.5495
[Epoch 23] loss: 0.7483704972088032 acc: 0.553
[Epoch 27] loss: 0.5829774971498781 acc: 0.5514
[Epoch 31] loss: 0.4910011990257846 acc: 0.5505
[Epoch 35] loss: 0.4591927616130513 acc: 0.544
[Epoch 39] loss: 0.41364321216721744 acc: 0.5446
[Epoch 43] loss: 0.3798001429228031 acc: 0.5532
[Epoch 47] loss: 0.35721898254940804 acc: 0.5452
[Epoch 51] loss: 0.3388098599115277 acc: 0.5386
[Epoch 55] loss: 0.30046643954856544 acc: 0.5497
[Epoch 59] loss: 0.31815412900436796 acc: 0.5417
[Epoch 63] loss: 0.280877676125511 acc: 0.5444
[Epoch 67] loss: 0.2836682501884982 acc: 0.5422
[Epoch 71] loss: 0.2624413103837034 acc: 0.5459
--> [test] acc: 0.5527
--> [accuracy] finished 0.5527
new state: tensor([608.,   1.,   2.,   7.,   1.], device='cuda:0')
new reward: 0.5527
--> [reward] 0.5527
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     7.0      |     1.0     | 0.5527 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   7.,   1.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([608.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.866672963437522 acc: 0.5366
[Epoch 7] loss: 4.197227677268446 acc: 0.6178
[Epoch 11] loss: 2.8070114655872747 acc: 0.6209
[Epoch 15] loss: 1.4490945806055118 acc: 0.6134
[Epoch 19] loss: 0.7623015630447194 acc: 0.6019
[Epoch 23] loss: 0.5242632875776352 acc: 0.5995
[Epoch 27] loss: 0.44301699164807035 acc: 0.5997
[Epoch 31] loss: 0.38357778024547695 acc: 0.5993
[Epoch 35] loss: 0.33725279739455266 acc: 0.5931
[Epoch 39] loss: 0.3140343545252443 acc: 0.598
[Epoch 43] loss: 0.29581029784610813 acc: 0.5973
[Epoch 47] loss: 0.27478301233690605 acc: 0.5876
[Epoch 51] loss: 0.2700752004066392 acc: 0.5982
[Epoch 55] loss: 0.23219405821479305 acc: 0.5913
[Epoch 59] loss: 0.23475056467220531 acc: 0.5998
[Epoch 63] loss: 0.2422828223768269 acc: 0.5909
[Epoch 67] loss: 0.2083570606079991 acc: 0.5902
[Epoch 71] loss: 0.20529118194209073 acc: 0.5964
--> [test] acc: 0.592
--> [accuracy] finished 0.592
new state: tensor([608.,   1.,   2.,   6.,   1.], device='cuda:0')
new reward: 0.592
--> [reward] 0.592
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     1.0     | 0.592  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([608.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.8390860548409655 acc: 0.5431
[Epoch 7] loss: 4.128942214467031 acc: 0.6154
[Epoch 11] loss: 2.7682777202647664 acc: 0.6198
[Epoch 15] loss: 1.4594676578060135 acc: 0.6068
[Epoch 19] loss: 0.7518990014386756 acc: 0.6105
[Epoch 23] loss: 0.5206979712819123 acc: 0.6088
[Epoch 27] loss: 0.4403578425683748 acc: 0.6066
[Epoch 31] loss: 0.3777269268708537 acc: 0.6128
[Epoch 35] loss: 0.33289879156500485 acc: 0.6095
[Epoch 39] loss: 0.31270435497240945 acc: 0.6034
[Epoch 43] loss: 0.28994492126052335 acc: 0.6009
[Epoch 47] loss: 0.2733142196493762 acc: 0.6024
[Epoch 51] loss: 0.254509044895687 acc: 0.6066
[Epoch 55] loss: 0.236757629537893 acc: 0.5997
[Epoch 59] loss: 0.2410133864344729 acc: 0.5995
[Epoch 63] loss: 0.2086582081070851 acc: 0.5915
[Epoch 67] loss: 0.22708267296123727 acc: 0.6019
[Epoch 71] loss: 0.19570534417639152 acc: 0.6025
--> [test] acc: 0.597
--> [accuracy] finished 0.597
new state: tensor([608.,   1.,   2.,   6.,   1.], device='cuda:0')
new reward: 0.597
--> [reward] 0.597
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     1.0     | 0.597  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   1.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.892588558404342 acc: 0.5367
[Epoch 7] loss: 4.261987501398072 acc: 0.6131
[Epoch 11] loss: 2.9939530775370193 acc: 0.6164
[Epoch 15] loss: 1.6935975341616993 acc: 0.6099
[Epoch 19] loss: 0.8890255795139105 acc: 0.6018
[Epoch 23] loss: 0.5965911919332069 acc: 0.5984
[Epoch 27] loss: 0.4625138607914643 acc: 0.5917
[Epoch 31] loss: 0.41435141532736663 acc: 0.5965
[Epoch 35] loss: 0.3738432383960318 acc: 0.6034
[Epoch 39] loss: 0.3414565671016188 acc: 0.5962
[Epoch 43] loss: 0.32942022029024637 acc: 0.599
[Epoch 47] loss: 0.29354314248094243 acc: 0.5925
[Epoch 51] loss: 0.26672337018906156 acc: 0.6006
[Epoch 55] loss: 0.28221923632242374 acc: 0.5969
[Epoch 59] loss: 0.2368725115371406 acc: 0.5968
[Epoch 63] loss: 0.24573140331160495 acc: 0.6035
[Epoch 67] loss: 0.23784474555469687 acc: 0.598
[Epoch 71] loss: 0.21205630878229503 acc: 0.5981
--> [test] acc: 0.5903
--> [accuracy] finished 0.5903
new state: tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.5903
--> [reward] 0.5903
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     2.0     | 0.5903 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0918, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0907, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3970, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.86514732477915 acc: 0.5399
[Epoch 7] loss: 4.264564649375808 acc: 0.6132
[Epoch 11] loss: 2.9788818168060858 acc: 0.615
[Epoch 15] loss: 1.6571288117209968 acc: 0.6098
[Epoch 19] loss: 0.8785532058676337 acc: 0.5994
[Epoch 23] loss: 0.5839717838856037 acc: 0.603
[Epoch 27] loss: 0.47756662248345594 acc: 0.6042
[Epoch 31] loss: 0.4072034116917769 acc: 0.6006
[Epoch 35] loss: 0.36857243494638015 acc: 0.6063
[Epoch 39] loss: 0.3288175823652874 acc: 0.6046
[Epoch 43] loss: 0.3141162818050026 acc: 0.5981
[Epoch 47] loss: 0.30156771126953535 acc: 0.6056
[Epoch 51] loss: 0.27859140022436296 acc: 0.602
[Epoch 55] loss: 0.26938520692041157 acc: 0.6033
[Epoch 59] loss: 0.249557565192661 acc: 0.5965
[Epoch 63] loss: 0.24133816175281886 acc: 0.5949
[Epoch 67] loss: 0.23472700773350075 acc: 0.5973
[Epoch 71] loss: 0.2308087442749087 acc: 0.6009
--> [test] acc: 0.5931
--> [accuracy] finished 0.5931
new state: tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.5931
--> [reward] 0.5931
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1757]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3514]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.5928]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6137]], device='cuda:0')
------ ------
delta_t: tensor([[0.5928]], device='cuda:0')
rewards[i]: 0.5931
values[i+1]: tensor([[0.0209]], device='cuda:0')
values[i]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.5928]], device='cuda:0')
delta_t: tensor([[0.5928]], device='cuda:0')
------ ------
policy_loss: 1.3974270820617676
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.5928]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.8681]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.3848]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.1768]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.1979]], device='cuda:0')
------ ------
delta_t: tensor([[0.5899]], device='cuda:0')
rewards[i]: 0.5903
values[i+1]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0211]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.1768]], device='cuda:0')
delta_t: tensor([[0.5899]], device='cuda:0')
------ ------
policy_loss: 4.195439338684082
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.1768]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.4199]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.1036]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.7617]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.7829]], device='cuda:0')
------ ------
delta_t: tensor([[0.5967]], device='cuda:0')
rewards[i]: 0.597
values[i+1]: tensor([[0.0211]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0212]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.7617]], device='cuda:0')
delta_t: tensor([[0.5967]], device='cuda:0')
------ ------
policy_loss: 8.395606994628906
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.7617]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.1482]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.4566]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.3359]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.3571]], device='cuda:0')
------ ------
delta_t: tensor([[0.5918]], device='cuda:0')
rewards[i]: 0.592
values[i+1]: tensor([[0.0212]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0212]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.3359]], device='cuda:0')
delta_t: tensor([[0.5918]], device='cuda:0')
------ ------
policy_loss: 13.972888946533203
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.3359]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[9.2530]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.2095]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.8652]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8862]], device='cuda:0')
------ ------
delta_t: tensor([[0.5526]], device='cuda:0')
rewards[i]: 0.5527
values[i+1]: tensor([[0.0212]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.8652]], device='cuda:0')
delta_t: tensor([[0.5526]], device='cuda:0')
------ ------
policy_loss: 20.818660736083984
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.8652]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[14.9525]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[11.3990]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3762]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3973]], device='cuda:0')
------ ------
delta_t: tensor([[0.5397]], device='cuda:0')
rewards[i]: 0.5399
values[i+1]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3762]], device='cuda:0')
delta_t: tensor([[0.5397]], device='cuda:0')
------ ------
policy_loss: 28.893550872802734
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3762]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[22.4911]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[15.0771]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.8829]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.9040]], device='cuda:0')
------ ------
delta_t: tensor([[0.5405]], device='cuda:0')
rewards[i]: 0.5407
values[i+1]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0211]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.8829]], device='cuda:0')
delta_t: tensor([[0.5405]], device='cuda:0')
------ ------
policy_loss: 38.180538177490234
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.8829]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[32.1527]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.3234]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3958]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.4170]], device='cuda:0')
------ ------
delta_t: tensor([[0.5517]], device='cuda:0')
rewards[i]: 0.552
values[i+1]: tensor([[0.0211]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0211]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3958]], device='cuda:0')
delta_t: tensor([[0.5517]], device='cuda:0')
------ ------
policy_loss: 48.69743347167969
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3958]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[44.1381]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.9707]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8960]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.9172]], device='cuda:0')
------ ------
delta_t: tensor([[0.5441]], device='cuda:0')
rewards[i]: 0.5444
values[i+1]: tensor([[0.0211]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0212]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8960]], device='cuda:0')
delta_t: tensor([[0.5441]], device='cuda:0')
------ ------
policy_loss: 60.417877197265625
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8960]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[58.6545]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.0329]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.3882]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4095]], device='cuda:0')
------ ------
delta_t: tensor([[0.5412]], device='cuda:0')
rewards[i]: 0.5415
values[i+1]: tensor([[0.0212]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0213]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.3882]], device='cuda:0')
delta_t: tensor([[0.5412]], device='cuda:0')
------ ------
policy_loss: 73.31346893310547
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.3882]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[75.9329]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.5568]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8785]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.8999]], device='cuda:0')
------ ------
delta_t: tensor([[0.5442]], device='cuda:0')
rewards[i]: 0.5445
values[i+1]: tensor([[0.0213]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0214]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8785]], device='cuda:0')
delta_t: tensor([[0.5442]], device='cuda:0')
------ ------
policy_loss: 87.38504028320312
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8785]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[96.1938]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[40.5217]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3657]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.3869]], device='cuda:0')
------ ------
delta_t: tensor([[0.5460]], device='cuda:0')
rewards[i]: 0.546
values[i+1]: tensor([[0.0214]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0213]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3657]], device='cuda:0')
delta_t: tensor([[0.5460]], device='cuda:0')
------ ------
policy_loss: 102.62631225585938
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3657]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[119.6495]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[46.9113]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.8492]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8702]], device='cuda:0')
------ ------
delta_t: tensor([[0.5472]], device='cuda:0')
rewards[i]: 0.5471
values[i+1]: tensor([[0.0213]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.8492]], device='cuda:0')
delta_t: tensor([[0.5472]], device='cuda:0')
------ ------
policy_loss: 119.02491760253906
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.8492]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[146.4975]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[53.6960]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.3278]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.3482]], device='cuda:0')
------ ------
delta_t: tensor([[0.5471]], device='cuda:0')
rewards[i]: 0.5468
values[i+1]: tensor([[0.0210]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0205]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.3278]], device='cuda:0')
delta_t: tensor([[0.5471]], device='cuda:0')
------ ------
policy_loss: 136.57102966308594
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.3278]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[176.9856]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[60.9763]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.8087]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.8285]], device='cuda:0')
------ ------
delta_t: tensor([[0.5543]], device='cuda:0')
rewards[i]: 0.5537
values[i+1]: tensor([[0.0205]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0197]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.8087]], device='cuda:0')
delta_t: tensor([[0.5543]], device='cuda:0')
------ ------
policy_loss: 155.27284240722656
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.8087]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 155.27284240722656
value_loss: 176.98561096191406
loss: 243.76565551757812



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.6719e-03, -2.6883e-06, -2.2873e-06, -1.8934e-05, -2.8041e-06],
        [ 2.3285e-02,  3.7543e-05,  2.7350e-05,  2.6486e-04,  3.9495e-05],
        [-7.9740e-04, -1.2680e-06, -1.4855e-06, -8.8689e-06, -1.2984e-06],
        [ 9.9101e-02,  1.5854e-04,  1.4775e-04,  1.1138e-03,  1.6492e-04],
        [ 3.2141e-01,  5.1608e-04,  4.0037e-04,  3.6345e-03,  5.4214e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 1.2112e-05,  1.7189e-05, -1.0374e-06, -1.7455e-05,  1.0872e-05],
        [-1.6836e-04, -2.4061e-04,  1.3458e-05,  2.4440e-04, -1.5095e-04],
        [ 5.7770e-06,  8.0570e-06, -5.7780e-07, -8.1744e-06,  5.2009e-06],
        [-7.1621e-04, -1.0121e-03,  6.3889e-05,  1.0275e-03, -6.4337e-04],
        [-2.3191e-03, -3.3037e-03,  1.9117e-04,  3.3551e-03, -2.0804e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.0993, -0.0958,  0.0990, -0.0832,  0.0500],
        [-0.0202,  0.0194, -0.0201,  0.0168, -0.0101],
        [-0.0629,  0.0608, -0.0628,  0.0530, -0.0318],
        [ 0.0055, -0.0053,  0.0055, -0.0047,  0.0028],
        [-0.0216,  0.0209, -0.0216,  0.0182, -0.0109]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-1.2884e-02, -1.7480e-02,  1.4067e-03,  1.7733e-02, -1.1560e-02],
        [-2.5104e-02, -3.4178e-02,  2.9736e-03,  3.4631e-02, -2.2675e-02],
        [ 6.5444e-02,  8.8906e-02, -7.4056e-03, -9.0104e-02,  5.8925e-02],
        [ 9.8984e-03,  1.3452e-02, -1.4627e-03, -1.3628e-02,  9.0431e-03],
        [-2.4948e-02, -3.3965e-02,  2.9551e-03,  3.4416e-02, -2.2534e-02],
        [ 3.4437e-04,  1.0394e-04, -5.3671e-05, -1.5848e-04,  1.3696e-04],
        [-1.5227e-02, -2.0755e-02,  1.6230e-03,  2.1046e-02, -1.3683e-02],
        [ 3.4860e-02,  4.7988e-02, -3.3784e-03, -4.8647e-02,  3.1363e-02],
        [-2.4661e-02, -3.3575e-02,  2.9211e-03,  3.4021e-02, -2.2275e-02],
        [-2.0036e-02, -2.7301e-02,  2.2585e-03,  2.7680e-02, -1.8040e-02],
        [ 1.2314e-02,  1.6805e-02, -1.8376e-03, -1.6990e-02,  1.1298e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.5038,  2.0474, -0.1781, -2.0746,  1.3583]], device='cuda:0')
--> [loss] 243.76565551757812

---------------------------------- [[#9 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     2.0     | 0.5931 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.544012161776843 acc: 0.5881
[Epoch 7] loss: 3.6112867665412787 acc: 0.6503
[Epoch 11] loss: 2.0928204381252495 acc: 0.6643
[Epoch 15] loss: 0.9659174286838993 acc: 0.6546
[Epoch 19] loss: 0.5507058246428018 acc: 0.6443
[Epoch 23] loss: 0.38130849773240516 acc: 0.6515
[Epoch 27] loss: 0.3491917466101668 acc: 0.6579
[Epoch 31] loss: 0.29269915826313786 acc: 0.6566
[Epoch 35] loss: 0.2620976058280338 acc: 0.6506
[Epoch 39] loss: 0.24272223990029462 acc: 0.6539
[Epoch 43] loss: 0.21997692284252865 acc: 0.6545
[Epoch 47] loss: 0.20806312114667252 acc: 0.6493
[Epoch 51] loss: 0.19955327164839068 acc: 0.6534
[Epoch 55] loss: 0.1859025429020805 acc: 0.6512
[Epoch 59] loss: 0.18094645750344449 acc: 0.6575
[Epoch 63] loss: 0.1745964448322611 acc: 0.652
[Epoch 67] loss: 0.15634389531791515 acc: 0.6549
[Epoch 71] loss: 0.1625418203008716 acc: 0.65
--> [test] acc: 0.646
--> [accuracy] finished 0.646
new state: tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.646
--> [reward] 0.646
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     6.0      |     2.0     | 0.646  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.535855586876345 acc: 0.602
[Epoch 7] loss: 3.622027814388275 acc: 0.6632
[Epoch 11] loss: 2.0678813273034744 acc: 0.6645
[Epoch 15] loss: 0.9382011921090239 acc: 0.6437
[Epoch 19] loss: 0.5339957430596699 acc: 0.6536
[Epoch 23] loss: 0.38721709954730993 acc: 0.6411
[Epoch 27] loss: 0.32722242810594304 acc: 0.6594
[Epoch 31] loss: 0.2914406523094191 acc: 0.6517
[Epoch 35] loss: 0.2630049891250632 acc: 0.6567
[Epoch 39] loss: 0.23483779040091407 acc: 0.6466
[Epoch 43] loss: 0.22522124965363147 acc: 0.646
[Epoch 47] loss: 0.1984061784422992 acc: 0.6495
[Epoch 51] loss: 0.19972400116445996 acc: 0.6493
[Epoch 55] loss: 0.186760686334375 acc: 0.6385
[Epoch 59] loss: 0.17844369397183066 acc: 0.6459
[Epoch 63] loss: 0.1642048282648825 acc: 0.6383
[Epoch 67] loss: 0.16143244695242331 acc: 0.6467
[Epoch 71] loss: 0.15171400076754943 acc: 0.6443
--> [test] acc: 0.649
--> [accuracy] finished 0.649
new state: tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.649
--> [reward] 0.649
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     6.0      |     2.0     | 0.649  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.556942739297667 acc: 0.5822
[Epoch 7] loss: 3.6157556160941455 acc: 0.659
[Epoch 11] loss: 2.1303858356478877 acc: 0.6603
[Epoch 15] loss: 0.9930041149029951 acc: 0.651
[Epoch 19] loss: 0.5628769160546077 acc: 0.6563
[Epoch 23] loss: 0.39869269402816776 acc: 0.6488
[Epoch 27] loss: 0.3370104472074286 acc: 0.651
[Epoch 31] loss: 0.2997551664157444 acc: 0.652
[Epoch 35] loss: 0.26364168963487955 acc: 0.6476
[Epoch 39] loss: 0.24841551782082186 acc: 0.6452
[Epoch 43] loss: 0.2278401519069472 acc: 0.642
[Epoch 47] loss: 0.2158966722405132 acc: 0.6567
[Epoch 51] loss: 0.20403656466623477 acc: 0.6439
[Epoch 55] loss: 0.18227304716039536 acc: 0.6428
[Epoch 59] loss: 0.18335875402183255 acc: 0.6526
[Epoch 63] loss: 0.18053311682806428 acc: 0.6476
[Epoch 67] loss: 0.16324826066389375 acc: 0.6435
[Epoch 71] loss: 0.14242546826296146 acc: 0.642
--> [test] acc: 0.6526
--> [accuracy] finished 0.6526
new state: tensor([576.,   2.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.6526
--> [reward] 0.6526
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     2.0      |     2.0     |     6.0      |     2.0     | 0.6526 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.509865933367053 acc: 0.5994
[Epoch 7] loss: 3.5747044661923137 acc: 0.6622
[Epoch 11] loss: 2.05212136535236 acc: 0.6643
[Epoch 15] loss: 0.9575025266431787 acc: 0.6587
[Epoch 19] loss: 0.5378099728013624 acc: 0.6613
[Epoch 23] loss: 0.40326451513049244 acc: 0.6387
[Epoch 27] loss: 0.34040163943301077 acc: 0.6521
[Epoch 31] loss: 0.28680988262786206 acc: 0.6443
[Epoch 35] loss: 0.26882606917215734 acc: 0.638
[Epoch 39] loss: 0.23542571439862708 acc: 0.6489
[Epoch 43] loss: 0.22820807280033217 acc: 0.6423
[Epoch 47] loss: 0.2161779407831028 acc: 0.6487
[Epoch 51] loss: 0.19057821340696968 acc: 0.6536
[Epoch 55] loss: 0.18638966626266157 acc: 0.643
[Epoch 59] loss: 0.18031291179942524 acc: 0.6525
[Epoch 63] loss: 0.16893934722646803 acc: 0.6464
[Epoch 67] loss: 0.1685467713281436 acc: 0.6446
[Epoch 71] loss: 0.1550711349755659 acc: 0.647
--> [test] acc: 0.6493
--> [accuracy] finished 0.6493
new state: tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.6493
--> [reward] 0.6493
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     6.0      |     2.0     | 0.6493 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.555008422230821 acc: 0.5904
[Epoch 7] loss: 3.593579624315052 acc: 0.6503
[Epoch 11] loss: 2.0637592263996143 acc: 0.662
[Epoch 15] loss: 0.9259818933256294 acc: 0.6452
[Epoch 19] loss: 0.5300803237387439 acc: 0.6607
[Epoch 23] loss: 0.40503904679813957 acc: 0.6465
[Epoch 27] loss: 0.33272902931198667 acc: 0.6444
[Epoch 31] loss: 0.28854696433562455 acc: 0.6478
[Epoch 35] loss: 0.27240154653539894 acc: 0.6514
[Epoch 39] loss: 0.23680745941036574 acc: 0.645
[Epoch 43] loss: 0.2246994112887422 acc: 0.6447
[Epoch 47] loss: 0.2102410702696999 acc: 0.6543
[Epoch 51] loss: 0.187341447238384 acc: 0.6484
[Epoch 55] loss: 0.18617835534674584 acc: 0.6391
[Epoch 59] loss: 0.17151638270412928 acc: 0.6392
[Epoch 63] loss: 0.17095431268734435 acc: 0.6399
[Epoch 67] loss: 0.1631793666945394 acc: 0.637
[Epoch 71] loss: 0.1578236429760342 acc: 0.641
--> [test] acc: 0.6511
--> [accuracy] finished 0.6511
new state: tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.6511
--> [reward] 0.6511
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     6.0      |     2.0     | 0.6511 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.8014611344203315 acc: 0.5625
[Epoch 7] loss: 4.026834089585277 acc: 0.606
[Epoch 11] loss: 2.74109698530963 acc: 0.634
[Epoch 15] loss: 1.5045203341699926 acc: 0.6291
[Epoch 19] loss: 0.8076832026643369 acc: 0.6268
[Epoch 23] loss: 0.5548922989727058 acc: 0.6193
[Epoch 27] loss: 0.44801882484837263 acc: 0.6241
[Epoch 31] loss: 0.39407552029851756 acc: 0.6218
[Epoch 35] loss: 0.35189671735839007 acc: 0.6171
[Epoch 39] loss: 0.32396936361127726 acc: 0.6139
[Epoch 43] loss: 0.30077397419363644 acc: 0.6124
[Epoch 47] loss: 0.2721118273063446 acc: 0.6094
[Epoch 51] loss: 0.28334248188854483 acc: 0.5983
[Epoch 55] loss: 0.24169651043417928 acc: 0.6147
[Epoch 59] loss: 0.2366083450615406 acc: 0.6203
[Epoch 63] loss: 0.2322958344403092 acc: 0.6162
[Epoch 67] loss: 0.2279021744699696 acc: 0.6198
[Epoch 71] loss: 0.20003305966763393 acc: 0.6216
--> [test] acc: 0.6226
--> [accuracy] finished 0.6226
new state: tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
new reward: 0.6226
--> [reward] 0.6226
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     1.0     |     6.0      |     2.0     | 0.6226 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.776986878851186 acc: 0.5667
[Epoch 7] loss: 4.040129741591871 acc: 0.6142
[Epoch 11] loss: 2.744928714259506 acc: 0.6355
[Epoch 15] loss: 1.5206390478650627 acc: 0.6212
[Epoch 19] loss: 0.8088561699861456 acc: 0.6166
[Epoch 23] loss: 0.5438595461942579 acc: 0.6152
[Epoch 27] loss: 0.4532697482577637 acc: 0.6084
[Epoch 31] loss: 0.38879496838583055 acc: 0.6092
[Epoch 35] loss: 0.35043060126931164 acc: 0.6185
[Epoch 39] loss: 0.3087464490276583 acc: 0.615
[Epoch 43] loss: 0.2978236724829773 acc: 0.6139
[Epoch 47] loss: 0.2840185650979238 acc: 0.6134
[Epoch 51] loss: 0.2636241036493455 acc: 0.6159
[Epoch 55] loss: 0.26812196865706417 acc: 0.6121
[Epoch 59] loss: 0.2173157010532325 acc: 0.6119
[Epoch 63] loss: 0.2285438363626123 acc: 0.6103
[Epoch 67] loss: 0.22195069463280462 acc: 0.6187
[Epoch 71] loss: 0.21024378162482396 acc: 0.6057
--> [test] acc: 0.6033
--> [accuracy] finished 0.6033
new state: tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
new reward: 0.6033
--> [reward] 0.6033
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     1.0     |     6.0      |     2.0     | 0.6033 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.803853916085285 acc: 0.5713
[Epoch 7] loss: 4.0564326381744324 acc: 0.6266
[Epoch 11] loss: 2.7764967838516625 acc: 0.6463
[Epoch 15] loss: 1.5497730594995383 acc: 0.6295
[Epoch 19] loss: 0.8274325576451276 acc: 0.6279
[Epoch 23] loss: 0.5579849643075405 acc: 0.6151
[Epoch 27] loss: 0.4635097667660631 acc: 0.6201
[Epoch 31] loss: 0.4006015136599769 acc: 0.6207
[Epoch 35] loss: 0.34248912542620125 acc: 0.6137
[Epoch 39] loss: 0.3335490025356031 acc: 0.6132
[Epoch 43] loss: 0.298141823257403 acc: 0.617
[Epoch 47] loss: 0.2868313781269219 acc: 0.6199
[Epoch 51] loss: 0.2545234256631235 acc: 0.6123
[Epoch 55] loss: 0.24883454342556122 acc: 0.6109
[Epoch 59] loss: 0.24214690404674968 acc: 0.6017
[Epoch 63] loss: 0.22579124398157954 acc: 0.6175
[Epoch 67] loss: 0.2281967989809792 acc: 0.6092
[Epoch 71] loss: 0.21445966997991323 acc: 0.6159
--> [test] acc: 0.606
--> [accuracy] finished 0.606
new state: tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
new reward: 0.606
--> [reward] 0.606
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     1.0     |     6.0      |     2.0     | 0.606  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   1.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.547816270452631 acc: 0.5678
[Epoch 7] loss: 3.634574193936175 acc: 0.6647
[Epoch 11] loss: 2.1082223739541703 acc: 0.6589
[Epoch 15] loss: 0.9817918023604262 acc: 0.6592
[Epoch 19] loss: 0.5431469101554064 acc: 0.6545
[Epoch 23] loss: 0.3948132282985217 acc: 0.6546
[Epoch 27] loss: 0.3348094584668041 acc: 0.6452
[Epoch 31] loss: 0.2977930446681769 acc: 0.6495
[Epoch 35] loss: 0.27092186413714875 acc: 0.6519
[Epoch 39] loss: 0.23838820027144592 acc: 0.6506
[Epoch 43] loss: 0.22412258416385678 acc: 0.6502
[Epoch 47] loss: 0.20013131167206083 acc: 0.6545
[Epoch 51] loss: 0.19992668693289734 acc: 0.6353
[Epoch 55] loss: 0.19191492227194332 acc: 0.6501
[Epoch 59] loss: 0.17911706869954916 acc: 0.6569
[Epoch 63] loss: 0.1740150016588409 acc: 0.635
[Epoch 67] loss: 0.15623203217340134 acc: 0.6468
[Epoch 71] loss: 0.15822571374790367 acc: 0.6416
--> [test] acc: 0.6452
--> [accuracy] finished 0.6452
new state: tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.6452
--> [reward] 0.6452
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     6.0      |     2.0     | 0.6452 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.918944821333336 acc: 0.5405
[Epoch 7] loss: 4.295728857407484 acc: 0.6146
[Epoch 11] loss: 3.0088184454556925 acc: 0.6073
[Epoch 15] loss: 1.6768619227211188 acc: 0.6071
[Epoch 19] loss: 0.8791585122723409 acc: 0.5917
[Epoch 23] loss: 0.583012717268656 acc: 0.5887
[Epoch 27] loss: 0.48324084388630467 acc: 0.5968
[Epoch 31] loss: 0.39246984602659557 acc: 0.5955
[Epoch 35] loss: 0.3683561424312689 acc: 0.5896
[Epoch 39] loss: 0.3376421930287462 acc: 0.5927
[Epoch 43] loss: 0.32665083402543876 acc: 0.5923
[Epoch 47] loss: 0.299204141930546 acc: 0.5913
[Epoch 51] loss: 0.28616363927960164 acc: 0.5878
[Epoch 55] loss: 0.24847244165237525 acc: 0.5896
[Epoch 59] loss: 0.2597237978525974 acc: 0.5964
[Epoch 63] loss: 0.2387543291739567 acc: 0.5966
[Epoch 67] loss: 0.23754760358468308 acc: 0.5914
[Epoch 71] loss: 0.21139289459208851 acc: 0.5908
--> [test] acc: 0.5886
--> [accuracy] finished 0.5886
new state: tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
new reward: 0.5886
--> [reward] 0.5886
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     2.0     | 0.5886 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   1.,   2.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.324696305767654 acc: 0.5004
[Epoch 7] loss: 4.831293731241884 acc: 0.5639
[Epoch 11] loss: 3.74867315822855 acc: 0.5694
[Epoch 15] loss: 2.545323705581753 acc: 0.5666
[Epoch 19] loss: 1.4587634254218367 acc: 0.553
[Epoch 23] loss: 0.8904098071958251 acc: 0.5478
[Epoch 27] loss: 0.6540867246477805 acc: 0.5476
[Epoch 31] loss: 0.5170132116512265 acc: 0.5443
[Epoch 35] loss: 0.4675233978706667 acc: 0.5473
[Epoch 39] loss: 0.43747356349645217 acc: 0.5413
[Epoch 43] loss: 0.3975845390378171 acc: 0.5432
[Epoch 47] loss: 0.3679602960972568 acc: 0.5384
[Epoch 51] loss: 0.33701999719395204 acc: 0.5419
[Epoch 55] loss: 0.33608235169649886 acc: 0.5445
[Epoch 59] loss: 0.30833750866029574 acc: 0.5348
[Epoch 63] loss: 0.2901495742418653 acc: 0.5337
[Epoch 67] loss: 0.3012619105613102 acc: 0.5382
[Epoch 71] loss: 0.2631383208281663 acc: 0.5372
--> [test] acc: 0.5426
--> [accuracy] finished 0.5426
new state: tensor([608.,   1.,   2.,   6.,   3.], device='cuda:0')
new reward: 0.5426
--> [reward] 0.5426
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     3.0     | 0.5426 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   1.,   2.,   6.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.273685372089181 acc: 0.5064
[Epoch 7] loss: 4.82722184740369 acc: 0.5609
[Epoch 11] loss: 3.7866012720805604 acc: 0.573
[Epoch 15] loss: 2.6501192344576503 acc: 0.5602
[Epoch 19] loss: 1.562103440313388 acc: 0.5613
[Epoch 23] loss: 0.9447106703105942 acc: 0.5576
[Epoch 27] loss: 0.6796180266420098 acc: 0.5479
[Epoch 31] loss: 0.5546529269812966 acc: 0.5499
[Epoch 35] loss: 0.47925973290582297 acc: 0.5506
[Epoch 39] loss: 0.43158152458660515 acc: 0.5497
[Epoch 43] loss: 0.40244727639619576 acc: 0.5478
[Epoch 47] loss: 0.3858474767778802 acc: 0.5422
[Epoch 51] loss: 0.35889287408479414 acc: 0.5414
[Epoch 55] loss: 0.34092363686350835 acc: 0.551
[Epoch 59] loss: 0.3064326060211758 acc: 0.5416
[Epoch 63] loss: 0.30940355158761107 acc: 0.5392
[Epoch 67] loss: 0.29024393362519535 acc: 0.5443
[Epoch 71] loss: 0.2700882160707432 acc: 0.548
--> [test] acc: 0.5417
--> [accuracy] finished 0.5417
new state: tensor([608.,   1.,   2.,   6.,   4.], device='cuda:0')
new reward: 0.5417
--> [reward] 0.5417
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     6.0      |     4.0     | 0.5417 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   6.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   1.,   2.,   6.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.265094464697191 acc: 0.5006
[Epoch 7] loss: 4.7970912406206745 acc: 0.5514
[Epoch 11] loss: 3.732040273868824 acc: 0.5769
[Epoch 15] loss: 2.5438627797319455 acc: 0.5728
[Epoch 19] loss: 1.4787110323491304 acc: 0.5645
[Epoch 23] loss: 0.8797865154702798 acc: 0.5558
[Epoch 27] loss: 0.6311037689352127 acc: 0.5555
[Epoch 31] loss: 0.544839528153467 acc: 0.5624
[Epoch 35] loss: 0.45949849200523113 acc: 0.5498
[Epoch 39] loss: 0.4164647519340753 acc: 0.551
[Epoch 43] loss: 0.3919910203779826 acc: 0.5494
[Epoch 47] loss: 0.35750575109725563 acc: 0.5513
[Epoch 51] loss: 0.3299919578706479 acc: 0.5467
[Epoch 55] loss: 0.3297991111345799 acc: 0.5499
[Epoch 59] loss: 0.3144479492379119 acc: 0.5568
[Epoch 63] loss: 0.28880132222905414 acc: 0.5482
[Epoch 67] loss: 0.28020891404527304 acc: 0.5391
[Epoch 71] loss: 0.27016979053645107 acc: 0.5385
--> [test] acc: 0.5437
--> [accuracy] finished 0.5437
new state: tensor([640.,   1.,   2.,   6.,   4.], device='cuda:0')
new reward: 0.5437
--> [reward] 0.5437
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     6.0      |     4.0     | 0.5437 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3988, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   6.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([640.,   1.,   2.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.58664215647656 acc: 0.4756
[Epoch 7] loss: 5.230171994175143 acc: 0.5266
[Epoch 11] loss: 4.256129821548072 acc: 0.5459
[Epoch 15] loss: 3.105966944752447 acc: 0.5289
[Epoch 19] loss: 1.9472757292830425 acc: 0.5243
[Epoch 23] loss: 1.2168761264447057 acc: 0.5237
[Epoch 27] loss: 0.8669222425335966 acc: 0.5238
[Epoch 31] loss: 0.7120646181351998 acc: 0.5212
[Epoch 35] loss: 0.6114691145184552 acc: 0.5112
[Epoch 39] loss: 0.538258557477037 acc: 0.5025
[Epoch 43] loss: 0.5049050678606228 acc: 0.5126
[Epoch 47] loss: 0.45259271825537506 acc: 0.5116
[Epoch 51] loss: 0.43742107147412834 acc: 0.519
[Epoch 55] loss: 0.42146155634737764 acc: 0.5065
[Epoch 59] loss: 0.39194947457336404 acc: 0.5178
[Epoch 63] loss: 0.3820587300011874 acc: 0.505
[Epoch 67] loss: 0.3393773790043981 acc: 0.4983
[Epoch 71] loss: 0.35781301811928184 acc: 0.51
--> [test] acc: 0.5049
--> [accuracy] finished 0.5049
new state: tensor([640.,   1.,   2.,   7.,   4.], device='cuda:0')
new reward: 0.5049
--> [reward] 0.5049
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     7.0      |     4.0     | 0.5049 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0911, 0.0900, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3977, -2.3989, -2.3975, -2.3978, -2.3979, -2.3981,
         -2.3985, -2.3981, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.865910709056708 acc: 0.4395
[Epoch 7] loss: 5.743388660118708 acc: 0.491
[Epoch 11] loss: 5.057716030903789 acc: 0.5086
[Epoch 15] loss: 4.279780331170163 acc: 0.5018
[Epoch 19] loss: 3.2716445076800977 acc: 0.4888
[Epoch 23] loss: 2.2197555727361107 acc: 0.4773
[Epoch 27] loss: 1.507394909210827 acc: 0.4715
[Epoch 31] loss: 1.1223684430808363 acc: 0.4757
[Epoch 35] loss: 0.9495457681110295 acc: 0.4715
[Epoch 39] loss: 0.771172088020674 acc: 0.4676
[Epoch 43] loss: 0.7209508148357844 acc: 0.4689
[Epoch 47] loss: 0.6670275236506139 acc: 0.4656
[Epoch 51] loss: 0.6012580492783843 acc: 0.467
[Epoch 55] loss: 0.5570335739037341 acc: 0.4622
[Epoch 59] loss: 0.5705386943343427 acc: 0.4614
[Epoch 63] loss: 0.5424186280068687 acc: 0.4582
[Epoch 67] loss: 0.48125147079224784 acc: 0.4636
[Epoch 71] loss: 0.491539882929505 acc: 0.4552
--> [test] acc: 0.4569
--> [accuracy] finished 0.4569
new state: tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.4569
--> [reward] 0.4569
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1044]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.2088]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.4569]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.4812]], device='cuda:0')
------ ------
delta_t: tensor([[0.4569]], device='cuda:0')
rewards[i]: 0.4569
values[i+1]: tensor([[0.0245]], device='cuda:0')
values[i]: tensor([[0.0243]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.4569]], device='cuda:0')
delta_t: tensor([[0.4569]], device='cuda:0')
------ ------
policy_loss: 1.0714619159698486
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.4569]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.5622]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.9156]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.9569]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.9813]], device='cuda:0')
------ ------
delta_t: tensor([[0.5045]], device='cuda:0')
rewards[i]: 0.5049
values[i+1]: tensor([[0.0243]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0244]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.9569]], device='cuda:0')
delta_t: tensor([[0.5045]], device='cuda:0')
------ ------
policy_loss: 3.3421058654785156
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.9569]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[1.6729]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.2215]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4905]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.5152]], device='cuda:0')
------ ------
delta_t: tensor([[0.5432]], device='cuda:0')
rewards[i]: 0.5437
values[i+1]: tensor([[0.0244]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0247]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4905]], device='cuda:0')
delta_t: tensor([[0.5432]], device='cuda:0')
------ ------
policy_loss: 6.890679359436035
log_probs[i]: tensor([[-2.3969]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4905]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[3.7067]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.0676]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.0168]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0417]], device='cuda:0')
------ ------
delta_t: tensor([[0.5413]], device='cuda:0')
rewards[i]: 0.5417
values[i+1]: tensor([[0.0247]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0249]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.0168]], device='cuda:0')
delta_t: tensor([[0.5413]], device='cuda:0')
------ ------
policy_loss: 11.703252792358398
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.0168]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[6.9294]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.4454]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5388]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5639]], device='cuda:0')
------ ------
delta_t: tensor([[0.5421]], device='cuda:0')
rewards[i]: 0.5426
values[i+1]: tensor([[0.0249]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0251]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5388]], device='cuda:0')
delta_t: tensor([[0.5421]], device='cuda:0')
------ ------
policy_loss: 17.767539978027344
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5388]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[11.7389]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.6189]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.1014]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1268]], device='cuda:0')
------ ------
delta_t: tensor([[0.5880]], device='cuda:0')
rewards[i]: 0.5886
values[i+1]: tensor([[0.0251]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0254]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.1014]], device='cuda:0')
delta_t: tensor([[0.5880]], device='cuda:0')
------ ------
policy_loss: 25.179826736450195
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.1014]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[18.6404]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.8030]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.7152]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7408]], device='cuda:0')
------ ------
delta_t: tensor([[0.6448]], device='cuda:0')
rewards[i]: 0.6452
values[i+1]: tensor([[0.0254]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0255]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.7152]], device='cuda:0')
delta_t: tensor([[0.6448]], device='cuda:0')
------ ------
policy_loss: 34.064266204833984
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.7152]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[27.8166]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[18.3524]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2840]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3094]], device='cuda:0')
------ ------
delta_t: tensor([[0.6059]], device='cuda:0')
rewards[i]: 0.606
values[i+1]: tensor([[0.0255]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0254]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2840]], device='cuda:0')
delta_t: tensor([[0.6059]], device='cuda:0')
------ ------
policy_loss: 44.31303405761719
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2840]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[39.5505]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.4678]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8444]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8696]], device='cuda:0')
------ ------
delta_t: tensor([[0.6032]], device='cuda:0')
rewards[i]: 0.6033
values[i+1]: tensor([[0.0254]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0252]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8444]], device='cuda:0')
delta_t: tensor([[0.6032]], device='cuda:0')
------ ------
policy_loss: 55.90606689453125
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8444]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[54.2304]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.3599]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4185]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4435]], device='cuda:0')
------ ------
delta_t: tensor([[0.6226]], device='cuda:0')
rewards[i]: 0.6226
values[i+1]: tensor([[0.0252]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0250]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4185]], device='cuda:0')
delta_t: tensor([[0.6226]], device='cuda:0')
------ ------
policy_loss: 68.872802734375
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4185]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[72.3208]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[36.1808]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.0151]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.0401]], device='cuda:0')
------ ------
delta_t: tensor([[0.6508]], device='cuda:0')
rewards[i]: 0.6511
values[i+1]: tensor([[0.0250]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0251]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.0151]], device='cuda:0')
delta_t: tensor([[0.6508]], device='cuda:0')
------ ------
policy_loss: 83.2779769897461
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.0151]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[94.1270]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[43.6124]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.6040]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.6290]], device='cuda:0')
------ ------
delta_t: tensor([[0.6491]], device='cuda:0')
rewards[i]: 0.6493
values[i+1]: tensor([[0.0251]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0251]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.6040]], device='cuda:0')
delta_t: tensor([[0.6491]], device='cuda:0')
------ ------
policy_loss: 99.08331298828125
log_probs[i]: tensor([[-2.3969]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.6040]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[119.9799]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[51.7059]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.1907]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2154]], device='cuda:0')
------ ------
delta_t: tensor([[0.6527]], device='cuda:0')
rewards[i]: 0.6526
values[i+1]: tensor([[0.0251]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0247]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.1907]], device='cuda:0')
delta_t: tensor([[0.6527]], device='cuda:0')
------ ------
policy_loss: 116.29966735839844
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.1907]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[150.1507]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[60.3415]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.7680]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7922]], device='cuda:0')
------ ------
delta_t: tensor([[0.6492]], device='cuda:0')
rewards[i]: 0.649
values[i+1]: tensor([[0.0247]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0242]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.7680]], device='cuda:0')
delta_t: tensor([[0.6492]], device='cuda:0')
------ ------
policy_loss: 134.90370178222656
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.7680]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[184.9029]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[69.5044]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.3369]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.3603]], device='cuda:0')
------ ------
delta_t: tensor([[0.6466]], device='cuda:0')
rewards[i]: 0.646
values[i+1]: tensor([[0.0242]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0234]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.3369]], device='cuda:0')
delta_t: tensor([[0.6466]], device='cuda:0')
------ ------
policy_loss: 154.87875366210938
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.3369]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 154.87875366210938
value_loss: 184.9029083251953
loss: 247.3302001953125



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-2.1281e-03, -3.9835e-06, -6.9339e-06, -2.1934e-05, -4.2338e-06],
        [ 2.3529e-02,  3.3024e-05,  7.8914e-05,  2.4713e-04,  3.1427e-05],
        [-1.1017e-03, -2.6300e-06, -3.4847e-06, -1.1145e-05, -3.0040e-06],
        [ 1.0038e-01,  1.7905e-04,  3.2922e-04,  1.0398e-03,  1.8846e-04],
        [ 3.7196e-01,  6.3580e-04,  1.2162e-03,  3.8727e-03,  6.4579e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 1.5719e-05,  2.1325e-05, -2.1587e-06, -2.1554e-05,  1.4323e-05],
        [-1.7298e-04, -2.3822e-04,  2.0710e-05,  2.4165e-04, -1.5769e-04],
        [ 8.1720e-06,  1.0897e-05, -1.2819e-06, -1.0968e-05,  7.4399e-06],
        [-7.3995e-04, -1.0063e-03,  9.9329e-05,  1.0178e-03, -6.7414e-04],
        [-2.7395e-03, -3.7353e-03,  3.5869e-04,  3.7806e-03, -2.4956e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.1099, -0.1060,  0.1096, -0.0920,  0.0542],
        [-0.0126,  0.0122, -0.0126,  0.0107, -0.0065],
        [-0.0739,  0.0712, -0.0737,  0.0617, -0.0361],
        [ 0.0046, -0.0044,  0.0046, -0.0038,  0.0022],
        [-0.0280,  0.0270, -0.0279,  0.0234, -0.0137]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 5.4829e-03,  7.1578e-03, -8.5891e-04, -7.1205e-03,  4.9142e-03],
        [ 8.7634e-03,  1.1126e-02, -1.9641e-03, -1.1068e-02,  7.9435e-03],
        [-1.1281e-02, -1.4847e-02,  1.9384e-03,  1.4823e-02, -1.0339e-02],
        [ 3.3923e-02,  4.4854e-02, -5.5493e-03, -4.4991e-02,  3.1096e-02],
        [-1.2913e-04, -1.7003e-04, -2.1233e-04,  2.2195e-04, -6.5224e-05],
        [-8.5985e-03, -1.1456e-02,  1.4696e-03,  1.1422e-02, -7.9672e-03],
        [-2.4144e-02, -3.1404e-02,  4.4274e-03,  3.1437e-02, -2.1935e-02],
        [ 3.1659e-02,  4.1432e-02, -5.5142e-03, -4.1395e-02,  2.8774e-02],
        [-2.3970e-02, -3.1178e-02,  4.3955e-03,  3.1211e-02, -2.1777e-02],
        [-5.4163e-03, -7.0957e-03,  8.4247e-04,  7.0575e-03, -4.8028e-03],
        [-6.2893e-03, -8.4187e-03,  1.0255e-03,  8.4016e-03, -5.8410e-03]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.4616,  1.9012, -0.2680, -1.9032,  1.3279]], device='cuda:0')
--> [loss] 247.3302001953125

---------------------------------- [[#10 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.4569 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3985, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.8945275045111964 acc: 0.4428
[Epoch 7] loss: 5.744066011265415 acc: 0.469
[Epoch 11] loss: 5.056826096056672 acc: 0.5149
[Epoch 15] loss: 4.289933361665672 acc: 0.5031
[Epoch 19] loss: 3.3080708350214505 acc: 0.4951
[Epoch 23] loss: 2.3004006874530822 acc: 0.4857
[Epoch 27] loss: 1.5547814997260834 acc: 0.4709
[Epoch 31] loss: 1.135668573858183 acc: 0.4743
[Epoch 35] loss: 0.9383130204456541 acc: 0.4751
[Epoch 39] loss: 0.8127870957755372 acc: 0.4739
[Epoch 43] loss: 0.7593293355020416 acc: 0.4636
[Epoch 47] loss: 0.6596005243699417 acc: 0.4654
[Epoch 51] loss: 0.6229969642084577 acc: 0.4684
[Epoch 55] loss: 0.596320732303745 acc: 0.4687
[Epoch 59] loss: 0.5752163340368539 acc: 0.4666
[Epoch 63] loss: 0.5454940621972161 acc: 0.479
[Epoch 67] loss: 0.4936930458287678 acc: 0.4637
[Epoch 71] loss: 0.49941283978921985 acc: 0.4707
--> [test] acc: 0.4711
--> [accuracy] finished 0.4711
new state: tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.4711
--> [reward] 0.4711
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.4711 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3985, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.863956168179622 acc: 0.4422
[Epoch 7] loss: 5.735356143368479 acc: 0.4809
[Epoch 11] loss: 5.018771472641879 acc: 0.5007
[Epoch 15] loss: 4.224918957104158 acc: 0.5078
[Epoch 19] loss: 3.18367592460664 acc: 0.494
[Epoch 23] loss: 2.145753400138272 acc: 0.483
[Epoch 27] loss: 1.4376411077464024 acc: 0.4824
[Epoch 31] loss: 1.081511462550334 acc: 0.4732
[Epoch 35] loss: 0.8789645485827685 acc: 0.4629
[Epoch 39] loss: 0.7980389104169958 acc: 0.4694
[Epoch 43] loss: 0.6766864522205441 acc: 0.4689
[Epoch 47] loss: 0.6469842087134452 acc: 0.4677
[Epoch 51] loss: 0.6244281106688025 acc: 0.4804
[Epoch 55] loss: 0.548785239105563 acc: 0.4614
[Epoch 59] loss: 0.5541349995423994 acc: 0.4718
[Epoch 63] loss: 0.4966958664224276 acc: 0.468
[Epoch 67] loss: 0.5006886908827383 acc: 0.4711
[Epoch 71] loss: 0.45906721727203226 acc: 0.468
--> [test] acc: 0.4699
--> [accuracy] finished 0.4699
new state: tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.4699
--> [reward] 0.4699
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.4699 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3985, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.870888130134329 acc: 0.4339
[Epoch 7] loss: 5.728762615672158 acc: 0.4875
[Epoch 11] loss: 5.010630951970435 acc: 0.4919
[Epoch 15] loss: 4.209267389134068 acc: 0.4991
[Epoch 19] loss: 3.1718165237275535 acc: 0.4936
[Epoch 23] loss: 2.121296330867216 acc: 0.4815
[Epoch 27] loss: 1.4451786108562708 acc: 0.4704
[Epoch 31] loss: 1.067782790135697 acc: 0.4765
[Epoch 35] loss: 0.8978868937286575 acc: 0.4759
[Epoch 39] loss: 0.7706139711162928 acc: 0.4681
[Epoch 43] loss: 0.7106878364368168 acc: 0.4627
[Epoch 47] loss: 0.6331653132120056 acc: 0.4649
[Epoch 51] loss: 0.619874699050775 acc: 0.4641
[Epoch 55] loss: 0.5717189012195372 acc: 0.468
[Epoch 59] loss: 0.5576264660071839 acc: 0.4657
[Epoch 63] loss: 0.5029674930988676 acc: 0.4649
[Epoch 67] loss: 0.5176242484241876 acc: 0.4641
[Epoch 71] loss: 0.4818271267778047 acc: 0.4694
--> [test] acc: 0.466
--> [accuracy] finished 0.466
new state: tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.466
--> [reward] 0.466
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.466  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3985, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([672.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.844867193180582 acc: 0.4347
[Epoch 7] loss: 5.716816086598369 acc: 0.4955
[Epoch 11] loss: 5.015803755854097 acc: 0.5108
[Epoch 15] loss: 4.214901736477756 acc: 0.4988
[Epoch 19] loss: 3.2020096400814593 acc: 0.4924
[Epoch 23] loss: 2.1762150256225214 acc: 0.4901
[Epoch 27] loss: 1.4739275154326579 acc: 0.4778
[Epoch 31] loss: 1.0867699905658317 acc: 0.4766
[Epoch 35] loss: 0.8871737850253539 acc: 0.4727
[Epoch 39] loss: 0.7748295265009336 acc: 0.4665
[Epoch 43] loss: 0.716144407163267 acc: 0.4632
[Epoch 47] loss: 0.6441895945945664 acc: 0.4691
[Epoch 51] loss: 0.6057498403646223 acc: 0.471
[Epoch 55] loss: 0.5608950124486632 acc: 0.4675
[Epoch 59] loss: 0.5245701626200429 acc: 0.471
[Epoch 63] loss: 0.5152612551046378 acc: 0.4636
[Epoch 67] loss: 0.5002193050197018 acc: 0.4694
[Epoch 71] loss: 0.46792364306271533 acc: 0.4607
--> [test] acc: 0.4664
--> [accuracy] finished 0.4664
new state: tensor([672.,   1.,   1.,   7.,   4.], device='cuda:0')
new reward: 0.4664
--> [reward] 0.4664
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     7.0      |     4.0     | 0.4664 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0919, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0903,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   7.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   1.,   1.,   6.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.658991492312888 acc: 0.4687
[Epoch 7] loss: 5.397865210347773 acc: 0.4997
[Epoch 11] loss: 4.602983183263208 acc: 0.5332
[Epoch 15] loss: 3.741128853519859 acc: 0.5275
[Epoch 19] loss: 2.7017130242741625 acc: 0.5156
[Epoch 23] loss: 1.7192544653592512 acc: 0.5129
[Epoch 27] loss: 1.1259474267092202 acc: 0.4997
[Epoch 31] loss: 0.8501013411909265 acc: 0.4955
[Epoch 35] loss: 0.7370129439246167 acc: 0.4995
[Epoch 39] loss: 0.6366019121387883 acc: 0.4939
[Epoch 43] loss: 0.5818538115386044 acc: 0.5028
[Epoch 47] loss: 0.5242995726125662 acc: 0.4955
[Epoch 51] loss: 0.5052868953150937 acc: 0.4996
[Epoch 55] loss: 0.465446149975614 acc: 0.4953
[Epoch 59] loss: 0.4597167884311674 acc: 0.4987
[Epoch 63] loss: 0.4304912320988448 acc: 0.4898
[Epoch 67] loss: 0.4031301333742869 acc: 0.4875
[Epoch 71] loss: 0.40978650116097287 acc: 0.495
--> [test] acc: 0.49
--> [accuracy] finished 0.49
new state: tensor([672.,   1.,   1.,   6.,   4.], device='cuda:0')
new reward: 0.49
--> [reward] 0.49
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     6.0      |     4.0     |  0.49  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   6.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   1.,   1.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.528674226282807 acc: 0.4687
[Epoch 7] loss: 5.226442001207405 acc: 0.5174
[Epoch 11] loss: 4.3905037080540374 acc: 0.5223
[Epoch 15] loss: 3.450962913005858 acc: 0.5381
[Epoch 19] loss: 2.346938160145679 acc: 0.5275
[Epoch 23] loss: 1.440269165103088 acc: 0.5217
[Epoch 27] loss: 0.9391922271998642 acc: 0.5145
[Epoch 31] loss: 0.7179705759753352 acc: 0.5126
[Epoch 35] loss: 0.6235624715409545 acc: 0.5083
[Epoch 39] loss: 0.575329647992578 acc: 0.5141
[Epoch 43] loss: 0.5029813105626332 acc: 0.5189
[Epoch 47] loss: 0.47206122051362337 acc: 0.5148
[Epoch 51] loss: 0.4387211729307919 acc: 0.5131
[Epoch 55] loss: 0.43526623488577737 acc: 0.5115
[Epoch 59] loss: 0.39008260854874804 acc: 0.5075
[Epoch 63] loss: 0.40076654177168597 acc: 0.5151
[Epoch 67] loss: 0.3656444723629738 acc: 0.5069
[Epoch 71] loss: 0.3456556470421574 acc: 0.5081
--> [test] acc: 0.5052
--> [accuracy] finished 0.5052
new state: tensor([672.,   1.,   1.,   5.,   4.], device='cuda:0')
new reward: 0.5052
--> [reward] 0.5052
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     4.0     | 0.5052 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0912, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3969, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.515397730080978 acc: 0.4773
[Epoch 7] loss: 5.24110149026222 acc: 0.5275
[Epoch 11] loss: 4.419390169403437 acc: 0.5445
[Epoch 15] loss: 3.493379255237482 acc: 0.5445
[Epoch 19] loss: 2.423717064930655 acc: 0.5115
[Epoch 23] loss: 1.5129879560235822 acc: 0.5055
[Epoch 27] loss: 0.9921207243905348 acc: 0.515
[Epoch 31] loss: 0.7392492477050827 acc: 0.5071
[Epoch 35] loss: 0.6245883228185841 acc: 0.4982
[Epoch 39] loss: 0.58773983243729 acc: 0.502
[Epoch 43] loss: 0.5096134041362178 acc: 0.4998
[Epoch 47] loss: 0.48021922834560543 acc: 0.5023
[Epoch 51] loss: 0.44955082165310756 acc: 0.5015
[Epoch 55] loss: 0.4371473888969025 acc: 0.4975
[Epoch 59] loss: 0.39718492955798307 acc: 0.505
[Epoch 63] loss: 0.40125735418256514 acc: 0.5083
[Epoch 67] loss: 0.3598715828069488 acc: 0.5008
[Epoch 71] loss: 0.3580958503786751 acc: 0.5084
--> [test] acc: 0.5031
--> [accuracy] finished 0.5031
new state: tensor([672.,   1.,   1.,   5.,   4.], device='cuda:0')
new reward: 0.5031
--> [reward] 0.5031
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     4.0     | 0.5031 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.560175456964147 acc: 0.459
[Epoch 7] loss: 5.302546473262865 acc: 0.4942
[Epoch 11] loss: 4.494162050659394 acc: 0.5309
[Epoch 15] loss: 3.567832360487155 acc: 0.5239
[Epoch 19] loss: 2.479337440046203 acc: 0.5169
[Epoch 23] loss: 1.5312102217884624 acc: 0.5183
[Epoch 27] loss: 1.025002861800401 acc: 0.4996
[Epoch 31] loss: 0.7587825734682782 acc: 0.5073
[Epoch 35] loss: 0.655098354343868 acc: 0.5062
[Epoch 39] loss: 0.5824399814009666 acc: 0.4955
[Epoch 43] loss: 0.5372175397470479 acc: 0.5002
[Epoch 47] loss: 0.4822708553017672 acc: 0.4983
[Epoch 51] loss: 0.4755109716444979 acc: 0.4948
[Epoch 55] loss: 0.4232262640362582 acc: 0.4965
[Epoch 59] loss: 0.4365134362364783 acc: 0.5087
[Epoch 63] loss: 0.40105458327314203 acc: 0.4991
[Epoch 67] loss: 0.38919423722550084 acc: 0.4954
[Epoch 71] loss: 0.3507515917081967 acc: 0.4949
--> [test] acc: 0.495
--> [accuracy] finished 0.495
new state: tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
new reward: 0.495
--> [reward] 0.495
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     3.0     | 0.495  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.043817010986835 acc: 0.5451
[Epoch 7] loss: 4.482401270848101 acc: 0.5969
[Epoch 11] loss: 3.3694818583900665 acc: 0.5973
[Epoch 15] loss: 2.2076890635520905 acc: 0.5972
[Epoch 19] loss: 1.2462056377507231 acc: 0.5863
[Epoch 23] loss: 0.7776896211097155 acc: 0.583
[Epoch 27] loss: 0.5789746768019922 acc: 0.5736
[Epoch 31] loss: 0.48620325940496784 acc: 0.5834
[Epoch 35] loss: 0.43868328325564754 acc: 0.5866
[Epoch 39] loss: 0.4107644761295613 acc: 0.5814
[Epoch 43] loss: 0.3754104451631265 acc: 0.5863
[Epoch 47] loss: 0.3679427904841464 acc: 0.5845
[Epoch 51] loss: 0.3323823712632784 acc: 0.5807
[Epoch 55] loss: 0.31442493197443844 acc: 0.5802
[Epoch 59] loss: 0.31665599011027673 acc: 0.5881
[Epoch 63] loss: 0.2957368962028447 acc: 0.5792
[Epoch 67] loss: 0.2709097519607457 acc: 0.578
[Epoch 71] loss: 0.2741504873665969 acc: 0.5743
--> [test] acc: 0.5752
--> [accuracy] finished 0.5752
new state: tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
new reward: 0.5752
--> [reward] 0.5752
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     2.0     | 0.5752 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([672.,   1.,   1.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.140552096354687 acc: 0.5206
[Epoch 7] loss: 4.681218912381955 acc: 0.5684
[Epoch 11] loss: 3.645237003903255 acc: 0.5876
[Epoch 15] loss: 2.516325152667282 acc: 0.5758
[Epoch 19] loss: 1.4718622747437118 acc: 0.5702
[Epoch 23] loss: 0.9010140810190411 acc: 0.568
[Epoch 27] loss: 0.6643000664213277 acc: 0.5636
[Epoch 31] loss: 0.5571973270086377 acc: 0.5614
[Epoch 35] loss: 0.47461947732512144 acc: 0.5612
[Epoch 39] loss: 0.45049438633672567 acc: 0.5728
[Epoch 43] loss: 0.4132559030909863 acc: 0.5513
[Epoch 47] loss: 0.3960598296821689 acc: 0.5651
[Epoch 51] loss: 0.3711147684570583 acc: 0.5592
[Epoch 55] loss: 0.33120862369561366 acc: 0.5469
[Epoch 59] loss: 0.3368093375345249 acc: 0.5584
[Epoch 63] loss: 0.33292633402721045 acc: 0.566
[Epoch 67] loss: 0.3053443331052275 acc: 0.5543
[Epoch 71] loss: 0.3051208190636619 acc: 0.5622
--> [test] acc: 0.5587
--> [accuracy] finished 0.5587
new state: tensor([672.,   1.,   1.,   6.,   2.], device='cuda:0')
new reward: 0.5587
--> [reward] 0.5587
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     6.0      |     2.0     | 0.5587 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.031068935723561 acc: 0.5385
[Epoch 7] loss: 4.4893064940981855 acc: 0.586
[Epoch 11] loss: 3.3661754815779683 acc: 0.6071
[Epoch 15] loss: 2.1650866103904023 acc: 0.5965
[Epoch 19] loss: 1.2030267667625567 acc: 0.59
[Epoch 23] loss: 0.7441229193335603 acc: 0.587
[Epoch 27] loss: 0.5656082348807541 acc: 0.584
[Epoch 31] loss: 0.49066580801992615 acc: 0.5847
[Epoch 35] loss: 0.4312730235907504 acc: 0.5859
[Epoch 39] loss: 0.3998316067968831 acc: 0.5851
[Epoch 43] loss: 0.36841863641262895 acc: 0.58
[Epoch 47] loss: 0.35172819466713595 acc: 0.5873
[Epoch 51] loss: 0.3252652912684109 acc: 0.5937
[Epoch 55] loss: 0.3065661855061036 acc: 0.5896
[Epoch 59] loss: 0.29663851622806486 acc: 0.5852
[Epoch 63] loss: 0.2955624348855556 acc: 0.5843
[Epoch 67] loss: 0.263843603987161 acc: 0.5798
[Epoch 71] loss: 0.25885418333980204 acc: 0.5807
--> [test] acc: 0.5853
--> [accuracy] finished 0.5853
new state: tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
new reward: 0.5853
--> [reward] 0.5853
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     2.0     | 0.5853 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.553973770812345 acc: 0.4644
[Epoch 7] loss: 5.333227079237818 acc: 0.5164
[Epoch 11] loss: 4.5237911140827265 acc: 0.5282
[Epoch 15] loss: 3.6151434087082555 acc: 0.521
[Epoch 19] loss: 2.5590725528157274 acc: 0.5285
[Epoch 23] loss: 1.621978774552455 acc: 0.5171
[Epoch 27] loss: 1.0568212783130844 acc: 0.5088
[Epoch 31] loss: 0.8110508383597101 acc: 0.4937
[Epoch 35] loss: 0.6722570314260242 acc: 0.5104
[Epoch 39] loss: 0.5884616352484354 acc: 0.5
[Epoch 43] loss: 0.5734396868187677 acc: 0.5057
[Epoch 47] loss: 0.5048582484740811 acc: 0.5067
[Epoch 51] loss: 0.46695260461800925 acc: 0.4998
[Epoch 55] loss: 0.4518394306673647 acc: 0.5042
[Epoch 59] loss: 0.42350467489770305 acc: 0.491
[Epoch 63] loss: 0.4175637121933996 acc: 0.4992
[Epoch 67] loss: 0.37390736704144406 acc: 0.5048
[Epoch 71] loss: 0.364482363462067 acc: 0.4965
--> [test] acc: 0.5024
--> [accuracy] finished 0.5024
new state: tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
new reward: 0.5024
--> [reward] 0.5024
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     3.0     | 0.5024 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.609660734301028 acc: 0.4645
[Epoch 7] loss: 5.3216598954651975 acc: 0.5132
[Epoch 11] loss: 4.506041638381646 acc: 0.5296
[Epoch 15] loss: 3.606460712754818 acc: 0.5351
[Epoch 19] loss: 2.561140524456873 acc: 0.5124
[Epoch 23] loss: 1.6346962035388288 acc: 0.5138
[Epoch 27] loss: 1.0630677498286338 acc: 0.5085
[Epoch 31] loss: 0.7976202991459985 acc: 0.5117
[Epoch 35] loss: 0.6642076861484886 acc: 0.5052
[Epoch 39] loss: 0.5898034693697072 acc: 0.5093
[Epoch 43] loss: 0.5219502332703689 acc: 0.5038
[Epoch 47] loss: 0.5053978392716183 acc: 0.5054
[Epoch 51] loss: 0.4626266411279359 acc: 0.4966
[Epoch 55] loss: 0.45139181627260755 acc: 0.4981
[Epoch 59] loss: 0.4252161275967003 acc: 0.4997
[Epoch 63] loss: 0.4062480812325426 acc: 0.5098
[Epoch 67] loss: 0.39622508328350836 acc: 0.506
[Epoch 71] loss: 0.35842735379400764 acc: 0.5035
--> [test] acc: 0.4941
--> [accuracy] finished 0.4941
new state: tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
new reward: 0.4941
--> [reward] 0.4941
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     3.0     | 0.4941 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.033634118716735 acc: 0.5111
[Epoch 7] loss: 4.48246488714462 acc: 0.5842
[Epoch 11] loss: 3.3841894977842757 acc: 0.604
[Epoch 15] loss: 2.221727110616996 acc: 0.5965
[Epoch 19] loss: 1.276053526784148 acc: 0.5908
[Epoch 23] loss: 0.7823721201580656 acc: 0.5914
[Epoch 27] loss: 0.580015354115716 acc: 0.5755
[Epoch 31] loss: 0.5088865258314116 acc: 0.5738
[Epoch 35] loss: 0.4467492374970251 acc: 0.5789
[Epoch 39] loss: 0.42204117195685503 acc: 0.5792
[Epoch 43] loss: 0.38148289000677404 acc: 0.5835
[Epoch 47] loss: 0.3541695518833597 acc: 0.5761
[Epoch 51] loss: 0.3190598003637722 acc: 0.5791
[Epoch 55] loss: 0.3244936586193302 acc: 0.5797
[Epoch 59] loss: 0.30482287775448824 acc: 0.5819
[Epoch 63] loss: 0.300305416223967 acc: 0.5798
[Epoch 67] loss: 0.2870290457268658 acc: 0.5788
[Epoch 71] loss: 0.2777532524443553 acc: 0.5773
--> [test] acc: 0.5756
--> [accuracy] finished 0.5756
new state: tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
new reward: 0.5756
--> [reward] 0.5756
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     5.0      |     2.0     | 0.5756 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0913, 0.0910, 0.0909, 0.0908, 0.0902,
         0.0907, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3976, -2.3968, -2.3976, -2.3989, -2.3975, -2.3978, -2.3979, -2.3980,
         -2.3986, -2.3981, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.07495568079107 acc: 0.5346
[Epoch 7] loss: 4.491934555113468 acc: 0.6029
[Epoch 11] loss: 3.3833128492850477 acc: 0.6003
[Epoch 15] loss: 2.2328220564691 acc: 0.5976
[Epoch 19] loss: 1.263811408863653 acc: 0.5841
[Epoch 23] loss: 0.7947887028079204 acc: 0.5931
[Epoch 27] loss: 0.5868661833064788 acc: 0.5941
[Epoch 31] loss: 0.5014827255531192 acc: 0.5925
[Epoch 35] loss: 0.4553075926973844 acc: 0.5906
[Epoch 39] loss: 0.39081504533443684 acc: 0.5806
[Epoch 43] loss: 0.39507312593205124 acc: 0.5948
[Epoch 47] loss: 0.3539690611898289 acc: 0.5882
[Epoch 51] loss: 0.34500922619000723 acc: 0.5896
[Epoch 55] loss: 0.31951271251911095 acc: 0.589
[Epoch 59] loss: 0.3057985845977045 acc: 0.5848
[Epoch 63] loss: 0.28623515793153315 acc: 0.5889
[Epoch 67] loss: 0.27502937975299097 acc: 0.5912
[Epoch 71] loss: 0.2650583574900885 acc: 0.5931
--> [test] acc: 0.5847
--> [accuracy] finished 0.5847
new state: tensor([704.,   1.,   1.,   5.,   2.], device='cuda:0')
new reward: 0.5847
--> [reward] 0.5847
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1707]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3415]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.5844]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6144]], device='cuda:0')
------ ------
delta_t: tensor([[0.5844]], device='cuda:0')
rewards[i]: 0.5847
values[i+1]: tensor([[0.0300]], device='cuda:0')
values[i]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.5844]], device='cuda:0')
delta_t: tensor([[0.5844]], device='cuda:0')
------ ------
policy_loss: 1.3766655921936035
log_probs[i]: tensor([[-2.3968]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.5844]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.8364]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.3314]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.1539]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.1839]], device='cuda:0')
------ ------
delta_t: tensor([[0.5753]], device='cuda:0')
rewards[i]: 0.5756
values[i+1]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.1539]], device='cuda:0')
delta_t: tensor([[0.5753]], device='cuda:0')
------ ------
policy_loss: 4.120303630828857
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.1539]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.1749]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.6769]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.6361]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.6661]], device='cuda:0')
------ ------
delta_t: tensor([[0.4938]], device='cuda:0')
rewards[i]: 0.4941
values[i+1]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.6361]], device='cuda:0')
delta_t: tensor([[0.4938]], device='cuda:0')
------ ------
policy_loss: 8.01971435546875
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.6361]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[4.4260]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.5023]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1219]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1519]], device='cuda:0')
------ ------
delta_t: tensor([[0.5021]], device='cuda:0')
rewards[i]: 0.5024
values[i+1]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1219]], device='cuda:0')
delta_t: tensor([[0.5021]], device='cuda:0')
------ ------
policy_loss: 13.084258079528809
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1219]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[8.0326]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.2131]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.6857]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7156]], device='cuda:0')
------ ------
delta_t: tensor([[0.5851]], device='cuda:0')
rewards[i]: 0.5853
values[i+1]: tensor([[0.0300]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0299]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.6857]], device='cuda:0')
delta_t: tensor([[0.5851]], device='cuda:0')
------ ------
policy_loss: 19.500503540039062
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.6857]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[13.2085]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.3519]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.2174]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2472]], device='cuda:0')
------ ------
delta_t: tensor([[0.5586]], device='cuda:0')
rewards[i]: 0.5587
values[i+1]: tensor([[0.0299]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0298]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.2174]], device='cuda:0')
delta_t: tensor([[0.5586]], device='cuda:0')
------ ------
policy_loss: 27.19202423095703
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.2174]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[20.2786]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.1402]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.7603]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7899]], device='cuda:0')
------ ------
delta_t: tensor([[0.5751]], device='cuda:0')
rewards[i]: 0.5752
values[i+1]: tensor([[0.0298]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0296]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.7603]], device='cuda:0')
delta_t: tensor([[0.5751]], device='cuda:0')
------ ------
policy_loss: 36.187442779541016
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.7603]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[29.1728]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.7883]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2176]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2470]], device='cuda:0')
------ ------
delta_t: tensor([[0.4949]], device='cuda:0')
rewards[i]: 0.495
values[i+1]: tensor([[0.0296]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0294]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2176]], device='cuda:0')
delta_t: tensor([[0.4949]], device='cuda:0')
------ ------
policy_loss: 46.27965545654297
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2176]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[40.1164]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[21.8873]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.6784]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7076]], device='cuda:0')
------ ------
delta_t: tensor([[0.5029]], device='cuda:0')
rewards[i]: 0.5031
values[i+1]: tensor([[0.0294]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0293]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.6784]], device='cuda:0')
delta_t: tensor([[0.5029]], device='cuda:0')
------ ------
policy_loss: 57.47193908691406
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.6784]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[53.3090]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[26.3851]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.1366]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.1658]], device='cuda:0')
------ ------
delta_t: tensor([[0.5050]], device='cuda:0')
rewards[i]: 0.5052
values[i+1]: tensor([[0.0293]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0291]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.1366]], device='cuda:0')
delta_t: tensor([[0.5050]], device='cuda:0')
------ ------
policy_loss: 69.76535034179688
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.1366]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[68.8495]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[31.0810]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.5750]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.6041]], device='cuda:0')
------ ------
delta_t: tensor([[0.4898]], device='cuda:0')
rewards[i]: 0.49
values[i+1]: tensor([[0.0291]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0291]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.5750]], device='cuda:0')
delta_t: tensor([[0.4898]], device='cuda:0')
------ ------
policy_loss: 83.11000061035156
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.5750]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[86.7626]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[35.8262]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.9855]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.0145]], device='cuda:0')
------ ------
delta_t: tensor([[0.4662]], device='cuda:0')
rewards[i]: 0.4664
values[i+1]: tensor([[0.0291]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0290]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.9855]], device='cuda:0')
delta_t: tensor([[0.4662]], device='cuda:0')
------ ------
policy_loss: 97.43242645263672
log_probs[i]: tensor([[-2.3969]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.9855]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[107.1896]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[40.8540]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3917]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4203]], device='cuda:0')
------ ------
delta_t: tensor([[0.4661]], device='cuda:0')
rewards[i]: 0.466
values[i+1]: tensor([[0.0290]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0286]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3917]], device='cuda:0')
delta_t: tensor([[0.4661]], device='cuda:0')
------ ------
policy_loss: 112.73595428466797
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3917]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[130.2960]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[46.2129]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7980]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8260]], device='cuda:0')
------ ------
delta_t: tensor([[0.4702]], device='cuda:0')
rewards[i]: 0.4699
values[i+1]: tensor([[0.0286]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0280]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7980]], device='cuda:0')
delta_t: tensor([[0.4702]], device='cuda:0')
------ ------
policy_loss: 129.0137939453125
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7980]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[156.2291]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[51.8661]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2018]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2289]], device='cuda:0')
------ ------
delta_t: tensor([[0.4718]], device='cuda:0')
rewards[i]: 0.4711
values[i+1]: tensor([[0.0280]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0270]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2018]], device='cuda:0')
delta_t: tensor([[0.4718]], device='cuda:0')
------ ------
policy_loss: 146.26614379882812
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2018]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 146.26614379882812
value_loss: 156.22909545898438
loss: 224.3806915283203



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-2.8122e-03, -4.9680e-06, -7.1430e-06, -2.8223e-05, -1.5709e-05],
        [ 3.8067e-02,  6.8259e-05,  1.0065e-04,  3.8335e-04,  2.1253e-04],
        [-9.1266e-04, -1.5475e-06, -2.1178e-06, -9.1151e-06, -5.1510e-06],
        [ 1.3364e-01,  2.3436e-04,  3.4115e-04,  1.3422e-03,  7.5222e-04],
        [ 6.1664e-01,  1.0668e-03,  1.5286e-03,  6.1794e-03,  3.5050e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.0671e-05,  2.6711e-05, -3.9163e-06, -2.6669e-05,  1.8794e-05],
        [-2.7904e-04, -3.6143e-04,  5.2981e-05,  3.6079e-04, -2.5402e-04],
        [ 6.7251e-06,  8.6531e-06, -1.2691e-06, -8.6391e-06,  6.1001e-06],
        [-9.7989e-04, -1.2670e-03,  1.8577e-04,  1.2646e-03, -8.9121e-04],
        [-4.5227e-03, -5.8400e-03,  8.5688e-04,  5.8282e-03, -4.1106e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.1724, -0.1669,  0.1720, -0.1454,  0.0858],
        [-0.0452,  0.0437, -0.0451,  0.0380, -0.0222],
        [-0.0826,  0.0800, -0.0824,  0.0698, -0.0413],
        [ 0.0027, -0.0026,  0.0027, -0.0023,  0.0014],
        [-0.0472,  0.0458, -0.0471,  0.0400, -0.0238]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0253, -0.0316,  0.0047,  0.0317, -0.0226],
        [ 0.0042,  0.0051, -0.0011, -0.0050,  0.0038],
        [-0.0253, -0.0316,  0.0047,  0.0316, -0.0226],
        [ 0.0057,  0.0084, -0.0009, -0.0081,  0.0056],
        [-0.0035, -0.0044,  0.0007,  0.0044, -0.0033],
        [-0.0253, -0.0315,  0.0047,  0.0316, -0.0225],
        [ 0.0364,  0.0451, -0.0069, -0.0453,  0.0323],
        [ 0.0480,  0.0602, -0.0091, -0.0601,  0.0432],
        [ 0.0175,  0.0215, -0.0031, -0.0218,  0.0153],
        [-0.0151, -0.0191,  0.0029,  0.0190, -0.0136],
        [-0.0174, -0.0220,  0.0033,  0.0219, -0.0157]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.5282,  1.9075, -0.2866, -1.9113,  1.3642]], device='cuda:0')
--> [loss] 224.3806915283203

---------------------------------- [[#11 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     1.0     |     5.0      |     2.0     | 0.5847 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   1.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.163749223474956 acc: 0.5159
[Epoch 7] loss: 4.596834266582109 acc: 0.5796
[Epoch 11] loss: 3.476605395679279 acc: 0.5872
[Epoch 15] loss: 2.2786128240092025 acc: 0.5813
[Epoch 19] loss: 1.2873564960096804 acc: 0.5812
[Epoch 23] loss: 0.8095847610717691 acc: 0.5744
[Epoch 27] loss: 0.6002368448788057 acc: 0.5728
[Epoch 31] loss: 0.5156297690980612 acc: 0.5726
[Epoch 35] loss: 0.4490678018945105 acc: 0.5675
[Epoch 39] loss: 0.4251047879471765 acc: 0.5687
[Epoch 43] loss: 0.37680914854664177 acc: 0.5615
[Epoch 47] loss: 0.3659034585484954 acc: 0.5742
[Epoch 51] loss: 0.32877915943770303 acc: 0.571
[Epoch 55] loss: 0.32612623066386526 acc: 0.5713
[Epoch 59] loss: 0.3104693320177286 acc: 0.5717
[Epoch 63] loss: 0.28848694149843035 acc: 0.5672
[Epoch 67] loss: 0.2841655988522503 acc: 0.5713
[Epoch 71] loss: 0.26100653739970014 acc: 0.5729
--> [test] acc: 0.5675
--> [accuracy] finished 0.5675
new state: tensor([704.,   1.,   1.,   4.,   2.], device='cuda:0')
new reward: 0.5675
--> [reward] 0.5675
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     1.0     |     4.0      |     2.0     | 0.5675 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.1686011028411745 acc: 0.5231
[Epoch 7] loss: 4.5909085747835885 acc: 0.5745
[Epoch 11] loss: 3.4489835099033685 acc: 0.5934
[Epoch 15] loss: 2.2464035119089627 acc: 0.5864
[Epoch 19] loss: 1.267061022236524 acc: 0.5734
[Epoch 23] loss: 0.7993145835731188 acc: 0.583
[Epoch 27] loss: 0.5749314578959857 acc: 0.574
[Epoch 31] loss: 0.5097112465516457 acc: 0.5758
[Epoch 35] loss: 0.4522230684509515 acc: 0.5797
[Epoch 39] loss: 0.4134132135040162 acc: 0.5753
[Epoch 43] loss: 0.3770860829188124 acc: 0.5641
[Epoch 47] loss: 0.3463681919995667 acc: 0.5726
[Epoch 51] loss: 0.3527130246486353 acc: 0.5646
[Epoch 55] loss: 0.3117526819229202 acc: 0.5689
[Epoch 59] loss: 0.30036747107363265 acc: 0.5751
[Epoch 63] loss: 0.29851613559848283 acc: 0.5722
[Epoch 67] loss: 0.2815624531763403 acc: 0.5685
[Epoch 71] loss: 0.25974809659211456 acc: 0.5691
--> [test] acc: 0.5709
--> [accuracy] finished 0.5709
new state: tensor([736.,   1.,   1.,   4.,   2.], device='cuda:0')
new reward: 0.5709
--> [reward] 0.5709
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     1.0     |     4.0      |     2.0     | 0.5709 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.120977286487589 acc: 0.5159
[Epoch 7] loss: 4.558601121463434 acc: 0.5656
[Epoch 11] loss: 3.464377125815662 acc: 0.5857
[Epoch 15] loss: 2.2757194932083338 acc: 0.5941
[Epoch 19] loss: 1.2802785675010413 acc: 0.5751
[Epoch 23] loss: 0.8020683308143902 acc: 0.5778
[Epoch 27] loss: 0.6085052453172024 acc: 0.5766
[Epoch 31] loss: 0.5064035477378713 acc: 0.5701
[Epoch 35] loss: 0.45834342811894996 acc: 0.5778
[Epoch 39] loss: 0.4215873877572663 acc: 0.5631
[Epoch 43] loss: 0.3894972678683603 acc: 0.5838
[Epoch 47] loss: 0.3512806190139688 acc: 0.573
[Epoch 51] loss: 0.3412017578406788 acc: 0.5687
[Epoch 55] loss: 0.32257298324995526 acc: 0.5741
[Epoch 59] loss: 0.3092041332822512 acc: 0.5663
[Epoch 63] loss: 0.2932706597211111 acc: 0.5722
[Epoch 67] loss: 0.28417862283156425 acc: 0.5763
[Epoch 71] loss: 0.27404666892812607 acc: 0.5705
--> [test] acc: 0.5695
--> [accuracy] finished 0.5695
new state: tensor([704.,   1.,   1.,   4.,   2.], device='cuda:0')
new reward: 0.5695
--> [reward] 0.5695
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     1.0     |     4.0      |     2.0     | 0.5695 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.155950303882589 acc: 0.5242
[Epoch 7] loss: 4.597696187855948 acc: 0.576
[Epoch 11] loss: 3.4723314218356482 acc: 0.5927
[Epoch 15] loss: 2.278865070065574 acc: 0.5889
[Epoch 19] loss: 1.2802901310879555 acc: 0.5807
[Epoch 23] loss: 0.7918189409215127 acc: 0.5721
[Epoch 27] loss: 0.5955997280528783 acc: 0.5769
[Epoch 31] loss: 0.5045453962939017 acc: 0.5816
[Epoch 35] loss: 0.4408475941290026 acc: 0.5761
[Epoch 39] loss: 0.42082471072273636 acc: 0.5669
[Epoch 43] loss: 0.37758173861676625 acc: 0.5822
[Epoch 47] loss: 0.3634072438077739 acc: 0.5622
[Epoch 51] loss: 0.350739742157614 acc: 0.5753
[Epoch 55] loss: 0.3161430115218434 acc: 0.5725
[Epoch 59] loss: 0.3002985758907007 acc: 0.5617
[Epoch 63] loss: 0.2915023748799825 acc: 0.5663
[Epoch 67] loss: 0.2856174397194172 acc: 0.5755
[Epoch 71] loss: 0.27826744281803556 acc: 0.5692
--> [test] acc: 0.5765
--> [accuracy] finished 0.5765
new state: tensor([736.,   1.,   1.,   4.,   2.], device='cuda:0')
new reward: 0.5765
--> [reward] 0.5765
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     1.0     |     4.0      |     2.0     | 0.5765 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   1.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   1.,   2.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.911432065317393 acc: 0.5493
[Epoch 7] loss: 4.1253177985510865 acc: 0.6072
[Epoch 11] loss: 2.6491626453064288 acc: 0.6169
[Epoch 15] loss: 1.3713865071878104 acc: 0.6134
[Epoch 19] loss: 0.7316203470320424 acc: 0.617
[Epoch 23] loss: 0.5127902282711566 acc: 0.6039
[Epoch 27] loss: 0.40014741027160833 acc: 0.6017
[Epoch 31] loss: 0.36041080486743954 acc: 0.6045
[Epoch 35] loss: 0.33087002133945825 acc: 0.6099
[Epoch 39] loss: 0.29354753557478297 acc: 0.6121
[Epoch 43] loss: 0.2631653897854907 acc: 0.6081
[Epoch 47] loss: 0.26753220020238394 acc: 0.6076
[Epoch 51] loss: 0.24478429015916403 acc: 0.6082
[Epoch 55] loss: 0.24132350306300557 acc: 0.6059
[Epoch 59] loss: 0.21278692257669193 acc: 0.6121
[Epoch 63] loss: 0.21169109872120725 acc: 0.6071
[Epoch 67] loss: 0.21097045691440935 acc: 0.6013
[Epoch 71] loss: 0.19259798319062307 acc: 0.6059
--> [test] acc: 0.6069
--> [accuracy] finished 0.6069
new state: tensor([736.,   1.,   2.,   4.,   2.], device='cuda:0')
new reward: 0.6069
--> [reward] 0.6069
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     2.0     |     4.0      |     2.0     | 0.6069 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   2.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   2.,   2.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.316794410995815 acc: 0.6114
[Epoch 7] loss: 3.200642405416045 acc: 0.7006
[Epoch 11] loss: 1.5729856924213412 acc: 0.697
[Epoch 15] loss: 0.6659117093752793 acc: 0.6945
[Epoch 19] loss: 0.39102032425744304 acc: 0.6864
[Epoch 23] loss: 0.3153103551139002 acc: 0.6767
[Epoch 27] loss: 0.25457921273568096 acc: 0.688
[Epoch 31] loss: 0.2458269079537381 acc: 0.6893
[Epoch 35] loss: 0.21439669105698309 acc: 0.6833
[Epoch 39] loss: 0.18494312587620504 acc: 0.6869
[Epoch 43] loss: 0.19162957614187695 acc: 0.6905
[Epoch 47] loss: 0.169581307298349 acc: 0.6867
[Epoch 51] loss: 0.1530042422478042 acc: 0.6873
[Epoch 55] loss: 0.15205329888300192 acc: 0.6859
[Epoch 59] loss: 0.13773324502759454 acc: 0.6838
[Epoch 63] loss: 0.14546135762263365 acc: 0.681
[Epoch 67] loss: 0.13295181094290084 acc: 0.6773
[Epoch 71] loss: 0.12474918903266687 acc: 0.683
--> [test] acc: 0.6803
--> [accuracy] finished 0.6803
new state: tensor([736.,   2.,   2.,   4.,   2.], device='cuda:0')
new reward: 0.6803
--> [reward] 0.6803
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     4.0      |     2.0     | 0.6803 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   2.,   2.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.267226630921864 acc: 0.6293
[Epoch 7] loss: 3.1256755024880705 acc: 0.6864
[Epoch 11] loss: 1.4876093463900755 acc: 0.6845
[Epoch 15] loss: 0.6435970753964866 acc: 0.6958
[Epoch 19] loss: 0.3863295266390457 acc: 0.6906
[Epoch 23] loss: 0.2966142269637426 acc: 0.6987
[Epoch 27] loss: 0.273330118361851 acc: 0.6969
[Epoch 31] loss: 0.2296768354104303 acc: 0.6919
[Epoch 35] loss: 0.2070090737946503 acc: 0.6939
[Epoch 39] loss: 0.19450670207524315 acc: 0.6939
[Epoch 43] loss: 0.18051923333626727 acc: 0.682
[Epoch 47] loss: 0.1720518010699421 acc: 0.6838
[Epoch 51] loss: 0.15880635397180037 acc: 0.691
[Epoch 55] loss: 0.14902320466911817 acc: 0.6983
[Epoch 59] loss: 0.13776734943100896 acc: 0.6775
[Epoch 63] loss: 0.1384744288785683 acc: 0.6847
[Epoch 67] loss: 0.12368434731005346 acc: 0.6892
[Epoch 71] loss: 0.1334155255867897 acc: 0.6907
--> [test] acc: 0.683
--> [accuracy] finished 0.683
new state: tensor([768.,   2.,   2.,   4.,   2.], device='cuda:0')
new reward: 0.683
--> [reward] 0.683
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     2.0     |     4.0      |     2.0     | 0.683  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   2.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([768.,   2.,   3.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.392755742573067 acc: 0.6103
[Epoch 7] loss: 3.3349926974767308 acc: 0.6468
[Epoch 11] loss: 1.656524007030003 acc: 0.6689
[Epoch 15] loss: 0.6918904443683527 acc: 0.6691
[Epoch 19] loss: 0.40999767453173924 acc: 0.6588
[Epoch 23] loss: 0.31065282089840573 acc: 0.6526
[Epoch 27] loss: 0.26894017814865806 acc: 0.6489
[Epoch 31] loss: 0.23865619274523214 acc: 0.653
[Epoch 35] loss: 0.21534882496346902 acc: 0.645
[Epoch 39] loss: 0.201882649512004 acc: 0.6489
[Epoch 43] loss: 0.17552931264609747 acc: 0.6504
[Epoch 47] loss: 0.16891254216094342 acc: 0.6547
[Epoch 51] loss: 0.161935001059109 acc: 0.6464
[Epoch 55] loss: 0.15469400139878053 acc: 0.6468
[Epoch 59] loss: 0.1434736638044095 acc: 0.6533
[Epoch 63] loss: 0.13254636047435375 acc: 0.6519
[Epoch 67] loss: 0.12928791693233124 acc: 0.6416
[Epoch 71] loss: 0.1275660137906718 acc: 0.6417
--> [test] acc: 0.6564
--> [accuracy] finished 0.6564
new state: tensor([768.,   2.,   3.,   4.,   2.], device='cuda:0')
new reward: 0.6564
--> [reward] 0.6564
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     3.0     |     4.0      |     2.0     | 0.6564 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   3.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   2.,   3.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.4152759443158685 acc: 0.5922
[Epoch 7] loss: 3.3362276208827564 acc: 0.656
[Epoch 11] loss: 1.6638306551386632 acc: 0.6597
[Epoch 15] loss: 0.7078215454173896 acc: 0.659
[Epoch 19] loss: 0.417663753370914 acc: 0.6505
[Epoch 23] loss: 0.3245354202192496 acc: 0.6516
[Epoch 27] loss: 0.2716956675371817 acc: 0.6428
[Epoch 31] loss: 0.23952565281211263 acc: 0.6494
[Epoch 35] loss: 0.21562507313669033 acc: 0.6536
[Epoch 39] loss: 0.1902874520267634 acc: 0.6474
[Epoch 43] loss: 0.19197477739008947 acc: 0.6475
[Epoch 47] loss: 0.17261835724553642 acc: 0.6467
[Epoch 51] loss: 0.16101082287076146 acc: 0.6499
[Epoch 55] loss: 0.15219364842623853 acc: 0.6525
[Epoch 59] loss: 0.1443958838001046 acc: 0.6513
[Epoch 63] loss: 0.13726791622988938 acc: 0.6441
[Epoch 67] loss: 0.13127573041121482 acc: 0.6548
[Epoch 71] loss: 0.13014536208170643 acc: 0.6506
--> [test] acc: 0.6524
--> [accuracy] finished 0.6524
new state: tensor([736.,   2.,   3.,   4.,   2.], device='cuda:0')
new reward: 0.6524
--> [reward] 0.6524
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     3.0     |     4.0      |     2.0     | 0.6524 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   3.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   2.,   3.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.46199812837269 acc: 0.5987
[Epoch 7] loss: 3.3970748790542182 acc: 0.669
[Epoch 11] loss: 1.6983484964998787 acc: 0.6616
[Epoch 15] loss: 0.7124159736534976 acc: 0.6558
[Epoch 19] loss: 0.43023572709706737 acc: 0.6482
[Epoch 23] loss: 0.3212887701409323 acc: 0.65
[Epoch 27] loss: 0.2683356397611368 acc: 0.6493
[Epoch 31] loss: 0.24134090663555563 acc: 0.6497
[Epoch 35] loss: 0.21568857950141743 acc: 0.6506
[Epoch 39] loss: 0.20018769294509423 acc: 0.6396
[Epoch 43] loss: 0.18253814549449726 acc: 0.6462
[Epoch 47] loss: 0.17331506591052045 acc: 0.653
[Epoch 51] loss: 0.15038441630470975 acc: 0.6512
[Epoch 55] loss: 0.15442517665458982 acc: 0.6484
[Epoch 59] loss: 0.14431221762319546 acc: 0.6406
[Epoch 63] loss: 0.14053290795363352 acc: 0.6416
[Epoch 67] loss: 0.13173412958212444 acc: 0.6468
[Epoch 71] loss: 0.12987548195937995 acc: 0.6483
--> [test] acc: 0.649
--> [accuracy] finished 0.649
new state: tensor([736.,   2.,   3.,   4.,   2.], device='cuda:0')
new reward: 0.649
--> [reward] 0.649
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     3.0     |     4.0      |     2.0     | 0.649  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   3.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   2.,   3.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.42263668135304 acc: 0.596
[Epoch 7] loss: 3.412818162871139 acc: 0.6529
[Epoch 11] loss: 1.7947521683810008 acc: 0.6582
[Epoch 15] loss: 0.7675968830276023 acc: 0.6594
[Epoch 19] loss: 0.4428299214224071 acc: 0.6439
[Epoch 23] loss: 0.331204106278546 acc: 0.6386
[Epoch 27] loss: 0.2841586985070344 acc: 0.6476
[Epoch 31] loss: 0.2504353567009882 acc: 0.6473
[Epoch 35] loss: 0.21682033686996308 acc: 0.6543
[Epoch 39] loss: 0.21058033203558468 acc: 0.6431
[Epoch 43] loss: 0.18478209905021484 acc: 0.6508
[Epoch 47] loss: 0.17573828346160292 acc: 0.6447
[Epoch 51] loss: 0.17444117463019956 acc: 0.6409
[Epoch 55] loss: 0.14478739587169692 acc: 0.6541
[Epoch 59] loss: 0.16381556239774656 acc: 0.646
[Epoch 63] loss: 0.14070661491750147 acc: 0.65
[Epoch 67] loss: 0.13565860539936767 acc: 0.6491
[Epoch 71] loss: 0.1319986543846805 acc: 0.6381
--> [test] acc: 0.6487
--> [accuracy] finished 0.6487
new state: tensor([736.,   2.,   3.,   5.,   2.], device='cuda:0')
new reward: 0.6487
--> [reward] 0.6487
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     3.0     |     5.0      |     2.0     | 0.6487 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   3.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   1.,   3.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.994039880955006 acc: 0.5354
[Epoch 7] loss: 4.35289437798283 acc: 0.5647
[Epoch 11] loss: 2.9992437632492437 acc: 0.5852
[Epoch 15] loss: 1.6470079274700427 acc: 0.5826
[Epoch 19] loss: 0.8674503021761585 acc: 0.5846
[Epoch 23] loss: 0.5558204018818144 acc: 0.5814
[Epoch 27] loss: 0.44319703486388373 acc: 0.588
[Epoch 31] loss: 0.385210526340148 acc: 0.5734
[Epoch 35] loss: 0.34000338283617554 acc: 0.5715
[Epoch 39] loss: 0.3074893365873743 acc: 0.5797
[Epoch 43] loss: 0.29524109614512806 acc: 0.5764
[Epoch 47] loss: 0.261268067476161 acc: 0.5758
[Epoch 51] loss: 0.24749675137288582 acc: 0.5686
[Epoch 55] loss: 0.22852833039910936 acc: 0.5694
[Epoch 59] loss: 0.23038998588233653 acc: 0.5674
[Epoch 63] loss: 0.2021725265514058 acc: 0.5713
[Epoch 67] loss: 0.21202296819752248 acc: 0.5788
[Epoch 71] loss: 0.1987111934453554 acc: 0.5783
--> [test] acc: 0.5776
--> [accuracy] finished 0.5776
new state: tensor([736.,   1.,   3.,   5.,   2.], device='cuda:0')
new reward: 0.5776
--> [reward] 0.5776
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     3.0     |     5.0      |     2.0     | 0.5776 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   3.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   1.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.7380371112042985 acc: 0.5413
[Epoch 7] loss: 3.974230352874912 acc: 0.6196
[Epoch 11] loss: 2.4411377099621325 acc: 0.6155
[Epoch 15] loss: 1.1606896897430157 acc: 0.6024
[Epoch 19] loss: 0.6244397513625567 acc: 0.6073
[Epoch 23] loss: 0.4393082486270258 acc: 0.5951
[Epoch 27] loss: 0.35167506127320514 acc: 0.603
[Epoch 31] loss: 0.3171246629660887 acc: 0.6092
[Epoch 35] loss: 0.2864329519555392 acc: 0.5977
[Epoch 39] loss: 0.2615880581057247 acc: 0.6024
[Epoch 43] loss: 0.24591291172053578 acc: 0.602
[Epoch 47] loss: 0.22440948156053986 acc: 0.6028
[Epoch 51] loss: 0.20165459331853883 acc: 0.5986
[Epoch 55] loss: 0.20390609611549873 acc: 0.5998
[Epoch 59] loss: 0.1966639072884379 acc: 0.606
[Epoch 63] loss: 0.1824947762996187 acc: 0.5979
[Epoch 67] loss: 0.16880101884198387 acc: 0.5982
[Epoch 71] loss: 0.16328741627199875 acc: 0.5987
--> [test] acc: 0.5955
--> [accuracy] finished 0.5955
new state: tensor([736.,   1.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.5955
--> [reward] 0.5955
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     4.0     |     5.0      |     2.0     | 0.5955 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   1.,   4.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.711029050142868 acc: 0.5561
[Epoch 7] loss: 3.8942559061147977 acc: 0.6227
[Epoch 11] loss: 2.3364490219546705 acc: 0.6207
[Epoch 15] loss: 1.0969966111318838 acc: 0.6194
[Epoch 19] loss: 0.5794317929974526 acc: 0.6147
[Epoch 23] loss: 0.4192887698455006 acc: 0.5981
[Epoch 27] loss: 0.3453507901400404 acc: 0.6179
[Epoch 31] loss: 0.29939651448289145 acc: 0.6209
[Epoch 35] loss: 0.26533385055482656 acc: 0.611
[Epoch 39] loss: 0.24854363510956812 acc: 0.6102
[Epoch 43] loss: 0.23288941977169278 acc: 0.6155
[Epoch 47] loss: 0.21236267583349439 acc: 0.6095
[Epoch 51] loss: 0.19683186889714216 acc: 0.6067
[Epoch 55] loss: 0.19402152636205144 acc: 0.609
[Epoch 59] loss: 0.1785752856556107 acc: 0.6153
[Epoch 63] loss: 0.1656857820089592 acc: 0.614
[Epoch 67] loss: 0.1713516133625413 acc: 0.6158
[Epoch 71] loss: 0.15739459992217286 acc: 0.6144
--> [test] acc: 0.6148
--> [accuracy] finished 0.6148
new state: tensor([736.,   1.,   4.,   5.,   3.], device='cuda:0')
new reward: 0.6148
--> [reward] 0.6148
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     4.0     |     5.0      |     3.0     | 0.6148 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0920, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0908]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3968, -2.3976, -2.3989, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3982, -2.3980]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   4.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   1.,   4.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.684238355635377 acc: 0.5715
[Epoch 7] loss: 3.877998358758209 acc: 0.6305
[Epoch 11] loss: 2.3407609019133138 acc: 0.6358
[Epoch 15] loss: 1.116211522532546 acc: 0.6343
[Epoch 19] loss: 0.5824965252719648 acc: 0.6261
[Epoch 23] loss: 0.41550089582048183 acc: 0.6298
[Epoch 27] loss: 0.36027473019783757 acc: 0.6264
[Epoch 31] loss: 0.29723127203924427 acc: 0.6253
[Epoch 35] loss: 0.2681617353949934 acc: 0.63
[Epoch 39] loss: 0.24080677089445732 acc: 0.6231
[Epoch 43] loss: 0.22726062840551062 acc: 0.6238
[Epoch 47] loss: 0.21337634604066955 acc: 0.6234
[Epoch 51] loss: 0.1996455983606541 acc: 0.6203
[Epoch 55] loss: 0.19679057732691316 acc: 0.6232
[Epoch 59] loss: 0.17186497006317614 acc: 0.6208
[Epoch 63] loss: 0.17219857935248242 acc: 0.6232
[Epoch 67] loss: 0.16541836471022928 acc: 0.6203
[Epoch 71] loss: 0.15054487311960105 acc: 0.6236
--> [test] acc: 0.6182
--> [accuracy] finished 0.6182
new state: tensor([736.,   1.,   4.,   5.,   4.], device='cuda:0')
new reward: 0.6182
--> [reward] 0.6182
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1907]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3814]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6176]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6513]], device='cuda:0')
------ ------
delta_t: tensor([[0.6176]], device='cuda:0')
rewards[i]: 0.6182
values[i+1]: tensor([[0.0334]], device='cuda:0')
values[i]: tensor([[0.0337]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6176]], device='cuda:0')
delta_t: tensor([[0.6176]], device='cuda:0')
------ ------
policy_loss: 1.457110047340393
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6176]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.9417]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.5020]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2255]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.2596]], device='cuda:0')
------ ------
delta_t: tensor([[0.6141]], device='cuda:0')
rewards[i]: 0.6148
values[i+1]: tensor([[0.0337]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0340]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2255]], device='cuda:0')
delta_t: tensor([[0.6141]], device='cuda:0')
------ ------
policy_loss: 4.372222423553467
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2255]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.5761]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.2689]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8080]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.8425]], device='cuda:0')
------ ------
delta_t: tensor([[0.5947]], device='cuda:0')
rewards[i]: 0.5955
values[i+1]: tensor([[0.0340]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0345]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8080]], device='cuda:0')
delta_t: tensor([[0.5947]], device='cuda:0')
------ ------
policy_loss: 8.683470726013184
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8080]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.3772]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.6021]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.3669]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.4017]], device='cuda:0')
------ ------
delta_t: tensor([[0.5769]], device='cuda:0')
rewards[i]: 0.5776
values[i+1]: tensor([[0.0345]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.3669]], device='cuda:0')
delta_t: tensor([[0.5769]], device='cuda:0')
------ ------
policy_loss: 14.334280014038086
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.3669]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[9.8517]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.9491]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9915]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.0263]], device='cuda:0')
------ ------
delta_t: tensor([[0.6483]], device='cuda:0')
rewards[i]: 0.6487
values[i+1]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9915]], device='cuda:0')
delta_t: tensor([[0.6483]], device='cuda:0')
------ ------
policy_loss: 21.48388671875
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9915]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[16.3689]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.0343]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6103]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6451]], device='cuda:0')
------ ------
delta_t: tensor([[0.6487]], device='cuda:0')
rewards[i]: 0.649
values[i+1]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6103]], device='cuda:0')
delta_t: tensor([[0.6487]], device='cuda:0')
------ ------
policy_loss: 30.117549896240234
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6103]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[25.2994]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.8611]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2262]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2610]], device='cuda:0')
------ ------
delta_t: tensor([[0.6520]], device='cuda:0')
rewards[i]: 0.6524
values[i+1]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2262]], device='cuda:0')
delta_t: tensor([[0.6520]], device='cuda:0')
------ ------
policy_loss: 40.22610092163086
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2262]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.0115]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.4242]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8399]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8748]], device='cuda:0')
------ ------
delta_t: tensor([[0.6559]], device='cuda:0')
rewards[i]: 0.6564
values[i+1]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0350]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8399]], device='cuda:0')
delta_t: tensor([[0.6559]], device='cuda:0')
------ ------
policy_loss: 51.80706024169922
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8399]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[51.9954]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.9678]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4743]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.5091]], device='cuda:0')
------ ------
delta_t: tensor([[0.6828]], device='cuda:0')
rewards[i]: 0.683
values[i+1]: tensor([[0.0350]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4743]], device='cuda:0')
delta_t: tensor([[0.6828]], device='cuda:0')
------ ------
policy_loss: 64.90364074707031
log_probs[i]: tensor([[-2.3968]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4743]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[70.5988]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.2070]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.0998]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1343]], device='cuda:0')
------ ------
delta_t: tensor([[0.6802]], device='cuda:0')
rewards[i]: 0.6803
values[i+1]: tensor([[0.0348]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0345]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.0998]], device='cuda:0')
delta_t: tensor([[0.6802]], device='cuda:0')
------ ------
policy_loss: 79.51261138916016
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.0998]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[92.6783]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[44.1589]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.6452]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.6798]], device='cuda:0')
------ ------
delta_t: tensor([[0.6065]], device='cuda:0')
rewards[i]: 0.6069
values[i+1]: tensor([[0.0345]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0346]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.6452]], device='cuda:0')
delta_t: tensor([[0.6065]], device='cuda:0')
------ ------
policy_loss: 95.42247772216797
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.6452]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[118.2775]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[51.1985]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.1553]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1895]], device='cuda:0')
------ ------
delta_t: tensor([[0.5765]], device='cuda:0')
rewards[i]: 0.5765
values[i+1]: tensor([[0.0346]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0342]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.1553]], device='cuda:0')
delta_t: tensor([[0.5765]], device='cuda:0')
------ ------
policy_loss: 112.54813385009766
log_probs[i]: tensor([[-2.3968]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.1553]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[147.5639]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[58.5727]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.6533]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.6871]], device='cuda:0')
------ ------
delta_t: tensor([[0.5695]], device='cuda:0')
rewards[i]: 0.5695
values[i+1]: tensor([[0.0342]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0339]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.6533]], device='cuda:0')
delta_t: tensor([[0.5695]], device='cuda:0')
------ ------
policy_loss: 130.87315368652344
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.6533]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[180.7576]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[66.3874]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.1478]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1812]], device='cuda:0')
------ ------
delta_t: tensor([[0.5711]], device='cuda:0')
rewards[i]: 0.5709
values[i+1]: tensor([[0.0339]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0333]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.1478]], device='cuda:0')
delta_t: tensor([[0.5711]], device='cuda:0')
------ ------
policy_loss: 150.37774658203125
log_probs[i]: tensor([[-2.3968]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.1478]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[218.0330]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.5507]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6343]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6669]], device='cuda:0')
------ ------
delta_t: tensor([[0.5679]], device='cuda:0')
rewards[i]: 0.5675
values[i+1]: tensor([[0.0333]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0326]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6343]], device='cuda:0')
delta_t: tensor([[0.5679]], device='cuda:0')
------ ------
policy_loss: 171.05844116210938
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6343]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 171.05844116210938
value_loss: 218.032958984375
loss: 280.0749206542969



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-2.8215e-03, -4.4150e-06, -4.7109e-06, -1.9307e-05, -9.5225e-06],
        [ 6.0512e-02,  9.5036e-05,  1.0131e-04,  4.0989e-04,  2.0186e-04],
        [-8.8856e-04, -1.4476e-06, -1.6299e-06, -5.8984e-06, -2.8816e-06],
        [ 2.5471e-01,  4.1044e-04,  4.5089e-04,  1.6893e-03,  8.2985e-04],
        [ 1.1981e+00,  1.9417e-03,  2.1246e-03,  7.8936e-03,  3.8818e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.1841e-05,  2.6393e-05, -3.9620e-06, -2.6792e-05,  1.9125e-05],
        [-4.6821e-04, -5.6507e-04,  8.4897e-05,  5.7372e-04, -4.0977e-04],
        [ 6.8858e-06,  8.2837e-06, -1.2486e-06, -8.4171e-06,  6.0120e-06],
        [-1.9716e-03, -2.3723e-03,  3.5737e-04,  2.4101e-03, -1.7223e-03],
        [-9.2714e-03, -1.1147e-02,  1.6801e-03,  1.1326e-02, -8.0966e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.3468, -0.3394,  0.3464, -0.3038,  0.1943],
        [-0.1084,  0.1060, -0.1083,  0.0946, -0.0603],
        [-0.1669,  0.1634, -0.1667,  0.1466, -0.0940],
        [-0.0032,  0.0031, -0.0032,  0.0028, -0.0017],
        [-0.0683,  0.0668, -0.0682,  0.0598, -0.0383]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0286,  0.0338, -0.0049, -0.0345,  0.0246],
        [ 0.0739,  0.0867, -0.0135, -0.0883,  0.0641],
        [-0.0215, -0.0253,  0.0038,  0.0258, -0.0187],
        [-0.0017, -0.0022,  0.0004,  0.0021, -0.0015],
        [-0.0341, -0.0401,  0.0062,  0.0409, -0.0294],
        [ 0.0368,  0.0427, -0.0065, -0.0438,  0.0312],
        [ 0.0086,  0.0108, -0.0015, -0.0108,  0.0080],
        [-0.0180, -0.0212,  0.0031,  0.0216, -0.0157],
        [-0.0336, -0.0396,  0.0061,  0.0404, -0.0290],
        [-0.0243, -0.0285,  0.0043,  0.0291, -0.0209],
        [-0.0146, -0.0171,  0.0026,  0.0174, -0.0128]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.0459,  2.4055, -0.3711, -2.4551,  1.7631]], device='cuda:0')
--> [loss] 280.0749206542969

---------------------------------- [[#12 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     4.0     |     5.0      |     4.0     | 0.6182 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   4.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   1.,   4.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.709488089432192 acc: 0.5579
[Epoch 7] loss: 3.86422370004532 acc: 0.6198
[Epoch 11] loss: 2.335586684972734 acc: 0.629
[Epoch 15] loss: 1.1005258844674701 acc: 0.6156
[Epoch 19] loss: 0.5723129253062751 acc: 0.6171
[Epoch 23] loss: 0.42752915213022696 acc: 0.6096
[Epoch 27] loss: 0.3450594440416988 acc: 0.614
[Epoch 31] loss: 0.31319618149115075 acc: 0.6272
[Epoch 35] loss: 0.2682401262261831 acc: 0.6222
[Epoch 39] loss: 0.2583795573914905 acc: 0.6207
[Epoch 43] loss: 0.23107651025866685 acc: 0.62
[Epoch 47] loss: 0.21633830471107227 acc: 0.6235
[Epoch 51] loss: 0.2023137573610105 acc: 0.6108
[Epoch 55] loss: 0.19156582541215947 acc: 0.6173
[Epoch 59] loss: 0.18272403003100088 acc: 0.616
[Epoch 63] loss: 0.16913032738635997 acc: 0.6178
[Epoch 67] loss: 0.1703693152398176 acc: 0.6205
[Epoch 71] loss: 0.1640291485862206 acc: 0.6128
--> [test] acc: 0.6224
--> [accuracy] finished 0.6224
new state: tensor([736.,   1.,   4.,   5.,   4.], device='cuda:0')
new reward: 0.6224
--> [reward] 0.6224
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     4.0     |     5.0      |     4.0     | 0.6224 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   4.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   2.,   4.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.208044737043893 acc: 0.6093
[Epoch 7] loss: 3.0305289884509943 acc: 0.6659
[Epoch 11] loss: 1.3677216101333003 acc: 0.6789
[Epoch 15] loss: 0.5582005458471873 acc: 0.6791
[Epoch 19] loss: 0.35475094588783085 acc: 0.6803
[Epoch 23] loss: 0.2809695718748033 acc: 0.6716
[Epoch 27] loss: 0.2334095566602581 acc: 0.6798
[Epoch 31] loss: 0.20206846704330209 acc: 0.6788
[Epoch 35] loss: 0.18501866405324824 acc: 0.6794
[Epoch 39] loss: 0.17564762515418442 acc: 0.6739
[Epoch 43] loss: 0.1527729474396333 acc: 0.6819
[Epoch 47] loss: 0.14558915138039785 acc: 0.6781
[Epoch 51] loss: 0.1419070663116396 acc: 0.6777
[Epoch 55] loss: 0.132873159264574 acc: 0.672
[Epoch 59] loss: 0.12802702476820715 acc: 0.6763
[Epoch 63] loss: 0.11719412327555896 acc: 0.6629
[Epoch 67] loss: 0.10687901245192964 acc: 0.6607
[Epoch 71] loss: 0.10853325632115578 acc: 0.6652
--> [test] acc: 0.6833
--> [accuracy] finished 0.6833
new state: tensor([736.,   2.,   4.,   5.,   4.], device='cuda:0')
new reward: 0.6833
--> [reward] 0.6833
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     5.0      |     4.0     | 0.6833 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   3.,   4.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.896806816615717 acc: 0.6678
[Epoch 7] loss: 2.44945877981003 acc: 0.7268
[Epoch 11] loss: 0.8625935048598539 acc: 0.7113
[Epoch 15] loss: 0.37862395498510976 acc: 0.7151
[Epoch 19] loss: 0.26255444094510105 acc: 0.7163
[Epoch 23] loss: 0.2134962089102515 acc: 0.7069
[Epoch 27] loss: 0.17680122232888743 acc: 0.7193
[Epoch 31] loss: 0.15926324304454076 acc: 0.7213
[Epoch 35] loss: 0.14676500988178565 acc: 0.7096
[Epoch 39] loss: 0.13032658775622868 acc: 0.7169
[Epoch 43] loss: 0.12520316969770032 acc: 0.701
[Epoch 47] loss: 0.10817229863740932 acc: 0.7093
[Epoch 51] loss: 0.1107903077257285 acc: 0.7031
[Epoch 55] loss: 0.0996805641804929 acc: 0.7113
[Epoch 59] loss: 0.09520989876982454 acc: 0.7065
[Epoch 63] loss: 0.08874100893545334 acc: 0.7056
[Epoch 67] loss: 0.09191520404620834 acc: 0.7077
[Epoch 71] loss: 0.07336774314476815 acc: 0.6998
--> [test] acc: 0.7082
--> [accuracy] finished 0.7082
new state: tensor([736.,   3.,   4.,   5.,   4.], device='cuda:0')
new reward: 0.7082
--> [reward] 0.7082
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     5.0      |     4.0     | 0.7082 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   3.,   4.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.822089858981959 acc: 0.6427
[Epoch 7] loss: 2.417459812844196 acc: 0.7201
[Epoch 11] loss: 0.8479245889198292 acc: 0.7162
[Epoch 15] loss: 0.39255892111422 acc: 0.7132
[Epoch 19] loss: 0.25237862045979104 acc: 0.7233
[Epoch 23] loss: 0.21744599553477734 acc: 0.7115
[Epoch 27] loss: 0.1876298361773486 acc: 0.7072
[Epoch 31] loss: 0.16200336356602058 acc: 0.7073
[Epoch 35] loss: 0.14024140216324413 acc: 0.7168
[Epoch 39] loss: 0.13389797419157173 acc: 0.6981
[Epoch 43] loss: 0.12301901303936759 acc: 0.7062
[Epoch 47] loss: 0.11295680429362466 acc: 0.7061
[Epoch 51] loss: 0.11697398136605693 acc: 0.7096
[Epoch 55] loss: 0.10170023675407748 acc: 0.7051
[Epoch 59] loss: 0.08875065106931893 acc: 0.704
[Epoch 63] loss: 0.10742367264073904 acc: 0.7028
[Epoch 67] loss: 0.08198976767061235 acc: 0.6993
[Epoch 71] loss: 0.08106221018663948 acc: 0.7084
--> [test] acc: 0.6924
--> [accuracy] finished 0.6924
new state: tensor([736.,   3.,   4.,   5.,   4.], device='cuda:0')
new reward: 0.6924
--> [reward] 0.6924
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     5.0      |     4.0     | 0.6924 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   2.,   4.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.171573854468363 acc: 0.6283
[Epoch 7] loss: 3.0202854677386908 acc: 0.693
[Epoch 11] loss: 1.3452518112252436 acc: 0.684
[Epoch 15] loss: 0.5597637336167609 acc: 0.6797
[Epoch 19] loss: 0.35041011234178493 acc: 0.683
[Epoch 23] loss: 0.26939772691368064 acc: 0.6801
[Epoch 27] loss: 0.2370603673750787 acc: 0.6802
[Epoch 31] loss: 0.20594801466025964 acc: 0.6844
[Epoch 35] loss: 0.1814148718243479 acc: 0.6699
[Epoch 39] loss: 0.16831970791025158 acc: 0.6803
[Epoch 43] loss: 0.16457269285493495 acc: 0.6749
[Epoch 47] loss: 0.1410486492866655 acc: 0.6736
[Epoch 51] loss: 0.13099381375981642 acc: 0.6778
[Epoch 55] loss: 0.12628733328855632 acc: 0.6693
[Epoch 59] loss: 0.125294987554126 acc: 0.6819
[Epoch 63] loss: 0.11212641550728199 acc: 0.6631
[Epoch 67] loss: 0.11077773372393787 acc: 0.6754
[Epoch 71] loss: 0.11128063896751922 acc: 0.676
--> [test] acc: 0.6768
--> [accuracy] finished 0.6768
new state: tensor([736.,   2.,   4.,   5.,   4.], device='cuda:0')
new reward: 0.6768
--> [reward] 0.6768
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     5.0      |     4.0     | 0.6768 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   2.,   4.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.200946066995411 acc: 0.5936
[Epoch 7] loss: 3.0038457854324596 acc: 0.6786
[Epoch 11] loss: 1.3335755424445395 acc: 0.6822
[Epoch 15] loss: 0.5628629044470046 acc: 0.6706
[Epoch 19] loss: 0.3472136838952332 acc: 0.6753
[Epoch 23] loss: 0.26671872711966715 acc: 0.6668
[Epoch 27] loss: 0.23643499998199513 acc: 0.6672
[Epoch 31] loss: 0.20677244005834355 acc: 0.6766
[Epoch 35] loss: 0.19298435948179354 acc: 0.6717
[Epoch 39] loss: 0.16457064543157587 acc: 0.6733
[Epoch 43] loss: 0.15528000042537976 acc: 0.6676
[Epoch 47] loss: 0.1512056231627341 acc: 0.6706
[Epoch 51] loss: 0.13692356275616074 acc: 0.6731
[Epoch 55] loss: 0.13358032574896198 acc: 0.6776
[Epoch 59] loss: 0.1299451642724521 acc: 0.6694
[Epoch 63] loss: 0.12050732372738325 acc: 0.6731
[Epoch 67] loss: 0.11540273605081279 acc: 0.6723
[Epoch 71] loss: 0.10402865381911397 acc: 0.6696
--> [test] acc: 0.6676
--> [accuracy] finished 0.6676
new state: tensor([736.,   2.,   4.,   5.,   3.], device='cuda:0')
new reward: 0.6676
--> [reward] 0.6676
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     5.0      |     3.0     | 0.6676 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   2.,   5.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.065916253782599 acc: 0.6575
[Epoch 7] loss: 2.7908587921458436 acc: 0.6858
[Epoch 11] loss: 1.1129522099042946 acc: 0.6907
[Epoch 15] loss: 0.46215908053566884 acc: 0.6786
[Epoch 19] loss: 0.30245259053566875 acc: 0.6797
[Epoch 23] loss: 0.2442261398653202 acc: 0.6788
[Epoch 27] loss: 0.20737793881808173 acc: 0.6719
[Epoch 31] loss: 0.1847243793523106 acc: 0.6817
[Epoch 35] loss: 0.1705776906388876 acc: 0.6743
[Epoch 39] loss: 0.15592306326655553 acc: 0.6871
[Epoch 43] loss: 0.1428723075662923 acc: 0.6718
[Epoch 47] loss: 0.1335521485106281 acc: 0.6799
[Epoch 51] loss: 0.12073400062616066 acc: 0.6822
[Epoch 55] loss: 0.11910883809708993 acc: 0.6825
[Epoch 59] loss: 0.10779720149538896 acc: 0.6724
[Epoch 63] loss: 0.10746094222059069 acc: 0.6804
[Epoch 67] loss: 0.09842210642212187 acc: 0.6801
[Epoch 71] loss: 0.09243410084541896 acc: 0.6687
--> [test] acc: 0.6678
--> [accuracy] finished 0.6678
new state: tensor([736.,   2.,   5.,   5.,   3.], device='cuda:0')
new reward: 0.6678
--> [reward] 0.6678
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     5.0     |     5.0      |     3.0     | 0.6678 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   5.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   2.,   5.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.066208366847709 acc: 0.625
[Epoch 7] loss: 2.8278085981945855 acc: 0.6765
[Epoch 11] loss: 1.0890104976551764 acc: 0.6805
[Epoch 15] loss: 0.4551112097771386 acc: 0.6805
[Epoch 19] loss: 0.2952696129350978 acc: 0.683
[Epoch 23] loss: 0.24354853168013207 acc: 0.6837
[Epoch 27] loss: 0.21237001942275355 acc: 0.6852
[Epoch 31] loss: 0.19132566093074163 acc: 0.675
[Epoch 35] loss: 0.16835132269831873 acc: 0.6767
[Epoch 39] loss: 0.15217360873263605 acc: 0.685
[Epoch 43] loss: 0.14826043490844462 acc: 0.6771
[Epoch 47] loss: 0.13568662328031056 acc: 0.6788
[Epoch 51] loss: 0.12364959947126525 acc: 0.6747
[Epoch 55] loss: 0.1145720350689462 acc: 0.6716
[Epoch 59] loss: 0.11521683326419777 acc: 0.6782
[Epoch 63] loss: 0.09579232798995274 acc: 0.6672
[Epoch 67] loss: 0.10890300446511496 acc: 0.6706
[Epoch 71] loss: 0.09720442177134725 acc: 0.6711
--> [test] acc: 0.6803
--> [accuracy] finished 0.6803
new state: tensor([736.,   2.,   5.,   5.,   2.], device='cuda:0')
new reward: 0.6803
--> [reward] 0.6803
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     5.0     |     5.0      |     2.0     | 0.6803 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0899, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   5.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.247474971787094 acc: 0.6244
[Epoch 7] loss: 3.109089351142459 acc: 0.6731
[Epoch 11] loss: 1.3979034879914178 acc: 0.6795
[Epoch 15] loss: 0.5799468155340541 acc: 0.6691
[Epoch 19] loss: 0.35971369543362913 acc: 0.6672
[Epoch 23] loss: 0.2719597824994484 acc: 0.6676
[Epoch 27] loss: 0.23804976264267322 acc: 0.6566
[Epoch 31] loss: 0.2092176278090805 acc: 0.6707
[Epoch 35] loss: 0.18530104465334846 acc: 0.667
[Epoch 39] loss: 0.17573534379310696 acc: 0.6601
[Epoch 43] loss: 0.16332631290216199 acc: 0.6598
[Epoch 47] loss: 0.14759091206866762 acc: 0.6637
[Epoch 51] loss: 0.13860405369868975 acc: 0.6564
[Epoch 55] loss: 0.13638908328795735 acc: 0.6673
[Epoch 59] loss: 0.12612677086918803 acc: 0.6652
[Epoch 63] loss: 0.1192112088279651 acc: 0.6717
[Epoch 67] loss: 0.1120718181440833 acc: 0.6705
[Epoch 71] loss: 0.11993711236048528 acc: 0.6621
--> [test] acc: 0.653
--> [accuracy] finished 0.653
new state: tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.653
--> [reward] 0.653
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     5.0      |     2.0     | 0.653  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   2.,   4.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.498582869996805 acc: 0.6057
[Epoch 7] loss: 3.4589045060717543 acc: 0.6351
[Epoch 11] loss: 1.7215952884663097 acc: 0.6562
[Epoch 15] loss: 0.739412439877496 acc: 0.6366
[Epoch 19] loss: 0.43895879376898794 acc: 0.6416
[Epoch 23] loss: 0.32768923229992847 acc: 0.6413
[Epoch 27] loss: 0.286989822033364 acc: 0.6378
[Epoch 31] loss: 0.24414390409865494 acc: 0.6318
[Epoch 35] loss: 0.22672439133629319 acc: 0.6426
[Epoch 39] loss: 0.20052914780652736 acc: 0.6314
[Epoch 43] loss: 0.18972338203941008 acc: 0.6327
[Epoch 47] loss: 0.16831257889199708 acc: 0.6337
[Epoch 51] loss: 0.17511799546968562 acc: 0.6407
[Epoch 55] loss: 0.14032537843454437 acc: 0.6211
[Epoch 59] loss: 0.15086552863249847 acc: 0.6339
[Epoch 63] loss: 0.1467814976635301 acc: 0.6395
[Epoch 67] loss: 0.1308855949627126 acc: 0.6215
[Epoch 71] loss: 0.12562259577888796 acc: 0.6403
--> [test] acc: 0.642
--> [accuracy] finished 0.642
new state: tensor([736.,   2.,   4.,   6.,   2.], device='cuda:0')
new reward: 0.642
--> [reward] 0.642
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     6.0      |     2.0     | 0.642  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.209570452990129 acc: 0.609
[Epoch 7] loss: 3.052212193875057 acc: 0.6792
[Epoch 11] loss: 1.3663317074289407 acc: 0.6602
[Epoch 15] loss: 0.569726245916065 acc: 0.6668
[Epoch 19] loss: 0.35931888910825066 acc: 0.671
[Epoch 23] loss: 0.27488451259796653 acc: 0.6766
[Epoch 27] loss: 0.23720285344554487 acc: 0.6687
[Epoch 31] loss: 0.21238688773730452 acc: 0.6695
[Epoch 35] loss: 0.1897985713504007 acc: 0.6734
[Epoch 39] loss: 0.1720361082524042 acc: 0.6662
[Epoch 43] loss: 0.1617215357296874 acc: 0.6624
[Epoch 47] loss: 0.15442683711609878 acc: 0.6582
[Epoch 51] loss: 0.1408482103244118 acc: 0.6679
[Epoch 55] loss: 0.1348314223325123 acc: 0.6741
[Epoch 59] loss: 0.12602280156777415 acc: 0.6691
[Epoch 63] loss: 0.12683354574315192 acc: 0.6628
[Epoch 67] loss: 0.11367393211651262 acc: 0.6707
[Epoch 71] loss: 0.11211336768873016 acc: 0.6705
--> [test] acc: 0.6544
--> [accuracy] finished 0.6544
new state: tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6544
--> [reward] 0.6544
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     5.0      |     2.0     | 0.6544 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.214999463399658 acc: 0.5859
[Epoch 7] loss: 3.0593838921135954 acc: 0.6692
[Epoch 11] loss: 1.3641378109336204 acc: 0.6821
[Epoch 15] loss: 0.5564229187777128 acc: 0.6689
[Epoch 19] loss: 0.349163474005354 acc: 0.668
[Epoch 23] loss: 0.2797101935243134 acc: 0.6673
[Epoch 27] loss: 0.23105720881744266 acc: 0.6756
[Epoch 31] loss: 0.2101241190586706 acc: 0.6754
[Epoch 35] loss: 0.1864868999254006 acc: 0.6805
[Epoch 39] loss: 0.1777087557856041 acc: 0.6681
[Epoch 43] loss: 0.1569707291010205 acc: 0.6735
[Epoch 47] loss: 0.14921720631961302 acc: 0.6746
[Epoch 51] loss: 0.13646296838871882 acc: 0.6707
[Epoch 55] loss: 0.1312406458260725 acc: 0.6712
[Epoch 59] loss: 0.12668651260097352 acc: 0.6752
[Epoch 63] loss: 0.11805646027178715 acc: 0.6713
[Epoch 67] loss: 0.1159547131360911 acc: 0.6702
[Epoch 71] loss: 0.10120125846190574 acc: 0.6632
--> [test] acc: 0.6615
--> [accuracy] finished 0.6615
new state: tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6615
--> [reward] 0.6615
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     4.0     |     5.0      |     2.0     | 0.6615 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([768.,   2.,   5.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.057800574528287 acc: 0.6254
[Epoch 7] loss: 2.821873423845872 acc: 0.6877
[Epoch 11] loss: 1.1006512438111447 acc: 0.6879
[Epoch 15] loss: 0.46789600075606036 acc: 0.6883
[Epoch 19] loss: 0.29730487162309227 acc: 0.6797
[Epoch 23] loss: 0.24361927792920596 acc: 0.6842
[Epoch 27] loss: 0.21117803754756595 acc: 0.6748
[Epoch 31] loss: 0.2013069887281112 acc: 0.6836
[Epoch 35] loss: 0.16062533786124966 acc: 0.6828
[Epoch 39] loss: 0.1559826267402038 acc: 0.683
[Epoch 43] loss: 0.13630174247362195 acc: 0.6744
[Epoch 47] loss: 0.14217901615726064 acc: 0.6807
[Epoch 51] loss: 0.12554583114231258 acc: 0.6825
[Epoch 55] loss: 0.11516753497385822 acc: 0.676
[Epoch 59] loss: 0.11198423156167006 acc: 0.6844
[Epoch 63] loss: 0.10927889738327173 acc: 0.69
[Epoch 67] loss: 0.10877650001120832 acc: 0.6811
[Epoch 71] loss: 0.09588581882953129 acc: 0.6785
--> [test] acc: 0.6814
--> [accuracy] finished 0.6814
new state: tensor([768.,   2.,   5.,   5.,   2.], device='cuda:0')
new reward: 0.6814
--> [reward] 0.6814
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     5.0     |     5.0      |     2.0     | 0.6814 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   5.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.266552327081675 acc: 0.6215
[Epoch 7] loss: 3.0545945776545484 acc: 0.6572
[Epoch 11] loss: 1.358383351217603 acc: 0.6666
[Epoch 15] loss: 0.5431675038841146 acc: 0.6767
[Epoch 19] loss: 0.3495591759796033 acc: 0.678
[Epoch 23] loss: 0.27231547297419184 acc: 0.6796
[Epoch 27] loss: 0.22821339661412685 acc: 0.6704
[Epoch 31] loss: 0.20306962423855462 acc: 0.6663
[Epoch 35] loss: 0.1942717793547665 acc: 0.6694
[Epoch 39] loss: 0.17079080287855394 acc: 0.6718
[Epoch 43] loss: 0.15225345966027445 acc: 0.6637
[Epoch 47] loss: 0.1491894794751883 acc: 0.6686
[Epoch 51] loss: 0.14023466291063277 acc: 0.6618
[Epoch 55] loss: 0.133632761969343 acc: 0.6723
[Epoch 59] loss: 0.115679919130176 acc: 0.6639
[Epoch 63] loss: 0.13428519146464518 acc: 0.6609
[Epoch 67] loss: 0.10746646266849831 acc: 0.6663
[Epoch 71] loss: 0.10880718144657843 acc: 0.6664
--> [test] acc: 0.6693
--> [accuracy] finished 0.6693
new state: tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6693
--> [reward] 0.6693
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     4.0     |     5.0      |     2.0     | 0.6693 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0921, 0.0912, 0.0898, 0.0914, 0.0910, 0.0908, 0.0908, 0.0902,
         0.0906, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3967, -2.3976, -2.3990, -2.3974, -2.3978, -2.3980, -2.3980,
         -2.3986, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([768.,   2.,   4.,   6.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.490095845878582 acc: 0.5692
[Epoch 7] loss: 3.4769555594000363 acc: 0.6493
[Epoch 11] loss: 1.7204959389879881 acc: 0.6337
[Epoch 15] loss: 0.722271605173264 acc: 0.6415
[Epoch 19] loss: 0.43731975363438846 acc: 0.6471
[Epoch 23] loss: 0.3263996957617876 acc: 0.6328
[Epoch 27] loss: 0.2792610835684154 acc: 0.6352
[Epoch 31] loss: 0.25407816278641027 acc: 0.6347
[Epoch 35] loss: 0.22176837423563842 acc: 0.6282
[Epoch 39] loss: 0.19497324676131425 acc: 0.6335
[Epoch 43] loss: 0.18780891535579777 acc: 0.6338
[Epoch 47] loss: 0.17111998662957564 acc: 0.6421
[Epoch 51] loss: 0.15612781341747403 acc: 0.6371
[Epoch 55] loss: 0.16181537509560012 acc: 0.6345
[Epoch 59] loss: 0.15228014102811827 acc: 0.6351
[Epoch 63] loss: 0.1393492234051418 acc: 0.6369
[Epoch 67] loss: 0.1252718661242472 acc: 0.627
[Epoch 71] loss: 0.12872716467208264 acc: 0.6246
--> [test] acc: 0.6234
--> [accuracy] finished 0.6234
new state: tensor([768.,   2.,   4.,   6.,   2.], device='cuda:0')
new reward: 0.6234
--> [reward] 0.6234
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1942]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3884]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6232]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6614]], device='cuda:0')
------ ------
delta_t: tensor([[0.6232]], device='cuda:0')
rewards[i]: 0.6234
values[i+1]: tensor([[0.0384]], device='cuda:0')
values[i]: tensor([[0.0381]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6232]], device='cuda:0')
delta_t: tensor([[0.6232]], device='cuda:0')
------ ------
policy_loss: 1.4705042839050293
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6232]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0213]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6542]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2862]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3241]], device='cuda:0')
------ ------
delta_t: tensor([[0.6692]], device='cuda:0')
rewards[i]: 0.6693
values[i+1]: tensor([[0.0381]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0379]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2862]], device='cuda:0')
delta_t: tensor([[0.6692]], device='cuda:0')
------ ------
policy_loss: 4.5300068855285645
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2862]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.9306]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.8185]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9541]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9922]], device='cuda:0')
------ ------
delta_t: tensor([[0.6808]], device='cuda:0')
rewards[i]: 0.6814
values[i+1]: tensor([[0.0379]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0381]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9541]], device='cuda:0')
delta_t: tensor([[0.6808]], device='cuda:0')
------ ------
policy_loss: 9.191520690917969
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9541]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.2995]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.7379]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5958]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.6338]], device='cuda:0')
------ ------
delta_t: tensor([[0.6612]], device='cuda:0')
rewards[i]: 0.6615
values[i+1]: tensor([[0.0381]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0380]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5958]], device='cuda:0')
delta_t: tensor([[0.6612]], device='cuda:0')
------ ------
policy_loss: 15.38875961303711
log_probs[i]: tensor([[-2.3967]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5958]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[11.4963]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.3936]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.2239]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2619]], device='cuda:0')
------ ------
delta_t: tensor([[0.6541]], device='cuda:0')
rewards[i]: 0.6544
values[i+1]: tensor([[0.0380]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0380]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.2239]], device='cuda:0')
delta_t: tensor([[0.6541]], device='cuda:0')
------ ------
policy_loss: 23.095626831054688
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.2239]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[18.8439]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.6952]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.8334]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.8712]], device='cuda:0')
------ ------
delta_t: tensor([[0.6418]], device='cuda:0')
rewards[i]: 0.642
values[i+1]: tensor([[0.0380]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0378]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.8334]], device='cuda:0')
delta_t: tensor([[0.6418]], device='cuda:0')
------ ------
policy_loss: 32.26405334472656
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.8334]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[28.7358]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.7838]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.4479]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.4855]], device='cuda:0')
------ ------
delta_t: tensor([[0.6528]], device='cuda:0')
rewards[i]: 0.653
values[i+1]: tensor([[0.0378]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0376]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.4479]], device='cuda:0')
delta_t: tensor([[0.6528]], device='cuda:0')
------ ------
policy_loss: 42.9036750793457
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.4479]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[41.6556]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[25.8395]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.0833]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.1210]], device='cuda:0')
------ ------
delta_t: tensor([[0.6798]], device='cuda:0')
rewards[i]: 0.6803
values[i+1]: tensor([[0.0376]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0377]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.0833]], device='cuda:0')
delta_t: tensor([[0.6798]], device='cuda:0')
------ ------
policy_loss: 55.07258605957031
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.0833]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[57.8985]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.4857]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.6996]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.7376]], device='cuda:0')
------ ------
delta_t: tensor([[0.6672]], device='cuda:0')
rewards[i]: 0.6678
values[i+1]: tensor([[0.0377]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0379]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.6996]], device='cuda:0')
delta_t: tensor([[0.6672]], device='cuda:0')
------ ------
policy_loss: 68.71504974365234
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.6996]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[77.8056]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[39.8142]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3098]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.3478]], device='cuda:0')
------ ------
delta_t: tensor([[0.6672]], device='cuda:0')
rewards[i]: 0.6676
values[i+1]: tensor([[0.0379]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0379]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3098]], device='cuda:0')
delta_t: tensor([[0.6672]], device='cuda:0')
------ ------
policy_loss: 83.82611846923828
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3098]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[101.7699]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[47.9288]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9231]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.9611]], device='cuda:0')
------ ------
delta_t: tensor([[0.6763]], device='cuda:0')
rewards[i]: 0.6768
values[i+1]: tensor([[0.0379]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0381]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9231]], device='cuda:0')
delta_t: tensor([[0.6763]], device='cuda:0')
------ ------
policy_loss: 100.4007339477539
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9231]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[130.2419]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.9439]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.5461]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5839]], device='cuda:0')
------ ------
delta_t: tensor([[0.6923]], device='cuda:0')
rewards[i]: 0.6924
values[i+1]: tensor([[0.0381]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0378]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.5461]], device='cuda:0')
delta_t: tensor([[0.6923]], device='cuda:0')
------ ------
policy_loss: 118.47293090820312
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.5461]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[163.6901]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[66.8964]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.1790]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.2163]], device='cuda:0')
------ ------
delta_t: tensor([[0.7084]], device='cuda:0')
rewards[i]: 0.7082
values[i+1]: tensor([[0.0378]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0372]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.1790]], device='cuda:0')
delta_t: tensor([[0.7084]], device='cuda:0')
------ ------
policy_loss: 138.07008361816406
log_probs[i]: tensor([[-2.3990]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.1790]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[202.2407]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[77.1012]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.7807]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.8174]], device='cuda:0')
------ ------
delta_t: tensor([[0.6835]], device='cuda:0')
rewards[i]: 0.6833
values[i+1]: tensor([[0.0372]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0367]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.7807]], device='cuda:0')
delta_t: tensor([[0.6835]], device='cuda:0')
------ ------
policy_loss: 159.1107177734375
log_probs[i]: tensor([[-2.3990]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.7807]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[245.6305]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[86.7796]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.3156]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.3516]], device='cuda:0')
------ ------
delta_t: tensor([[0.6226]], device='cuda:0')
rewards[i]: 0.6224
values[i+1]: tensor([[0.0367]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0361]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.3156]], device='cuda:0')
delta_t: tensor([[0.6226]], device='cuda:0')
------ ------
policy_loss: 181.42153930664062
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.3156]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 181.42153930664062
value_loss: 245.63050842285156
loss: 304.2367858886719



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-2.3843e-03, -5.4177e-06, -1.2261e-05, -1.5898e-05, -9.7626e-06],
        [ 5.6639e-02,  1.2550e-04,  2.8671e-04,  3.7581e-04,  2.2511e-04],
        [-1.0155e-03, -2.4036e-06, -5.3889e-06, -6.8319e-06, -4.3059e-06],
        [ 2.9901e-01,  6.7418e-04,  1.5366e-03,  1.9918e-03,  1.2093e-03],
        [ 1.5097e+00,  3.4210e-03,  7.7296e-03,  1.0045e-02,  6.1292e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 1.8634e-05,  2.1789e-05, -3.8009e-06, -2.2147e-05,  1.6100e-05],
        [-4.4291e-04, -5.1837e-04,  8.9449e-05,  5.2731e-04, -3.8231e-04],
        [ 7.9247e-06,  9.2472e-06, -1.6501e-06, -9.3865e-06,  6.8595e-06],
        [-2.3364e-03, -2.7320e-03,  4.7625e-04,  2.7775e-03, -2.0185e-03],
        [-1.1799e-02, -1.3798e-02,  2.4006e-03,  1.4028e-02, -1.0191e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.4277, -0.4208,  0.4273, -0.3826,  0.2437],
        [-0.1046,  0.1029, -0.1045,  0.0936, -0.0598],
        [-0.2426,  0.2387, -0.2424,  0.2171, -0.1381],
        [-0.0022,  0.0021, -0.0022,  0.0019, -0.0013],
        [-0.0783,  0.0770, -0.0782,  0.0700, -0.0446]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0361, -0.0417,  0.0080,  0.0422, -0.0314],
        [-0.0227, -0.0264,  0.0049,  0.0266, -0.0197],
        [ 0.0489,  0.0571, -0.0102, -0.0576,  0.0423],
        [ 0.0518,  0.0604, -0.0107, -0.0609,  0.0454],
        [-0.0060, -0.0070,  0.0011,  0.0070, -0.0051],
        [ 0.0044,  0.0048, -0.0012, -0.0050,  0.0037],
        [-0.0191, -0.0222,  0.0041,  0.0223, -0.0165],
        [-0.0126, -0.0147,  0.0026,  0.0147, -0.0108],
        [ 0.0235,  0.0269, -0.0058, -0.0271,  0.0202],
        [-0.0358, -0.0414,  0.0080,  0.0418, -0.0311],
        [ 0.0037,  0.0042, -0.0009, -0.0040,  0.0029]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.1819,  2.5197, -0.4850, -2.5466,  1.8942]], device='cuda:0')
--> [loss] 304.2367858886719

---------------------------------- [[#13 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     4.0     |     6.0      |     2.0     | 0.6234 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0898, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3974, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   4.,   6.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.252569736422175 acc: 0.6408
[Epoch 7] loss: 3.103587823450718 acc: 0.6784
[Epoch 11] loss: 1.359064250917691 acc: 0.6678
[Epoch 15] loss: 0.5504794784957339 acc: 0.6673
[Epoch 19] loss: 0.3570577325370839 acc: 0.6625
[Epoch 23] loss: 0.2753574071866472 acc: 0.6666
[Epoch 27] loss: 0.22615706337296673 acc: 0.6705
[Epoch 31] loss: 0.20677614640777983 acc: 0.6517
[Epoch 35] loss: 0.18790912347109726 acc: 0.658
[Epoch 39] loss: 0.17923693805504257 acc: 0.6679
[Epoch 43] loss: 0.1584765638255745 acc: 0.6661
[Epoch 47] loss: 0.14781623855566658 acc: 0.6592
[Epoch 51] loss: 0.13996975948882845 acc: 0.6645
[Epoch 55] loss: 0.12599991164896684 acc: 0.6597
[Epoch 59] loss: 0.12260153575602661 acc: 0.6682
[Epoch 63] loss: 0.11980149006057779 acc: 0.6613
[Epoch 67] loss: 0.1149577215704543 acc: 0.6644
[Epoch 71] loss: 0.11133371264664242 acc: 0.6514
--> [test] acc: 0.6631
--> [accuracy] finished 0.6631
new state: tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6631
--> [reward] 0.6631
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     2.0      |     4.0     |     5.0      |     2.0     | 0.6631 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0898, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3974, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.245929257491666 acc: 0.6218
[Epoch 7] loss: 3.0902753119426007 acc: 0.6723
[Epoch 11] loss: 1.3595241981051158 acc: 0.6702
[Epoch 15] loss: 0.5647592568231742 acc: 0.6581
[Epoch 19] loss: 0.36114866586635486 acc: 0.6646
[Epoch 23] loss: 0.27621042765819886 acc: 0.6555
[Epoch 27] loss: 0.23744892149620578 acc: 0.6603
[Epoch 31] loss: 0.2123302996130017 acc: 0.6618
[Epoch 35] loss: 0.1906688581470905 acc: 0.663
[Epoch 39] loss: 0.16636448889426517 acc: 0.662
[Epoch 43] loss: 0.1620105984692207 acc: 0.6622
[Epoch 47] loss: 0.15990577896466227 acc: 0.661
[Epoch 51] loss: 0.13933006202792533 acc: 0.6669
[Epoch 55] loss: 0.12886585325688657 acc: 0.6653
[Epoch 59] loss: 0.12664902375539397 acc: 0.6705
[Epoch 63] loss: 0.11922891743128639 acc: 0.6615
[Epoch 67] loss: 0.11783020167618685 acc: 0.6712
[Epoch 71] loss: 0.11007501995634607 acc: 0.6687
--> [test] acc: 0.6681
--> [accuracy] finished 0.6681
new state: tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6681
--> [reward] 0.6681
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     4.0     |     5.0      |     2.0     | 0.6681 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3974, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.826982830034193 acc: 0.6544
[Epoch 7] loss: 2.460989333593937 acc: 0.7064
[Epoch 11] loss: 0.8620964393372197 acc: 0.7128
[Epoch 15] loss: 0.3657626314183978 acc: 0.7209
[Epoch 19] loss: 0.2735166063870463 acc: 0.708
[Epoch 23] loss: 0.21517468341257032 acc: 0.7004
[Epoch 27] loss: 0.18893765618362465 acc: 0.7049
[Epoch 31] loss: 0.16672712964865635 acc: 0.7059
[Epoch 35] loss: 0.14574952066287666 acc: 0.705
[Epoch 39] loss: 0.14009358089112336 acc: 0.7049
[Epoch 43] loss: 0.13110786397724658 acc: 0.7077
[Epoch 47] loss: 0.11372207009526508 acc: 0.6992
[Epoch 51] loss: 0.11103808618081576 acc: 0.7102
[Epoch 55] loss: 0.10427991003560289 acc: 0.6968
[Epoch 59] loss: 0.10006961158936477 acc: 0.7065
[Epoch 63] loss: 0.0983383409277586 acc: 0.7064
[Epoch 67] loss: 0.08621863556294547 acc: 0.7051
[Epoch 71] loss: 0.09261449980620971 acc: 0.7042
--> [test] acc: 0.6961
--> [accuracy] finished 0.6961
new state: tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6961
--> [reward] 0.6961
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.6961 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.860453013416446 acc: 0.6643
[Epoch 7] loss: 2.4635897274974665 acc: 0.7195
[Epoch 11] loss: 0.8715557484122951 acc: 0.724
[Epoch 15] loss: 0.3835921965329848 acc: 0.7181
[Epoch 19] loss: 0.2706335624084448 acc: 0.7083
[Epoch 23] loss: 0.22140233060511785 acc: 0.7137
[Epoch 27] loss: 0.18148278698201298 acc: 0.6981
[Epoch 31] loss: 0.16989097480788407 acc: 0.7119
[Epoch 35] loss: 0.1550116845170784 acc: 0.7112
[Epoch 39] loss: 0.13479003255598515 acc: 0.7082
[Epoch 43] loss: 0.12778011003218573 acc: 0.7187
[Epoch 47] loss: 0.11882736390489904 acc: 0.7066
[Epoch 51] loss: 0.1153201186979461 acc: 0.7134
[Epoch 55] loss: 0.11149538223110043 acc: 0.7057
[Epoch 59] loss: 0.09977865318113895 acc: 0.7088
[Epoch 63] loss: 0.09354442284684247 acc: 0.714
[Epoch 67] loss: 0.09326993623900863 acc: 0.7073
[Epoch 71] loss: 0.09169269378339905 acc: 0.7088
--> [test] acc: 0.7078
--> [accuracy] finished 0.7078
new state: tensor([768.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.7078
--> [reward] 0.7078
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.7078 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.865044259049398 acc: 0.6652
[Epoch 7] loss: 2.461407289137621 acc: 0.7073
[Epoch 11] loss: 0.8760926250053946 acc: 0.7133
[Epoch 15] loss: 0.3863919312563127 acc: 0.7159
[Epoch 19] loss: 0.26402810983397923 acc: 0.7105
[Epoch 23] loss: 0.21307710365956778 acc: 0.7005
[Epoch 27] loss: 0.19372457191776818 acc: 0.7094
[Epoch 31] loss: 0.1629040971341188 acc: 0.7044
[Epoch 35] loss: 0.1520291123220035 acc: 0.7082
[Epoch 39] loss: 0.14110941155110973 acc: 0.7071
[Epoch 43] loss: 0.1266787790448007 acc: 0.6928
[Epoch 47] loss: 0.11761689331600457 acc: 0.7005
[Epoch 51] loss: 0.10856094664332512 acc: 0.6978
[Epoch 55] loss: 0.10893480341686197 acc: 0.7041
[Epoch 59] loss: 0.10013254479293132 acc: 0.7156
[Epoch 63] loss: 0.097373200195801 acc: 0.7041
[Epoch 67] loss: 0.08560938967625274 acc: 0.7145
[Epoch 71] loss: 0.08619699082658876 acc: 0.702
--> [test] acc: 0.6993
--> [accuracy] finished 0.6993
new state: tensor([768.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6993
--> [reward] 0.6993
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.6993 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([768.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([768.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.757663965834986 acc: 0.6333
[Epoch 7] loss: 2.3383299787254894 acc: 0.7224
[Epoch 11] loss: 0.7644364260270468 acc: 0.7205
[Epoch 15] loss: 0.34594456356523745 acc: 0.7183
[Epoch 19] loss: 0.242473336610743 acc: 0.7259
[Epoch 23] loss: 0.2054178388622563 acc: 0.7245
[Epoch 27] loss: 0.16866036837020187 acc: 0.7226
[Epoch 31] loss: 0.15357307399935125 acc: 0.7153
[Epoch 35] loss: 0.13993714165771404 acc: 0.7104
[Epoch 39] loss: 0.1264359984353966 acc: 0.6959
[Epoch 43] loss: 0.11980879668246412 acc: 0.7147
[Epoch 47] loss: 0.10983685175280856 acc: 0.7009
[Epoch 51] loss: 0.09849843995459855 acc: 0.7116
[Epoch 55] loss: 0.09323351177841882 acc: 0.7116
[Epoch 59] loss: 0.0970609867480843 acc: 0.7108
[Epoch 63] loss: 0.08634080518515842 acc: 0.7092
[Epoch 67] loss: 0.09268724017709856 acc: 0.711
[Epoch 71] loss: 0.07815479349864222 acc: 0.7087
--> [test] acc: 0.7134
--> [accuracy] finished 0.7134
new state: tensor([768.,   3.,   4.,   4.,   2.], device='cuda:0')
new reward: 0.7134
--> [reward] 0.7134
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     3.0      |     4.0     |     4.0      |     2.0     | 0.7134 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.724653504236275 acc: 0.6786
[Epoch 7] loss: 2.3048198139652265 acc: 0.7197
[Epoch 11] loss: 0.7745437338814863 acc: 0.7291
[Epoch 15] loss: 0.3555636575393131 acc: 0.7253
[Epoch 19] loss: 0.2447362080730898 acc: 0.7226
[Epoch 23] loss: 0.2051443004542414 acc: 0.7258
[Epoch 27] loss: 0.17149402421501364 acc: 0.7161
[Epoch 31] loss: 0.16212232648621283 acc: 0.713
[Epoch 35] loss: 0.135616764104675 acc: 0.7175
[Epoch 39] loss: 0.12892451253302795 acc: 0.7098
[Epoch 43] loss: 0.12040128266972387 acc: 0.7209
[Epoch 47] loss: 0.10204863009113602 acc: 0.7103
[Epoch 51] loss: 0.10965114507025948 acc: 0.7149
[Epoch 55] loss: 0.08854529603391581 acc: 0.7175
[Epoch 59] loss: 0.10531976498315668 acc: 0.7204
[Epoch 63] loss: 0.0826979245135413 acc: 0.72
[Epoch 67] loss: 0.08834444044727732 acc: 0.7188
[Epoch 71] loss: 0.07969505951603335 acc: 0.7085
--> [test] acc: 0.7213
--> [accuracy] finished 0.7213
new state: tensor([768.,   3.,   4.,   4.,   2.], device='cuda:0')
new reward: 0.7213
--> [reward] 0.7213
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     3.0      |     4.0     |     4.0      |     2.0     | 0.7213 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0898, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.713661278147832 acc: 0.6654
[Epoch 7] loss: 2.272129059561988 acc: 0.7275
[Epoch 11] loss: 0.7582701609453277 acc: 0.7267
[Epoch 15] loss: 0.33437022868820165 acc: 0.719
[Epoch 19] loss: 0.24294952826950786 acc: 0.7212
[Epoch 23] loss: 0.19994381892606808 acc: 0.723
[Epoch 27] loss: 0.17157372235632537 acc: 0.716
[Epoch 31] loss: 0.15586110086792418 acc: 0.7123
[Epoch 35] loss: 0.1354592520090968 acc: 0.7224
[Epoch 39] loss: 0.13047457666581738 acc: 0.7057
[Epoch 43] loss: 0.12418083887969446 acc: 0.7162
[Epoch 47] loss: 0.10771559261957474 acc: 0.7192
[Epoch 51] loss: 0.10708110992942015 acc: 0.7131
[Epoch 55] loss: 0.09754126936988071 acc: 0.722
[Epoch 59] loss: 0.09081345457521736 acc: 0.7118
[Epoch 63] loss: 0.08884781096523622 acc: 0.7235
[Epoch 67] loss: 0.09043986397519078 acc: 0.7165
[Epoch 71] loss: 0.07942940090494731 acc: 0.7138
--> [test] acc: 0.7091
--> [accuracy] finished 0.7091
new state: tensor([736.,   3.,   4.,   4.,   2.], device='cuda:0')
new reward: 0.7091
--> [reward] 0.7091
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     4.0      |     2.0     | 0.7091 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3990, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   3.,   3.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.951569699875229 acc: 0.6514
[Epoch 7] loss: 2.6934341446822865 acc: 0.6917
[Epoch 11] loss: 1.0520457247715167 acc: 0.6945
[Epoch 15] loss: 0.4411979275553123 acc: 0.6909
[Epoch 19] loss: 0.29875845233421494 acc: 0.6907
[Epoch 23] loss: 0.23895328064851673 acc: 0.6986
[Epoch 27] loss: 0.2028998607802955 acc: 0.6936
[Epoch 31] loss: 0.18850723556492982 acc: 0.6953
[Epoch 35] loss: 0.16511533704712567 acc: 0.6898
[Epoch 39] loss: 0.15068549974857237 acc: 0.6857
[Epoch 43] loss: 0.1375034997666069 acc: 0.6891
[Epoch 47] loss: 0.13283298631339235 acc: 0.6974
[Epoch 51] loss: 0.12449335767542158 acc: 0.6897
[Epoch 55] loss: 0.11781220788212346 acc: 0.6831
[Epoch 59] loss: 0.10204920040257752 acc: 0.6943
[Epoch 63] loss: 0.11280395445959343 acc: 0.6923
[Epoch 67] loss: 0.10070820721735715 acc: 0.6954
[Epoch 71] loss: 0.09821034197354941 acc: 0.6875
--> [test] acc: 0.6978
--> [accuracy] finished 0.6978
new state: tensor([736.,   3.,   3.,   4.,   2.], device='cuda:0')
new reward: 0.6978
--> [reward] 0.6978
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     3.0     |     4.0      |     2.0     | 0.6978 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   3.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   3.,   3.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.907534441069874 acc: 0.6615
[Epoch 7] loss: 2.5518581063469963 acc: 0.7175
[Epoch 11] loss: 0.9797053746736187 acc: 0.7125
[Epoch 15] loss: 0.4118178900559921 acc: 0.6908
[Epoch 19] loss: 0.27762114394055987 acc: 0.7193
[Epoch 23] loss: 0.23076215541928702 acc: 0.7017
[Epoch 27] loss: 0.19722221803891918 acc: 0.7178
[Epoch 31] loss: 0.1696537174903752 acc: 0.7101
[Epoch 35] loss: 0.15641805780646595 acc: 0.7089
[Epoch 39] loss: 0.14333774466925037 acc: 0.7133
[Epoch 43] loss: 0.1254682496876058 acc: 0.7086
[Epoch 47] loss: 0.12152686909548557 acc: 0.7169
[Epoch 51] loss: 0.12408815687546111 acc: 0.7162
[Epoch 55] loss: 0.10532783260723799 acc: 0.704
[Epoch 59] loss: 0.10745597846271313 acc: 0.7184
[Epoch 63] loss: 0.0912460961421985 acc: 0.7176
[Epoch 67] loss: 0.10036594195820181 acc: 0.708
[Epoch 71] loss: 0.09230462348331576 acc: 0.7003
--> [test] acc: 0.7128
--> [accuracy] finished 0.7128
new state: tensor([736.,   3.,   3.,   4.,   3.], device='cuda:0')
new reward: 0.7128
--> [reward] 0.7128
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     3.0     |     4.0      |     3.0     | 0.7128 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   3.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   3.,   3.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.928765960056763 acc: 0.6614
[Epoch 7] loss: 2.570913997033368 acc: 0.7033
[Epoch 11] loss: 0.9923466561681322 acc: 0.7094
[Epoch 15] loss: 0.4288694170467994 acc: 0.7122
[Epoch 19] loss: 0.2867732282747012 acc: 0.6956
[Epoch 23] loss: 0.23066237492396321 acc: 0.7046
[Epoch 27] loss: 0.2040229232057624 acc: 0.7027
[Epoch 31] loss: 0.1847728573386093 acc: 0.714
[Epoch 35] loss: 0.15556885890276803 acc: 0.7067
[Epoch 39] loss: 0.14248551834789117 acc: 0.7161
[Epoch 43] loss: 0.1318219383378201 acc: 0.7019
[Epoch 47] loss: 0.12761590402821066 acc: 0.701
[Epoch 51] loss: 0.11675956249183468 acc: 0.695
[Epoch 55] loss: 0.11999002541236752 acc: 0.6954
[Epoch 59] loss: 0.10551484762256265 acc: 0.6998
[Epoch 63] loss: 0.09744057718299977 acc: 0.7072
[Epoch 67] loss: 0.09798791054683878 acc: 0.696
[Epoch 71] loss: 0.09743249703563697 acc: 0.7067
--> [test] acc: 0.7107
--> [accuracy] finished 0.7107
new state: tensor([704.,   3.,   3.,   4.,   3.], device='cuda:0')
new reward: 0.7107
--> [reward] 0.7107
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     4.0      |     3.0     | 0.7107 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([704.,   3.,   3.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.956276701996698 acc: 0.6458
[Epoch 7] loss: 2.6527067778055624 acc: 0.7101
[Epoch 11] loss: 1.0333417972926136 acc: 0.6937
[Epoch 15] loss: 0.4522147950051767 acc: 0.6895
[Epoch 19] loss: 0.30734211437242187 acc: 0.7066
[Epoch 23] loss: 0.24001227772277792 acc: 0.6933
[Epoch 27] loss: 0.2156624790552594 acc: 0.7004
[Epoch 31] loss: 0.1863088258053831 acc: 0.6947
[Epoch 35] loss: 0.15717407920257287 acc: 0.6889
[Epoch 39] loss: 0.16241299739831586 acc: 0.69
[Epoch 43] loss: 0.1412044567439486 acc: 0.6962
[Epoch 47] loss: 0.12561430658875963 acc: 0.6901
[Epoch 51] loss: 0.13728723959470895 acc: 0.6888
[Epoch 55] loss: 0.10835300404769 acc: 0.6941
[Epoch 59] loss: 0.11085425004067705 acc: 0.6934
[Epoch 63] loss: 0.11058614128257346 acc: 0.689
[Epoch 67] loss: 0.09282201544388824 acc: 0.6905
[Epoch 71] loss: 0.09429239187165714 acc: 0.69
--> [test] acc: 0.6936
--> [accuracy] finished 0.6936
new state: tensor([704.,   3.,   3.,   4.,   2.], device='cuda:0')
new reward: 0.6936
--> [reward] 0.6936
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     4.0      |     2.0     | 0.6936 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([704.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.772281219892185 acc: 0.6762
[Epoch 7] loss: 2.332573220781658 acc: 0.7333
[Epoch 11] loss: 0.7735131393374918 acc: 0.7213
[Epoch 15] loss: 0.35218691866120794 acc: 0.7052
[Epoch 19] loss: 0.2483215071094196 acc: 0.7233
[Epoch 23] loss: 0.20109850747982408 acc: 0.7151
[Epoch 27] loss: 0.17910155284520993 acc: 0.6987
[Epoch 31] loss: 0.15700377796864723 acc: 0.723
[Epoch 35] loss: 0.14752117723174146 acc: 0.7191
[Epoch 39] loss: 0.12739900808514612 acc: 0.7142
[Epoch 43] loss: 0.1215064428750273 acc: 0.7156
[Epoch 47] loss: 0.10869601544450082 acc: 0.7159
[Epoch 51] loss: 0.10621395781326591 acc: 0.7097
[Epoch 55] loss: 0.10159734808280112 acc: 0.7138
[Epoch 59] loss: 0.09002169086660385 acc: 0.7156
[Epoch 63] loss: 0.08826847823992934 acc: 0.7092
[Epoch 67] loss: 0.08959953316167006 acc: 0.712
[Epoch 71] loss: 0.08648974113964268 acc: 0.7222
--> [test] acc: 0.7144
--> [accuracy] finished 0.7144
new state: tensor([704.,   3.,   4.,   4.,   2.], device='cuda:0')
new reward: 0.7144
--> [reward] 0.7144
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     4.0     |     4.0      |     2.0     | 0.7144 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   4.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   3.,   4.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.701178699503164 acc: 0.6877
[Epoch 7] loss: 2.2500794564214206 acc: 0.7245
[Epoch 11] loss: 0.7491865500836344 acc: 0.7336
[Epoch 15] loss: 0.3389911144362081 acc: 0.7271
[Epoch 19] loss: 0.25098202818566384 acc: 0.7294
[Epoch 23] loss: 0.1977557925140614 acc: 0.7277
[Epoch 27] loss: 0.16945997533409873 acc: 0.7206
[Epoch 31] loss: 0.15368236695139972 acc: 0.7197
[Epoch 35] loss: 0.1400966979497734 acc: 0.7281
[Epoch 39] loss: 0.12676625155970034 acc: 0.7108
[Epoch 43] loss: 0.11595693266506324 acc: 0.7252
[Epoch 47] loss: 0.10458368006462465 acc: 0.7173
[Epoch 51] loss: 0.10729309281839243 acc: 0.7241
[Epoch 55] loss: 0.09970098189935755 acc: 0.7211
[Epoch 59] loss: 0.09537811323409648 acc: 0.7208
[Epoch 63] loss: 0.09028595344870902 acc: 0.7102
[Epoch 67] loss: 0.08018576946643079 acc: 0.7163
[Epoch 71] loss: 0.08393389884623058 acc: 0.7046
--> [test] acc: 0.7202
--> [accuracy] finished 0.7202
new state: tensor([704.,   3.,   4.,   4.,   3.], device='cuda:0')
new reward: 0.7202
--> [reward] 0.7202
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     4.0     |     4.0      |     3.0     | 0.7202 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0905, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3983, -2.3981]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   4.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   3.,   4.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.707796088874797 acc: 0.6658
[Epoch 7] loss: 2.265125249276686 acc: 0.7304
[Epoch 11] loss: 0.7554095131785744 acc: 0.7282
[Epoch 15] loss: 0.3434715738991642 acc: 0.7295
[Epoch 19] loss: 0.24956517089682315 acc: 0.7321
[Epoch 23] loss: 0.20194271885935228 acc: 0.7198
[Epoch 27] loss: 0.17346212544414163 acc: 0.7153
[Epoch 31] loss: 0.1518651530309759 acc: 0.7199
[Epoch 35] loss: 0.13444835854613263 acc: 0.7117
[Epoch 39] loss: 0.12260378520567175 acc: 0.7257
[Epoch 43] loss: 0.11856731850549083 acc: 0.7291
[Epoch 47] loss: 0.1096995754061443 acc: 0.7254
[Epoch 51] loss: 0.10240980791513954 acc: 0.7056
[Epoch 55] loss: 0.0957961935488045 acc: 0.7251
[Epoch 59] loss: 0.09647785778463487 acc: 0.7182
[Epoch 63] loss: 0.08890716160726174 acc: 0.7165
[Epoch 67] loss: 0.08266969988255969 acc: 0.7125
[Epoch 71] loss: 0.08154912324803111 acc: 0.7103
--> [test] acc: 0.7101
--> [accuracy] finished 0.7101
new state: tensor([736.,   3.,   4.,   4.,   3.], device='cuda:0')
new reward: 0.7101
--> [reward] 0.7101
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2516]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5033]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7094]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7527]], device='cuda:0')
------ ------
delta_t: tensor([[0.7094]], device='cuda:0')
rewards[i]: 0.7101
values[i+1]: tensor([[0.0430]], device='cuda:0')
values[i]: tensor([[0.0433]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7094]], device='cuda:0')
delta_t: tensor([[0.7094]], device='cuda:0')
------ ------
policy_loss: 1.6762336492538452
log_probs[i]: tensor([[-2.3966]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7094]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.2623]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.0213]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4217]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4654]], device='cuda:0')
------ ------
delta_t: tensor([[0.7194]], device='cuda:0')
rewards[i]: 0.7202
values[i+1]: tensor([[0.0433]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0437]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4217]], device='cuda:0')
delta_t: tensor([[0.7194]], device='cuda:0')
------ ------
policy_loss: 5.062036991119385
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4217]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.5119]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.4991]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1211]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1651]], device='cuda:0')
------ ------
delta_t: tensor([[0.7136]], device='cuda:0')
rewards[i]: 0.7144
values[i+1]: tensor([[0.0437]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0440]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1211]], device='cuda:0')
delta_t: tensor([[0.7136]], device='cuda:0')
------ ------
policy_loss: 10.123987197875977
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1211]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.4124]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.8012]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.7931]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8371]], device='cuda:0')
------ ------
delta_t: tensor([[0.6932]], device='cuda:0')
rewards[i]: 0.6936
values[i+1]: tensor([[0.0440]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0440]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.7931]], device='cuda:0')
delta_t: tensor([[0.6932]], device='cuda:0')
------ ------
policy_loss: 16.79960823059082
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.7931]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[13.4513]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.0776]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.4753]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.5194]], device='cuda:0')
------ ------
delta_t: tensor([[0.7102]], device='cuda:0')
rewards[i]: 0.7107
values[i+1]: tensor([[0.0440]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0441]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.4753]], device='cuda:0')
delta_t: tensor([[0.7102]], device='cuda:0')
------ ------
policy_loss: 25.107601165771484
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.4753]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[22.0746]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.2468]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1529]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.1970]], device='cuda:0')
------ ------
delta_t: tensor([[0.7124]], device='cuda:0')
rewards[i]: 0.7128
values[i+1]: tensor([[0.0441]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0441]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1529]], device='cuda:0')
delta_t: tensor([[0.7124]], device='cuda:0')
------ ------
policy_loss: 35.04368591308594
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1529]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[33.6382]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.1271]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8091]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8528]], device='cuda:0')
------ ------
delta_t: tensor([[0.6977]], device='cuda:0')
rewards[i]: 0.6978
values[i+1]: tensor([[0.0441]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0438]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8091]], device='cuda:0')
delta_t: tensor([[0.6977]], device='cuda:0')
------ ------
policy_loss: 46.5491943359375
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8091]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[48.5967]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.9171]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4696]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.5134]], device='cuda:0')
------ ------
delta_t: tensor([[0.7087]], device='cuda:0')
rewards[i]: 0.7091
values[i+1]: tensor([[0.0438]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0438]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4696]], device='cuda:0')
delta_t: tensor([[0.7087]], device='cuda:0')
------ ------
policy_loss: 59.638633728027344
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4696]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[67.4215]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.6496]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1359]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1796]], device='cuda:0')
------ ------
delta_t: tensor([[0.7210]], device='cuda:0')
rewards[i]: 0.7213
values[i+1]: tensor([[0.0438]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0437]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1359]], device='cuda:0')
delta_t: tensor([[0.7210]], device='cuda:0')
------ ------
policy_loss: 74.329345703125
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1359]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[90.4575]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[46.0721]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7876]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8312]], device='cuda:0')
------ ------
delta_t: tensor([[0.7131]], device='cuda:0')
rewards[i]: 0.7134
values[i+1]: tensor([[0.0437]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0435]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7876]], device='cuda:0')
delta_t: tensor([[0.7131]], device='cuda:0')
------ ------
policy_loss: 90.58206939697266
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7876]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[117.9759]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[55.0367]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4187]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.4622]], device='cuda:0')
------ ------
delta_t: tensor([[0.6989]], device='cuda:0')
rewards[i]: 0.6993
values[i+1]: tensor([[0.0435]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0435]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4187]], device='cuda:0')
delta_t: tensor([[0.6989]], device='cuda:0')
------ ------
policy_loss: 108.34758758544922
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4187]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[150.3941]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[64.8366]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0521]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.0954]], device='cuda:0')
------ ------
delta_t: tensor([[0.7076]], device='cuda:0')
rewards[i]: 0.7078
values[i+1]: tensor([[0.0435]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0521]], device='cuda:0')
delta_t: tensor([[0.7076]], device='cuda:0')
------ ------
policy_loss: 127.62146759033203
log_probs[i]: tensor([[-2.3966]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0521]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[187.9601]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[75.1320]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6679]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7105]], device='cuda:0')
------ ------
delta_t: tensor([[0.6963]], device='cuda:0')
rewards[i]: 0.6961
values[i+1]: tensor([[0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0426]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6679]], device='cuda:0')
delta_t: tensor([[0.6963]], device='cuda:0')
------ ------
policy_loss: 148.3912811279297
log_probs[i]: tensor([[-2.3990]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6679]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[230.7358]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.5514]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2494]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2915]], device='cuda:0')
------ ------
delta_t: tensor([[0.6682]], device='cuda:0')
rewards[i]: 0.6681
values[i+1]: tensor([[0.0426]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0421]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2494]], device='cuda:0')
delta_t: tensor([[0.6682]], device='cuda:0')
------ ------
policy_loss: 170.54263305664062
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2494]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[278.9577]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[96.4438]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8206]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.8617]], device='cuda:0')
------ ------
delta_t: tensor([[0.6637]], device='cuda:0')
rewards[i]: 0.6631
values[i+1]: tensor([[0.0421]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0411]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8206]], device='cuda:0')
delta_t: tensor([[0.6637]], device='cuda:0')
------ ------
policy_loss: 194.0683135986328
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8206]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 194.0683135986328
value_loss: 278.95770263671875
loss: 333.54718017578125



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-2.5682e-03, -8.0664e-06, -1.3795e-05, -1.6872e-05, -7.0519e-06],
        [ 6.9084e-02,  2.1310e-04,  3.7299e-04,  4.5605e-04,  1.8809e-04],
        [-1.0695e-03, -3.4581e-06, -5.6855e-06, -6.9579e-06, -2.9384e-06],
        [ 3.7437e-01,  1.1647e-03,  2.0149e-03,  2.4625e-03,  1.0169e-03],
        [ 1.9713e+00,  6.1148e-03,  1.0623e-02,  1.2968e-02,  5.3522e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.0571e-05,  2.2926e-05, -4.7473e-06, -2.3348e-05,  1.7627e-05],
        [-5.5078e-04, -6.1548e-04,  1.2651e-04,  6.2668e-04, -4.7245e-04],
        [ 8.6218e-06,  9.5598e-06, -2.0006e-06, -9.7405e-06,  7.3699e-06],
        [-2.9890e-03, -3.3347e-03,  6.8733e-04,  3.3960e-03, -2.5617e-03],
        [-1.5716e-02, -1.7545e-02,  3.6125e-03,  1.7867e-02, -1.3472e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.5615, -0.5534,  0.5611, -0.5069,  0.3281],
        [-0.1496,  0.1475, -0.1495,  0.1351, -0.0874],
        [-0.3107,  0.3062, -0.3104,  0.2803, -0.1816],
        [-0.0057,  0.0057, -0.0057,  0.0052, -0.0033],
        [-0.0955,  0.0941, -0.0954,  0.0862, -0.0558]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0620,  0.0675, -0.0143, -0.0689,  0.0524],
        [ 0.0073,  0.0075, -0.0021, -0.0075,  0.0062],
        [-0.0417, -0.0451,  0.0101,  0.0461, -0.0353],
        [ 0.0069,  0.0077, -0.0014, -0.0079,  0.0062],
        [-0.0144, -0.0158,  0.0032,  0.0161, -0.0123],
        [-0.0299, -0.0327,  0.0069,  0.0334, -0.0254],
        [ 0.0517,  0.0577, -0.0113, -0.0588,  0.0442],
        [ 0.0005,  0.0002, -0.0002, -0.0002,  0.0002],
        [-0.0258, -0.0283,  0.0058,  0.0288, -0.0219],
        [-0.0102, -0.0117,  0.0019,  0.0119, -0.0088],
        [-0.0064, -0.0070,  0.0015,  0.0071, -0.0056]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.5118,  2.7172, -0.6054, -2.7732,  2.1256]], device='cuda:0')
--> [loss] 333.54718017578125

---------------------------------- [[#14 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     4.0      |     3.0     | 0.7101 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   3.,   3.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.847546675168645 acc: 0.6653
[Epoch 7] loss: 2.5374404939696613 acc: 0.7268
[Epoch 11] loss: 0.9817128962720446 acc: 0.7245
[Epoch 15] loss: 0.4233600041493202 acc: 0.7114
[Epoch 19] loss: 0.2924247514289778 acc: 0.7118
[Epoch 23] loss: 0.23579691875787914 acc: 0.7211
[Epoch 27] loss: 0.18786590884837423 acc: 0.6908
[Epoch 31] loss: 0.17485333885521154 acc: 0.7189
[Epoch 35] loss: 0.16236963094738516 acc: 0.7034
[Epoch 39] loss: 0.1489277226907556 acc: 0.7087
[Epoch 43] loss: 0.13189253396928655 acc: 0.7112
[Epoch 47] loss: 0.12190234944786486 acc: 0.7134
[Epoch 51] loss: 0.11933474366069602 acc: 0.7162
[Epoch 55] loss: 0.12150984635228848 acc: 0.7047
[Epoch 59] loss: 0.10088181476443833 acc: 0.7104
[Epoch 63] loss: 0.10054175215343798 acc: 0.7139
[Epoch 67] loss: 0.10184053633638356 acc: 0.7043
[Epoch 71] loss: 0.08521647947063893 acc: 0.7179
--> [test] acc: 0.6985
--> [accuracy] finished 0.6985
new state: tensor([736.,   3.,   3.,   4.,   3.], device='cuda:0')
new reward: 0.6985
--> [reward] 0.6985
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     3.0     |     4.0      |     3.0     | 0.6985 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   3.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   3.,   3.,   4.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.960337737941986 acc: 0.6251
[Epoch 7] loss: 2.6791454726625283 acc: 0.7151
[Epoch 11] loss: 1.0500731117870954 acc: 0.7036
[Epoch 15] loss: 0.45506134570654855 acc: 0.6944
[Epoch 19] loss: 0.29945275835845325 acc: 0.6937
[Epoch 23] loss: 0.24019525343042505 acc: 0.7028
[Epoch 27] loss: 0.21325212448378048 acc: 0.6931
[Epoch 31] loss: 0.18271386216316002 acc: 0.6828
[Epoch 35] loss: 0.16397690107507626 acc: 0.6917
[Epoch 39] loss: 0.15468124361932659 acc: 0.695
[Epoch 43] loss: 0.14219580454718503 acc: 0.6943
[Epoch 47] loss: 0.13064291597941838 acc: 0.6918
[Epoch 51] loss: 0.13105317927709997 acc: 0.6924
[Epoch 55] loss: 0.11010899564341815 acc: 0.6926
[Epoch 59] loss: 0.1156400296136813 acc: 0.6924
[Epoch 63] loss: 0.09806706302899325 acc: 0.6905
[Epoch 67] loss: 0.10333676480323724 acc: 0.6975
[Epoch 71] loss: 0.10100838568959566 acc: 0.7005
--> [test] acc: 0.695
--> [accuracy] finished 0.695
new state: tensor([736.,   3.,   3.,   4.,   2.], device='cuda:0')
new reward: 0.695
--> [reward] 0.695
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     3.0     |     4.0      |     2.0     | 0.695  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   3.,   4.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   3.,   3.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.081692197133818 acc: 0.6354
[Epoch 7] loss: 2.863503596666829 acc: 0.691
[Epoch 11] loss: 1.2001050995839069 acc: 0.6828
[Epoch 15] loss: 0.5032934551377355 acc: 0.6932
[Epoch 19] loss: 0.3277515233839717 acc: 0.6888
[Epoch 23] loss: 0.25880263683140814 acc: 0.6843
[Epoch 27] loss: 0.2195297414904384 acc: 0.6927
[Epoch 31] loss: 0.19772584923326283 acc: 0.687
[Epoch 35] loss: 0.17148726011681206 acc: 0.6803
[Epoch 39] loss: 0.1625070039926054 acc: 0.6808
[Epoch 43] loss: 0.15333163551271647 acc: 0.6812
[Epoch 47] loss: 0.1356183993683704 acc: 0.6794
[Epoch 51] loss: 0.12804095058337503 acc: 0.6781
[Epoch 55] loss: 0.12804877696334935 acc: 0.6871
[Epoch 59] loss: 0.12313106286403773 acc: 0.6889
[Epoch 63] loss: 0.1131604093471137 acc: 0.6752
[Epoch 67] loss: 0.11390455103005327 acc: 0.6828
[Epoch 71] loss: 0.10565537729394643 acc: 0.6819
--> [test] acc: 0.681
--> [accuracy] finished 0.681
new state: tensor([736.,   3.,   3.,   5.,   2.], device='cuda:0')
new reward: 0.681
--> [reward] 0.681
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     3.0     |     5.0      |     2.0     | 0.681  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   3.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.825124058881989 acc: 0.6289
[Epoch 7] loss: 2.4562571627252243 acc: 0.7076
[Epoch 11] loss: 0.8538193548493602 acc: 0.7018
[Epoch 15] loss: 0.38478308743880607 acc: 0.7037
[Epoch 19] loss: 0.26651124093595824 acc: 0.7062
[Epoch 23] loss: 0.21310225998401602 acc: 0.6909
[Epoch 27] loss: 0.18265345972745925 acc: 0.7035
[Epoch 31] loss: 0.17293842161869835 acc: 0.7019
[Epoch 35] loss: 0.1516510440022363 acc: 0.7085
[Epoch 39] loss: 0.13751813722178913 acc: 0.6964
[Epoch 43] loss: 0.12496081409771043 acc: 0.7077
[Epoch 47] loss: 0.12403875347488867 acc: 0.6985
[Epoch 51] loss: 0.11353198841304692 acc: 0.7064
[Epoch 55] loss: 0.10826472751974887 acc: 0.7048
[Epoch 59] loss: 0.0984099825881088 acc: 0.6962
[Epoch 63] loss: 0.09651510958792642 acc: 0.6972
[Epoch 67] loss: 0.09041712723334161 acc: 0.6991
[Epoch 71] loss: 0.08342073356989971 acc: 0.6977
--> [test] acc: 0.7033
--> [accuracy] finished 0.7033
new state: tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.7033
--> [reward] 0.7033
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.7033 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.867021024379584 acc: 0.6581
[Epoch 7] loss: 2.4780733585357666 acc: 0.716
[Epoch 11] loss: 0.8844583074531287 acc: 0.6994
[Epoch 15] loss: 0.3898115601971784 acc: 0.7093
[Epoch 19] loss: 0.26368897281887244 acc: 0.6954
[Epoch 23] loss: 0.2277250001135537 acc: 0.7122
[Epoch 27] loss: 0.1835222112972413 acc: 0.6991
[Epoch 31] loss: 0.16458077135298144 acc: 0.6999
[Epoch 35] loss: 0.15535473284374712 acc: 0.7021
[Epoch 39] loss: 0.1306738567924427 acc: 0.7011
[Epoch 43] loss: 0.13388877581623967 acc: 0.7017
[Epoch 47] loss: 0.11719959892589799 acc: 0.6933
[Epoch 51] loss: 0.1144918373028469 acc: 0.7043
[Epoch 55] loss: 0.10192279939246757 acc: 0.6933
[Epoch 59] loss: 0.1008699198419888 acc: 0.7019
[Epoch 63] loss: 0.09504913936828828 acc: 0.7044
[Epoch 67] loss: 0.0924426982842643 acc: 0.7109
[Epoch 71] loss: 0.0909269620694787 acc: 0.7036
--> [test] acc: 0.6987
--> [accuracy] finished 0.6987
new state: tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6987
--> [reward] 0.6987
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.6987 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.845929662284949 acc: 0.6649
[Epoch 7] loss: 2.4464260659483084 acc: 0.7178
[Epoch 11] loss: 0.8634415487746906 acc: 0.7102
[Epoch 15] loss: 0.38718512601664534 acc: 0.713
[Epoch 19] loss: 0.26641394495916415 acc: 0.7009
[Epoch 23] loss: 0.21904387013972415 acc: 0.6933
[Epoch 27] loss: 0.19391305987839885 acc: 0.702
[Epoch 31] loss: 0.1617225838034316 acc: 0.7034
[Epoch 35] loss: 0.14962550548984266 acc: 0.7068
[Epoch 39] loss: 0.14266061898507892 acc: 0.7018
[Epoch 43] loss: 0.12455369536633916 acc: 0.7032
[Epoch 47] loss: 0.10835958916998809 acc: 0.705
[Epoch 51] loss: 0.11757911442090636 acc: 0.7019
[Epoch 55] loss: 0.10253656853247634 acc: 0.7031
[Epoch 59] loss: 0.10196287072408954 acc: 0.701
[Epoch 63] loss: 0.08677822729343991 acc: 0.7073
[Epoch 67] loss: 0.09186308309459663 acc: 0.703
[Epoch 71] loss: 0.0890694885629901 acc: 0.7026
--> [test] acc: 0.7001
--> [accuracy] finished 0.7001
new state: tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.7001
--> [reward] 0.7001
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.7001 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.860103534920441 acc: 0.6558
[Epoch 7] loss: 2.478818157978375 acc: 0.7133
[Epoch 11] loss: 0.8856231039080321 acc: 0.7148
[Epoch 15] loss: 0.3845424882381621 acc: 0.708
[Epoch 19] loss: 0.27945718979296247 acc: 0.7114
[Epoch 23] loss: 0.21991392622327866 acc: 0.7071
[Epoch 27] loss: 0.18995085459135835 acc: 0.707
[Epoch 31] loss: 0.16875759927351075 acc: 0.7037
[Epoch 35] loss: 0.1624100194908107 acc: 0.6978
[Epoch 39] loss: 0.13869152563716977 acc: 0.6927
[Epoch 43] loss: 0.1240654833051507 acc: 0.6981
[Epoch 47] loss: 0.12214396727818322 acc: 0.7099
[Epoch 51] loss: 0.10988652295149538 acc: 0.7074
[Epoch 55] loss: 0.10896323453081186 acc: 0.6998
[Epoch 59] loss: 0.10520771549135695 acc: 0.7058
[Epoch 63] loss: 0.08830490381018642 acc: 0.6995
[Epoch 67] loss: 0.09309993302145891 acc: 0.6997
[Epoch 71] loss: 0.08947576565996689 acc: 0.6928
--> [test] acc: 0.6989
--> [accuracy] finished 0.6989
new state: tensor([704.,   3.,   4.,   5.,   2.], device='cuda:0')
new reward: 0.6989
--> [reward] 0.6989
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     4.0     |     5.0      |     2.0     | 0.6989 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   4.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([704.,   3.,   3.,   5.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.13937434256839 acc: 0.6218
[Epoch 7] loss: 2.862368349986308 acc: 0.6718
[Epoch 11] loss: 1.1593712104193847 acc: 0.6825
[Epoch 15] loss: 0.4945900721613632 acc: 0.6898
[Epoch 19] loss: 0.32581203149350557 acc: 0.6911
[Epoch 23] loss: 0.2624804867393411 acc: 0.674
[Epoch 27] loss: 0.21414060632476722 acc: 0.6755
[Epoch 31] loss: 0.19208785095740386 acc: 0.6744
[Epoch 35] loss: 0.17967896318763418 acc: 0.6719
[Epoch 39] loss: 0.15897505229953535 acc: 0.6822
[Epoch 43] loss: 0.1531596512538488 acc: 0.6729
[Epoch 47] loss: 0.134400653789806 acc: 0.6861
[Epoch 51] loss: 0.14046251908292437 acc: 0.6814
[Epoch 55] loss: 0.12963002341587448 acc: 0.6832
[Epoch 59] loss: 0.11208436638742085 acc: 0.6739
[Epoch 63] loss: 0.110069908532183 acc: 0.6703
[Epoch 67] loss: 0.10195441839828268 acc: 0.6709
[Epoch 71] loss: 0.10343964086414394 acc: 0.6813
--> [test] acc: 0.6777
--> [accuracy] finished 0.6777
new state: tensor([704.,   3.,   3.,   5.,   2.], device='cuda:0')
new reward: 0.6777
--> [reward] 0.6777
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     5.0      |     2.0     | 0.6777 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   5.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   3.,   3.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.013696462297074 acc: 0.6333
[Epoch 7] loss: 2.6863379332872914 acc: 0.7029
[Epoch 11] loss: 1.0834398675814767 acc: 0.7003
[Epoch 15] loss: 0.4542270215070995 acc: 0.7005
[Epoch 19] loss: 0.31029370249680166 acc: 0.6938
[Epoch 23] loss: 0.25069817774893377 acc: 0.6953
[Epoch 27] loss: 0.21387914368582656 acc: 0.6945
[Epoch 31] loss: 0.18777104375212242 acc: 0.7037
[Epoch 35] loss: 0.16952734427941044 acc: 0.6985
[Epoch 39] loss: 0.16159244159431865 acc: 0.6943
[Epoch 43] loss: 0.1450484104250865 acc: 0.6998
[Epoch 47] loss: 0.12366198497655256 acc: 0.6926
[Epoch 51] loss: 0.13504850152043907 acc: 0.6971
[Epoch 55] loss: 0.11797997493253988 acc: 0.6961
[Epoch 59] loss: 0.10471734170185025 acc: 0.6968
[Epoch 63] loss: 0.11476996910902422 acc: 0.6967
[Epoch 67] loss: 0.10129960870389323 acc: 0.6874
[Epoch 71] loss: 0.0992894563773323 acc: 0.6977
--> [test] acc: 0.705
--> [accuracy] finished 0.705
new state: tensor([704.,   3.,   3.,   5.,   3.], device='cuda:0')
new reward: 0.705
--> [reward] 0.705
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     5.0      |     3.0     | 0.705  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.995220334481095 acc: 0.6556
[Epoch 7] loss: 2.710070802160846 acc: 0.709
[Epoch 11] loss: 1.1125262689483746 acc: 0.6889
[Epoch 15] loss: 0.482866904138566 acc: 0.7033
[Epoch 19] loss: 0.31097703003098287 acc: 0.6995
[Epoch 23] loss: 0.25136344972283337 acc: 0.6848
[Epoch 27] loss: 0.2160104426705395 acc: 0.7022
[Epoch 31] loss: 0.18835934898947054 acc: 0.6937
[Epoch 35] loss: 0.17403253478229122 acc: 0.7001
[Epoch 39] loss: 0.15823300140421562 acc: 0.6921
[Epoch 43] loss: 0.1457514247578352 acc: 0.6949
[Epoch 47] loss: 0.13403755294896252 acc: 0.7005
[Epoch 51] loss: 0.12678496873773196 acc: 0.6971
[Epoch 55] loss: 0.12088510463409641 acc: 0.6894
[Epoch 59] loss: 0.10923349368802802 acc: 0.6969
[Epoch 63] loss: 0.1040909468372593 acc: 0.686
[Epoch 67] loss: 0.10733658818246992 acc: 0.697
[Epoch 71] loss: 0.09678178893186656 acc: 0.6921
--> [test] acc: 0.6911
--> [accuracy] finished 0.6911
new state: tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
new reward: 0.6911
--> [reward] 0.6911
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     5.0      |     4.0     | 0.6911 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.031992127218515 acc: 0.6505
[Epoch 7] loss: 2.7360184376349537 acc: 0.7048
[Epoch 11] loss: 1.100844458907919 acc: 0.7115
[Epoch 15] loss: 0.4789404742410192 acc: 0.696
[Epoch 19] loss: 0.29751448312779066 acc: 0.7074
[Epoch 23] loss: 0.25129914346396387 acc: 0.6949
[Epoch 27] loss: 0.21544948183333554 acc: 0.6988
[Epoch 31] loss: 0.18101505085628222 acc: 0.7027
[Epoch 35] loss: 0.16347253782784238 acc: 0.6935
[Epoch 39] loss: 0.15513447122862728 acc: 0.7018
[Epoch 43] loss: 0.14407520436817575 acc: 0.7032
[Epoch 47] loss: 0.1314772044134605 acc: 0.6987
[Epoch 51] loss: 0.12048935302702324 acc: 0.688
[Epoch 55] loss: 0.12152134818251213 acc: 0.6928
[Epoch 59] loss: 0.1132356708516817 acc: 0.6896
[Epoch 63] loss: 0.10303747087783749 acc: 0.6901
[Epoch 67] loss: 0.10255516935354266 acc: 0.6959
[Epoch 71] loss: 0.0960859234588902 acc: 0.6926
--> [test] acc: 0.6975
--> [accuracy] finished 0.6975
new state: tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
new reward: 0.6975
--> [reward] 0.6975
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     5.0      |     4.0     | 0.6975 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.045393131425619 acc: 0.6464
[Epoch 7] loss: 2.6994950111259888 acc: 0.7022
[Epoch 11] loss: 1.0835578698102775 acc: 0.6937
[Epoch 15] loss: 0.47513462979908644 acc: 0.7055
[Epoch 19] loss: 0.31979311804008453 acc: 0.7005
[Epoch 23] loss: 0.25302851212013255 acc: 0.6926
[Epoch 27] loss: 0.20661772771969514 acc: 0.6988
[Epoch 31] loss: 0.18338425882646572 acc: 0.6975
[Epoch 35] loss: 0.16954597387977344 acc: 0.7009
[Epoch 39] loss: 0.14784113004031924 acc: 0.6862
[Epoch 43] loss: 0.14875030798701774 acc: 0.7023
[Epoch 47] loss: 0.13145668219651102 acc: 0.7013
[Epoch 51] loss: 0.13137025161243765 acc: 0.7001
[Epoch 55] loss: 0.12016091281024124 acc: 0.6925
[Epoch 59] loss: 0.11076572943535988 acc: 0.6955
[Epoch 63] loss: 0.10357200783109316 acc: 0.7007
[Epoch 67] loss: 0.11229039715660159 acc: 0.7032
[Epoch 71] loss: 0.10233224484512149 acc: 0.6896
--> [test] acc: 0.7003
--> [accuracy] finished 0.7003
new state: tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
new reward: 0.7003
--> [reward] 0.7003
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     5.0      |     4.0     | 0.7003 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.922459904342661 acc: 0.6548
[Epoch 7] loss: 2.5617696847528446 acc: 0.7162
[Epoch 11] loss: 1.0241343246968202 acc: 0.7042
[Epoch 15] loss: 0.4233476014860222 acc: 0.7133
[Epoch 19] loss: 0.29332827460354244 acc: 0.6949
[Epoch 23] loss: 0.23322517737327025 acc: 0.7209
[Epoch 27] loss: 0.19969099865811865 acc: 0.7162
[Epoch 31] loss: 0.1729679367197749 acc: 0.7076
[Epoch 35] loss: 0.15223362932786766 acc: 0.6968
[Epoch 39] loss: 0.1489120819701163 acc: 0.7086
[Epoch 43] loss: 0.13682167222747182 acc: 0.7065
[Epoch 47] loss: 0.12152923254624885 acc: 0.7045
[Epoch 51] loss: 0.12916045122520278 acc: 0.7095
[Epoch 55] loss: 0.10751946048428188 acc: 0.7092
[Epoch 59] loss: 0.10137959241526454 acc: 0.6996
[Epoch 63] loss: 0.1021008694299099 acc: 0.706
[Epoch 67] loss: 0.09926835860131973 acc: 0.7114
[Epoch 71] loss: 0.10050129315347227 acc: 0.7017
--> [test] acc: 0.7124
--> [accuracy] finished 0.7124
new state: tensor([704.,   3.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7124
--> [reward] 0.7124
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     3.0     |     4.0      |     4.0     | 0.7124 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.926677872000448 acc: 0.6516
[Epoch 7] loss: 2.5615806793770215 acc: 0.7194
[Epoch 11] loss: 1.0144570327609244 acc: 0.6987
[Epoch 15] loss: 0.4328532521148472 acc: 0.7154
[Epoch 19] loss: 0.28683201506819644 acc: 0.7123
[Epoch 23] loss: 0.22948863470445738 acc: 0.7158
[Epoch 27] loss: 0.19826452559112664 acc: 0.7134
[Epoch 31] loss: 0.18372330652869037 acc: 0.7042
[Epoch 35] loss: 0.16421009520964358 acc: 0.7048
[Epoch 39] loss: 0.14407415335516796 acc: 0.7108
[Epoch 43] loss: 0.13110002606233007 acc: 0.7112
[Epoch 47] loss: 0.1228051927283673 acc: 0.7147
[Epoch 51] loss: 0.12928293016322356 acc: 0.7106
[Epoch 55] loss: 0.11356382786065264 acc: 0.7028
[Epoch 59] loss: 0.1026978591013976 acc: 0.7052
[Epoch 63] loss: 0.09509297096810025 acc: 0.7093
[Epoch 67] loss: 0.10145190751126906 acc: 0.7057
[Epoch 71] loss: 0.09419760554629947 acc: 0.7055
--> [test] acc: 0.7122
--> [accuracy] finished 0.7122
new state: tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7122
--> [reward] 0.7122
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     3.0      |     3.0     |     4.0      |     4.0     | 0.7122 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0913, 0.0922, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0907]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3975, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3984, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([672.,   4.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.653382981067423 acc: 0.6878
[Epoch 7] loss: 2.2300449548017642 acc: 0.7095
[Epoch 11] loss: 0.7638057791258749 acc: 0.7326
[Epoch 15] loss: 0.34629003784578777 acc: 0.7313
[Epoch 19] loss: 0.24161632835169505 acc: 0.7075
[Epoch 23] loss: 0.20354880527605104 acc: 0.7293
[Epoch 27] loss: 0.17344209527039467 acc: 0.7323
[Epoch 31] loss: 0.157266742683223 acc: 0.726
[Epoch 35] loss: 0.14317255015330166 acc: 0.7232
[Epoch 39] loss: 0.12059450893760527 acc: 0.7264
[Epoch 43] loss: 0.12027456893649933 acc: 0.7248
[Epoch 47] loss: 0.10674002219129668 acc: 0.7198
[Epoch 51] loss: 0.1020276169629072 acc: 0.7298
[Epoch 55] loss: 0.09977864509429354 acc: 0.7292
[Epoch 59] loss: 0.09032305591101662 acc: 0.7218
[Epoch 63] loss: 0.088665603749666 acc: 0.729
[Epoch 67] loss: 0.08495664455147241 acc: 0.7247
[Epoch 71] loss: 0.07980476888736276 acc: 0.7264
--> [test] acc: 0.7341
--> [accuracy] finished 0.7341
new state: tensor([672.,   4.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7341
--> [reward] 0.7341
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2692]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5385]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7338]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7815]], device='cuda:0')
------ ------
delta_t: tensor([[0.7338]], device='cuda:0')
rewards[i]: 0.7341
values[i+1]: tensor([[0.0479]], device='cuda:0')
values[i]: tensor([[0.0477]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7338]], device='cuda:0')
delta_t: tensor([[0.7338]], device='cuda:0')
------ ------
policy_loss: 1.7363989353179932
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7338]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.3033]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.0682]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4381]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4859]], device='cuda:0')
------ ------
delta_t: tensor([[0.7116]], device='cuda:0')
rewards[i]: 0.7122
values[i+1]: tensor([[0.0477]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0478]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4381]], device='cuda:0')
delta_t: tensor([[0.7116]], device='cuda:0')
------ ------
policy_loss: 5.160243034362793
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4381]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.5837]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.5607]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1356]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1835]], device='cuda:0')
------ ------
delta_t: tensor([[0.7118]], device='cuda:0')
rewards[i]: 0.7124
values[i+1]: tensor([[0.0478]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0479]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1356]], device='cuda:0')
delta_t: tensor([[0.7118]], device='cuda:0')
------ ------
policy_loss: 10.257341384887695
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1356]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.5428]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.9183]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.8139]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8619]], device='cuda:0')
------ ------
delta_t: tensor([[0.6997]], device='cuda:0')
rewards[i]: 0.7003
values[i+1]: tensor([[0.0479]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0480]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.8139]], device='cuda:0')
delta_t: tensor([[0.6997]], device='cuda:0')
------ ------
policy_loss: 16.980958938598633
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.8139]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[13.6074]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.1292]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.4827]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.5308]], device='cuda:0')
------ ------
delta_t: tensor([[0.6969]], device='cuda:0')
rewards[i]: 0.6975
values[i+1]: tensor([[0.0480]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.4827]], device='cuda:0')
delta_t: tensor([[0.6969]], device='cuda:0')
------ ------
policy_loss: 25.311691284179688
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.4827]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[22.1704]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.1260]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1384]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.1866]], device='cuda:0')
------ ------
delta_t: tensor([[0.6905]], device='cuda:0')
rewards[i]: 0.6911
values[i+1]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0483]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1384]], device='cuda:0')
delta_t: tensor([[0.6905]], device='cuda:0')
------ ------
policy_loss: 35.2131233215332
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1384]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[33.6974]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.0539]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8014]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8497]], device='cuda:0')
------ ------
delta_t: tensor([[0.7045]], device='cuda:0')
rewards[i]: 0.705
values[i+1]: tensor([[0.0483]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0483]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8014]], device='cuda:0')
delta_t: tensor([[0.7045]], device='cuda:0')
------ ------
policy_loss: 46.704917907714844
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8014]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[48.4445]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.4943]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4309]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4789]], device='cuda:0')
------ ------
delta_t: tensor([[0.6774]], device='cuda:0')
rewards[i]: 0.6777
values[i+1]: tensor([[0.0483]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4309]], device='cuda:0')
delta_t: tensor([[0.6774]], device='cuda:0')
------ ------
policy_loss: 59.70124435424805
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4309]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[66.8968]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[36.9045]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.0749]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1231]], device='cuda:0')
------ ------
delta_t: tensor([[0.6983]], device='cuda:0')
rewards[i]: 0.6989
values[i+1]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.0749]], device='cuda:0')
delta_t: tensor([[0.6983]], device='cuda:0')
------ ------
policy_loss: 74.2416000366211
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.0749]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[89.4342]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.0750]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7138]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7619]], device='cuda:0')
------ ------
delta_t: tensor([[0.6996]], device='cuda:0')
rewards[i]: 0.7001
values[i+1]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7138]], device='cuda:0')
delta_t: tensor([[0.6996]], device='cuda:0')
------ ------
policy_loss: 90.32351684570312
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7138]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[116.4082]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[53.9480]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.3449]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.3930]], device='cuda:0')
------ ------
delta_t: tensor([[0.6983]], device='cuda:0')
rewards[i]: 0.6987
values[i+1]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.3449]], device='cuda:0')
delta_t: tensor([[0.6983]], device='cuda:0')
------ ------
policy_loss: 107.91204071044922
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.3449]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[148.2039]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[63.5913]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9744]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.0224]], device='cuda:0')
------ ------
delta_t: tensor([[0.7029]], device='cuda:0')
rewards[i]: 0.7033
values[i+1]: tensor([[0.0481]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0480]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9744]], device='cuda:0')
delta_t: tensor([[0.7029]], device='cuda:0')
------ ------
policy_loss: 127.00884246826172
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9744]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[184.9755]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[73.5432]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.5757]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6231]], device='cuda:0')
------ ------
delta_t: tensor([[0.6811]], device='cuda:0')
rewards[i]: 0.681
values[i+1]: tensor([[0.0480]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0474]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.5757]], device='cuda:0')
delta_t: tensor([[0.6811]], device='cuda:0')
------ ------
policy_loss: 147.54873657226562
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.5757]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[227.1597]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[84.3685]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.1852]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2319]], device='cuda:0')
------ ------
delta_t: tensor([[0.6953]], device='cuda:0')
rewards[i]: 0.695
values[i+1]: tensor([[0.0474]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0467]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.1852]], device='cuda:0')
delta_t: tensor([[0.6953]], device='cuda:0')
------ ------
policy_loss: 169.55735778808594
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.1852]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[275.1043]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[95.8892]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.7923]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.8381]], device='cuda:0')
------ ------
delta_t: tensor([[0.6989]], device='cuda:0')
rewards[i]: 0.6985
values[i+1]: tensor([[0.0467]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0458]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.7923]], device='cuda:0')
delta_t: tensor([[0.6989]], device='cuda:0')
------ ------
policy_loss: 193.01007080078125
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.7923]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 193.01007080078125
value_loss: 275.10430908203125
loss: 330.5622253417969



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-3.6451e-03, -1.5176e-05, -1.7725e-05, -2.1967e-05, -1.2422e-05],
        [ 8.2838e-02,  3.4420e-04,  4.0199e-04,  4.9643e-04,  2.7791e-04],
        [-1.3548e-03, -5.6346e-06, -6.6009e-06, -8.2965e-06, -4.6513e-06],
        [ 3.9761e-01,  1.6498e-03,  1.9311e-03,  2.3950e-03,  1.3342e-03],
        [ 1.9899e+00,  8.2515e-03,  9.6493e-03,  1.1968e-02,  6.6945e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 3.1047e-05,  3.2398e-05, -8.5277e-06, -3.3000e-05,  2.6278e-05],
        [-7.0470e-04, -7.3626e-04,  1.9268e-04,  7.4999e-04, -5.9634e-04],
        [ 1.1540e-05,  1.1999e-05, -3.1984e-06, -1.2218e-05,  9.7739e-06],
        [-3.3799e-03, -3.5270e-03,  9.2670e-04,  3.5922e-03, -2.8610e-03],
        [-1.6903e-02, -1.7643e-02,  4.6325e-03,  1.7968e-02, -1.4309e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.5692, -0.5591,  0.5687, -0.5059,  0.3246],
        [-0.1517,  0.1490, -0.1515,  0.1348, -0.0865],
        [-0.3059,  0.3004, -0.3056,  0.2718, -0.1744],
        [-0.0071,  0.0070, -0.0071,  0.0063, -0.0040],
        [-0.1046,  0.1027, -0.1045,  0.0929, -0.0596]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0014,  0.0012, -0.0006, -0.0013,  0.0011],
        [-0.0420, -0.0430,  0.0120,  0.0437, -0.0356],
        [-0.0415, -0.0425,  0.0119,  0.0432, -0.0352],
        [ 0.0204,  0.0204, -0.0062, -0.0208,  0.0172],
        [ 0.0433,  0.0453, -0.0122, -0.0460,  0.0371],
        [ 0.0046,  0.0046, -0.0009, -0.0048,  0.0037],
        [-0.0295, -0.0303,  0.0082,  0.0308, -0.0250],
        [ 0.0654,  0.0669, -0.0183, -0.0681,  0.0553],
        [ 0.0102,  0.0113, -0.0024, -0.0113,  0.0088],
        [ 0.0088,  0.0081, -0.0032, -0.0084,  0.0074],
        [-0.0412, -0.0421,  0.0118,  0.0429, -0.0349]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.5038,  2.5626, -0.7153, -2.6054,  2.1226]], device='cuda:0')
--> [loss] 330.5622253417969

---------------------------------- [[#15 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     4.0      |     3.0     |     4.0      |     4.0     | 0.7341 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   4.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.886048580679442 acc: 0.6065
[Epoch 7] loss: 2.565778203251417 acc: 0.7069
[Epoch 11] loss: 1.0100714670413213 acc: 0.7088
[Epoch 15] loss: 0.4295401308933259 acc: 0.7182
[Epoch 19] loss: 0.29787071324560954 acc: 0.7169
[Epoch 23] loss: 0.23299792983933634 acc: 0.706
[Epoch 27] loss: 0.20258440589413163 acc: 0.713
[Epoch 31] loss: 0.171394911001596 acc: 0.7119
[Epoch 35] loss: 0.16783227391131317 acc: 0.7091
[Epoch 39] loss: 0.14456059595288903 acc: 0.7153
[Epoch 43] loss: 0.14173328459424817 acc: 0.7036
[Epoch 47] loss: 0.12515326209998714 acc: 0.7058
[Epoch 51] loss: 0.12273673824769447 acc: 0.7025
[Epoch 55] loss: 0.1118791369156188 acc: 0.6983
[Epoch 59] loss: 0.11399476663888339 acc: 0.708
[Epoch 63] loss: 0.10280352631730297 acc: 0.7012
[Epoch 67] loss: 0.0924250712401479 acc: 0.7121
[Epoch 71] loss: 0.09342726525526655 acc: 0.7049
--> [test] acc: 0.7055
--> [accuracy] finished 0.7055
new state: tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7055
--> [reward] 0.7055
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     3.0      |     3.0     |     4.0      |     4.0     | 0.7055 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([672.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.047836951282628 acc: 0.6417
[Epoch 7] loss: 2.726235923940873 acc: 0.7061
[Epoch 11] loss: 1.1378952964968847 acc: 0.6782
[Epoch 15] loss: 0.4822094340086974 acc: 0.6972
[Epoch 19] loss: 0.3054401341139737 acc: 0.6997
[Epoch 23] loss: 0.25363520325144845 acc: 0.6992
[Epoch 27] loss: 0.21126805930195944 acc: 0.7038
[Epoch 31] loss: 0.18966715085460706 acc: 0.7035
[Epoch 35] loss: 0.17209894923955354 acc: 0.6869
[Epoch 39] loss: 0.1482816507427684 acc: 0.6952
[Epoch 43] loss: 0.1456750388997976 acc: 0.7013
[Epoch 47] loss: 0.13325551609256212 acc: 0.6891
[Epoch 51] loss: 0.129384044388695 acc: 0.6997
[Epoch 55] loss: 0.11697980013790318 acc: 0.6878
[Epoch 59] loss: 0.11633408587435475 acc: 0.6934
[Epoch 63] loss: 0.10202378020419375 acc: 0.6936
[Epoch 67] loss: 0.10314809840501231 acc: 0.6972
[Epoch 71] loss: 0.10085732806557336 acc: 0.6974
--> [test] acc: 0.7011
--> [accuracy] finished 0.7011
new state: tensor([672.,   3.,   3.,   5.,   4.], device='cuda:0')
new reward: 0.7011
--> [reward] 0.7011
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     3.0      |     3.0     |     5.0      |     4.0     | 0.7011 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   3.,   3.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.989276912206274 acc: 0.64
[Epoch 7] loss: 2.6405066071873735 acc: 0.7171
[Epoch 11] loss: 1.0476602722254709 acc: 0.7105
[Epoch 15] loss: 0.4356359533889367 acc: 0.7171
[Epoch 19] loss: 0.3010769635700928 acc: 0.7044
[Epoch 23] loss: 0.24152879986692877 acc: 0.7098
[Epoch 27] loss: 0.19890642713021744 acc: 0.719
[Epoch 31] loss: 0.18329861844220505 acc: 0.7076
[Epoch 35] loss: 0.15924813560284007 acc: 0.7086
[Epoch 39] loss: 0.15094064759647907 acc: 0.6981
[Epoch 43] loss: 0.13114683189527002 acc: 0.6999
[Epoch 47] loss: 0.1369906901614145 acc: 0.7172
[Epoch 51] loss: 0.11761810624992232 acc: 0.7119
[Epoch 55] loss: 0.11457956373534353 acc: 0.7114
[Epoch 59] loss: 0.10560378386749816 acc: 0.704
[Epoch 63] loss: 0.09441360560394085 acc: 0.7087
[Epoch 67] loss: 0.10104746048164832 acc: 0.7123
[Epoch 71] loss: 0.09519068286170626 acc: 0.7075
--> [test] acc: 0.7073
--> [accuracy] finished 0.7073
new state: tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7073
--> [reward] 0.7073
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     3.0      |     3.0     |     4.0      |     4.0     | 0.7073 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.9124562304343105 acc: 0.6488
[Epoch 7] loss: 2.633328367148519 acc: 0.7199
[Epoch 11] loss: 1.0433651286527477 acc: 0.7084
[Epoch 15] loss: 0.4549736753296669 acc: 0.7025
[Epoch 19] loss: 0.30094377913505144 acc: 0.7147
[Epoch 23] loss: 0.23580778112916082 acc: 0.7132
[Epoch 27] loss: 0.20217137776143715 acc: 0.7129
[Epoch 31] loss: 0.18503126159996328 acc: 0.7173
[Epoch 35] loss: 0.1539130272383056 acc: 0.7095
[Epoch 39] loss: 0.15215322588477523 acc: 0.712
[Epoch 43] loss: 0.14130630313902331 acc: 0.7069
[Epoch 47] loss: 0.12620445417330775 acc: 0.709
[Epoch 51] loss: 0.12502070208963798 acc: 0.7073
[Epoch 55] loss: 0.10694374965237038 acc: 0.7041
[Epoch 59] loss: 0.11193985071109461 acc: 0.7046
[Epoch 63] loss: 0.10417242729536179 acc: 0.7123
[Epoch 67] loss: 0.1007693107948517 acc: 0.7068
[Epoch 71] loss: 0.08869828185533433 acc: 0.7104
--> [test] acc: 0.7169
--> [accuracy] finished 0.7169
new state: tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7169
--> [reward] 0.7169
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     3.0      |     3.0     |     4.0      |     4.0     | 0.7169 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([672.,   3.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([672.,   4.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.706356381821205 acc: 0.6739
[Epoch 7] loss: 2.2583314102247853 acc: 0.6981
[Epoch 11] loss: 0.7737798598806953 acc: 0.7266
[Epoch 15] loss: 0.34294024790587174 acc: 0.7111
[Epoch 19] loss: 0.23784205588795568 acc: 0.7283
[Epoch 23] loss: 0.20835719605707717 acc: 0.7295
[Epoch 27] loss: 0.16288507739415445 acc: 0.7268
[Epoch 31] loss: 0.14985113912690765 acc: 0.7242
[Epoch 35] loss: 0.14276851530251145 acc: 0.7234
[Epoch 39] loss: 0.12017486011073508 acc: 0.7225
[Epoch 43] loss: 0.12035734711638878 acc: 0.7291
[Epoch 47] loss: 0.10063390569080172 acc: 0.7288
[Epoch 51] loss: 0.10195903130985624 acc: 0.7294
[Epoch 55] loss: 0.09340970346729731 acc: 0.724
[Epoch 59] loss: 0.09372003348377507 acc: 0.7232
[Epoch 63] loss: 0.08064675503743865 acc: 0.723
[Epoch 67] loss: 0.08437538060174762 acc: 0.7215
[Epoch 71] loss: 0.0785557694452972 acc: 0.7253
--> [test] acc: 0.7277
--> [accuracy] finished 0.7277
new state: tensor([672.,   4.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.7277
--> [reward] 0.7277
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     4.0      |     3.0     |     4.0      |     4.0     | 0.7277 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   4.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.856977976496567 acc: 0.6633
[Epoch 7] loss: 2.4390489725810487 acc: 0.7098
[Epoch 11] loss: 0.8704106350288824 acc: 0.7227
[Epoch 15] loss: 0.38219003999115103 acc: 0.7085
[Epoch 19] loss: 0.25841597695608653 acc: 0.7187
[Epoch 23] loss: 0.21126532476976553 acc: 0.7153
[Epoch 27] loss: 0.1793082879875284 acc: 0.7128
[Epoch 31] loss: 0.16126573109842093 acc: 0.71
[Epoch 35] loss: 0.1432749167670641 acc: 0.7105
[Epoch 39] loss: 0.13250583163021928 acc: 0.7092
[Epoch 43] loss: 0.12475765177317898 acc: 0.7106
[Epoch 47] loss: 0.10891638062519195 acc: 0.7098
[Epoch 51] loss: 0.1005035125236015 acc: 0.7053
[Epoch 55] loss: 0.11678840596314587 acc: 0.7004
[Epoch 59] loss: 0.08669575723412874 acc: 0.7083
[Epoch 63] loss: 0.09561497162666667 acc: 0.7149
[Epoch 67] loss: 0.08842707779246581 acc: 0.707
[Epoch 71] loss: 0.08217925078551466 acc: 0.7051
--> [test] acc: 0.7047
--> [accuracy] finished 0.7047
new state: tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.7047
--> [reward] 0.7047
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     4.0      |     3.0     |     4.0      |     5.0     | 0.7047 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.876462946462509 acc: 0.6447
[Epoch 7] loss: 2.462693595749033 acc: 0.7165
[Epoch 11] loss: 0.9060364290976616 acc: 0.7139
[Epoch 15] loss: 0.3936328115537191 acc: 0.7161
[Epoch 19] loss: 0.25874971647930267 acc: 0.7095
[Epoch 23] loss: 0.2065687178041014 acc: 0.7079
[Epoch 27] loss: 0.18673823673106596 acc: 0.7062
[Epoch 31] loss: 0.15907336954418047 acc: 0.7074
[Epoch 35] loss: 0.14314063903554092 acc: 0.7139
[Epoch 39] loss: 0.13762221178111364 acc: 0.7072
[Epoch 43] loss: 0.12522626477662865 acc: 0.7088
[Epoch 47] loss: 0.11834195269571851 acc: 0.7062
[Epoch 51] loss: 0.10783358285715089 acc: 0.7079
[Epoch 55] loss: 0.10730307490643486 acc: 0.7003
[Epoch 59] loss: 0.09561394770865513 acc: 0.7062
[Epoch 63] loss: 0.09393909334341603 acc: 0.7077
[Epoch 67] loss: 0.09127053947014081 acc: 0.6922
[Epoch 71] loss: 0.08270567637862866 acc: 0.6903
--> [test] acc: 0.7085
--> [accuracy] finished 0.7085
new state: tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.7085
--> [reward] 0.7085
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     4.0      |     3.0     |     4.0      |     5.0     | 0.7085 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.831087368985881 acc: 0.6594
[Epoch 7] loss: 2.4327762043079755 acc: 0.7133
[Epoch 11] loss: 0.861846368607429 acc: 0.7088
[Epoch 15] loss: 0.38484718158955467 acc: 0.7122
[Epoch 19] loss: 0.25628128949829077 acc: 0.7117
[Epoch 23] loss: 0.2091936024978919 acc: 0.7125
[Epoch 27] loss: 0.18355665891609915 acc: 0.7117
[Epoch 31] loss: 0.16125854050152272 acc: 0.6924
[Epoch 35] loss: 0.13334601832425122 acc: 0.711
[Epoch 39] loss: 0.13447537989822475 acc: 0.7077
[Epoch 43] loss: 0.11939169781144394 acc: 0.7031
[Epoch 47] loss: 0.11293343680760706 acc: 0.7086
[Epoch 51] loss: 0.10466442725119775 acc: 0.7036
[Epoch 55] loss: 0.10607449402151357 acc: 0.7019
[Epoch 59] loss: 0.09787575620443315 acc: 0.7152
[Epoch 63] loss: 0.08713657337793475 acc: 0.7079
[Epoch 67] loss: 0.08539993650167692 acc: 0.7079
[Epoch 71] loss: 0.08564663408754353 acc: 0.7035
--> [test] acc: 0.705
--> [accuracy] finished 0.705
new state: tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.705
--> [reward] 0.705
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     4.0      |     3.0     |     4.0      |     5.0     | 0.705  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.846331261917758 acc: 0.6707
[Epoch 7] loss: 2.4329907316762163 acc: 0.7173
[Epoch 11] loss: 0.8568532587603077 acc: 0.7093
[Epoch 15] loss: 0.38373863794233487 acc: 0.7082
[Epoch 19] loss: 0.2609535828375679 acc: 0.7153
[Epoch 23] loss: 0.21210840749113685 acc: 0.7094
[Epoch 27] loss: 0.18851126999120274 acc: 0.7024
[Epoch 31] loss: 0.15265874536779453 acc: 0.7028
[Epoch 35] loss: 0.14915392181509748 acc: 0.7047
[Epoch 39] loss: 0.13627607915121728 acc: 0.7036
[Epoch 43] loss: 0.11992377254283032 acc: 0.7102
[Epoch 47] loss: 0.1133472806356652 acc: 0.6915
[Epoch 51] loss: 0.10541716692945384 acc: 0.6985
[Epoch 55] loss: 0.10375303318819312 acc: 0.7083
[Epoch 59] loss: 0.09898418548327569 acc: 0.6976
[Epoch 63] loss: 0.08592235833368338 acc: 0.701
[Epoch 67] loss: 0.08643540769727315 acc: 0.7031
[Epoch 71] loss: 0.08804847850122482 acc: 0.704
--> [test] acc: 0.7037
--> [accuracy] finished 0.7037
new state: tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.7037
--> [reward] 0.7037
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     4.0      |     3.0     |     4.0      |     5.0     | 0.7037 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3966, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.819697954770549 acc: 0.6447
[Epoch 7] loss: 2.426367494684961 acc: 0.7138
[Epoch 11] loss: 0.8794363104855009 acc: 0.7055
[Epoch 15] loss: 0.3758701094095131 acc: 0.7015
[Epoch 19] loss: 0.25938481983164197 acc: 0.7014
[Epoch 23] loss: 0.21255595068140026 acc: 0.7075
[Epoch 27] loss: 0.1783147390760348 acc: 0.7126
[Epoch 31] loss: 0.15833254929993998 acc: 0.7136
[Epoch 35] loss: 0.14878701443290887 acc: 0.7149
[Epoch 39] loss: 0.13531158500305757 acc: 0.7158
[Epoch 43] loss: 0.12500982090909404 acc: 0.7063
[Epoch 47] loss: 0.10777657016006577 acc: 0.711
[Epoch 51] loss: 0.11365578989700778 acc: 0.6971
[Epoch 55] loss: 0.09824152540920487 acc: 0.7137
[Epoch 59] loss: 0.09445892872950991 acc: 0.7115
[Epoch 63] loss: 0.09444232534266153 acc: 0.7098
[Epoch 67] loss: 0.08431081488029436 acc: 0.7123
[Epoch 71] loss: 0.08584699122969995 acc: 0.7031
--> [test] acc: 0.7131
--> [accuracy] finished 0.7131
new state: tensor([704.,   4.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.7131
--> [reward] 0.7131
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     3.0     |     4.0      |     5.0     | 0.7131 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([704.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.868607411451657 acc: 0.6537
[Epoch 7] loss: 2.4660687251850164 acc: 0.7109
[Epoch 11] loss: 0.8682551945052336 acc: 0.7108
[Epoch 15] loss: 0.3738522760670089 acc: 0.709
[Epoch 19] loss: 0.2567026956278421 acc: 0.7184
[Epoch 23] loss: 0.2124097709284376 acc: 0.7159
[Epoch 27] loss: 0.17783828282638278 acc: 0.7084
[Epoch 31] loss: 0.1530313935783475 acc: 0.7122
[Epoch 35] loss: 0.14011879468960758 acc: 0.7113
[Epoch 39] loss: 0.12999457026333988 acc: 0.7116
[Epoch 43] loss: 0.1253126597111149 acc: 0.7055
[Epoch 47] loss: 0.10979084739499652 acc: 0.7144
[Epoch 51] loss: 0.1065648726504439 acc: 0.7037
[Epoch 55] loss: 0.09725544405172167 acc: 0.7069
[Epoch 59] loss: 0.09629725960566832 acc: 0.7008
[Epoch 63] loss: 0.09262946781599918 acc: 0.7063
[Epoch 67] loss: 0.08478786781206346 acc: 0.7046
[Epoch 71] loss: 0.08526193248608228 acc: 0.703
--> [test] acc: 0.714
--> [accuracy] finished 0.714
new state: tensor([704.,   4.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.714
--> [reward] 0.714
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     3.0     |     4.0      |     5.0     | 0.714  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([704.,   4.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.084663996001339 acc: 0.6363
[Epoch 7] loss: 2.8661165547645306 acc: 0.6863
[Epoch 11] loss: 1.2452900618162301 acc: 0.6877
[Epoch 15] loss: 0.5266897678327607 acc: 0.6756
[Epoch 19] loss: 0.32717097104739046 acc: 0.6841
[Epoch 23] loss: 0.2570996996434525 acc: 0.6644
[Epoch 27] loss: 0.23072848691488318 acc: 0.6805
[Epoch 31] loss: 0.1894143659340413 acc: 0.679
[Epoch 35] loss: 0.17892192706675328 acc: 0.6866
[Epoch 39] loss: 0.1582728163142691 acc: 0.6791
[Epoch 43] loss: 0.15759033680586215 acc: 0.677
[Epoch 47] loss: 0.13374939013708412 acc: 0.6841
[Epoch 51] loss: 0.13486733484913208 acc: 0.6748
[Epoch 55] loss: 0.12359591754858414 acc: 0.6746
[Epoch 59] loss: 0.11424600025948585 acc: 0.68
[Epoch 63] loss: 0.10962798783276945 acc: 0.6748
[Epoch 67] loss: 0.10782082569512987 acc: 0.6785
[Epoch 71] loss: 0.11046245782290731 acc: 0.6839
--> [test] acc: 0.6782
--> [accuracy] finished 0.6782
new state: tensor([704.,   4.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.6782
--> [reward] 0.6782
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     2.0     |     4.0      |     5.0     | 0.6782 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([704.,   3.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.3126590427230385 acc: 0.6232
[Epoch 7] loss: 3.2103736879270706 acc: 0.685
[Epoch 11] loss: 1.6618529040071055 acc: 0.675
[Epoch 15] loss: 0.7358576215117636 acc: 0.67
[Epoch 19] loss: 0.4215501799388691 acc: 0.6566
[Epoch 23] loss: 0.32154920488557853 acc: 0.6687
[Epoch 27] loss: 0.26962563999788003 acc: 0.6405
[Epoch 31] loss: 0.2375435914506045 acc: 0.6582
[Epoch 35] loss: 0.20881911900961567 acc: 0.666
[Epoch 39] loss: 0.19413160304288807 acc: 0.6623
[Epoch 43] loss: 0.17835562204932578 acc: 0.6558
[Epoch 47] loss: 0.1633515230372853 acc: 0.6603
[Epoch 51] loss: 0.15727579358922283 acc: 0.6572
[Epoch 55] loss: 0.15027910143808673 acc: 0.6625
[Epoch 59] loss: 0.13627446662453588 acc: 0.6638
[Epoch 63] loss: 0.13623438688361889 acc: 0.6615
[Epoch 67] loss: 0.1304952952501071 acc: 0.6628
[Epoch 71] loss: 0.12549740931405054 acc: 0.6591
--> [test] acc: 0.659
--> [accuracy] finished 0.659
new state: tensor([704.,   3.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.659
--> [reward] 0.659
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     2.0     |     4.0      |     5.0     | 0.659  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.684057295627301 acc: 0.5729
[Epoch 7] loss: 3.8200439707092615 acc: 0.6101
[Epoch 11] loss: 2.3612546731367745 acc: 0.6363
[Epoch 15] loss: 1.195412564289082 acc: 0.6314
[Epoch 19] loss: 0.6412000199374945 acc: 0.6296
[Epoch 23] loss: 0.44773972035883486 acc: 0.6231
[Epoch 27] loss: 0.36825390404466624 acc: 0.6225
[Epoch 31] loss: 0.3037323812927927 acc: 0.6245
[Epoch 35] loss: 0.2869932983747071 acc: 0.6259
[Epoch 39] loss: 0.26354792383987735 acc: 0.6206
[Epoch 43] loss: 0.23999788821377147 acc: 0.6094
[Epoch 47] loss: 0.21911513909240207 acc: 0.6161
[Epoch 51] loss: 0.2184412064045058 acc: 0.6192
[Epoch 55] loss: 0.1975390617902417 acc: 0.6164
[Epoch 59] loss: 0.19131582395395125 acc: 0.6221
[Epoch 63] loss: 0.1749640435583013 acc: 0.6243
[Epoch 67] loss: 0.17320733723799933 acc: 0.6232
[Epoch 71] loss: 0.16013646326111178 acc: 0.6162
--> [test] acc: 0.6108
--> [accuracy] finished 0.6108
new state: tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.6108
--> [reward] 0.6108
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     4.0      |     5.0     | 0.6108 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0914, 0.0910, 0.0908, 0.0909, 0.0901,
         0.0904, 0.0906]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3979,
         -2.3987, -2.3985, -2.3982]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.717100457309762 acc: 0.5505
[Epoch 7] loss: 3.8517327182128303 acc: 0.6409
[Epoch 11] loss: 2.4022288658963444 acc: 0.6393
[Epoch 15] loss: 1.2134133159847516 acc: 0.6232
[Epoch 19] loss: 0.6463874236816336 acc: 0.6287
[Epoch 23] loss: 0.4595819653090461 acc: 0.632
[Epoch 27] loss: 0.3663411725936529 acc: 0.6257
[Epoch 31] loss: 0.3168466851577315 acc: 0.6257
[Epoch 35] loss: 0.2796004409627879 acc: 0.6223
[Epoch 39] loss: 0.2684120420495148 acc: 0.6288
[Epoch 43] loss: 0.23622713155587158 acc: 0.618
[Epoch 47] loss: 0.22106185761730537 acc: 0.6265
[Epoch 51] loss: 0.20084206729441348 acc: 0.6238
[Epoch 55] loss: 0.2004334215846513 acc: 0.6159
[Epoch 59] loss: 0.1918345088760852 acc: 0.6183
[Epoch 63] loss: 0.1667043985703679 acc: 0.6205
[Epoch 67] loss: 0.17427632606128599 acc: 0.6169
[Epoch 71] loss: 0.1656319401977236 acc: 0.611
--> [test] acc: 0.6209
--> [accuracy] finished 0.6209
new state: tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.6209
--> [reward] 0.6209
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1924]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3849]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6204]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6729]], device='cuda:0')
------ ------
delta_t: tensor([[0.6204]], device='cuda:0')
rewards[i]: 0.6209
values[i+1]: tensor([[0.0525]], device='cuda:0')
values[i]: tensor([[0.0525]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6204]], device='cuda:0')
delta_t: tensor([[0.6204]], device='cuda:0')
------ ------
policy_loss: 1.463809847831726
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6204]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.9419]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.4990]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2243]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.2769]], device='cuda:0')
------ ------
delta_t: tensor([[0.6102]], device='cuda:0')
rewards[i]: 0.6108
values[i+1]: tensor([[0.0525]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0526]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2243]], device='cuda:0')
delta_t: tensor([[0.6102]], device='cuda:0')
------ ------
policy_loss: 4.375190258026123
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2243]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.6916]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.4993]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8706]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9232]], device='cuda:0')
------ ------
delta_t: tensor([[0.6586]], device='cuda:0')
rewards[i]: 0.659
values[i+1]: tensor([[0.0526]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0525]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8706]], device='cuda:0')
delta_t: tensor([[0.6586]], device='cuda:0')
------ ------
policy_loss: 8.83612060546875
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8706]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.8920]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.4009]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5300]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5821]], device='cuda:0')
------ ------
delta_t: tensor([[0.6781]], device='cuda:0')
rewards[i]: 0.6782
values[i+1]: tensor([[0.0525]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5300]], device='cuda:0')
delta_t: tensor([[0.6781]], device='cuda:0')
------ ------
policy_loss: 14.877716064453125
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5300]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[11.0705]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.3569]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.2182]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2703]], device='cuda:0')
------ ------
delta_t: tensor([[0.7135]], device='cuda:0')
rewards[i]: 0.714
values[i+1]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.2182]], device='cuda:0')
delta_t: tensor([[0.7135]], device='cuda:0')
------ ------
policy_loss: 22.57028579711914
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.2182]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[18.6703]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[15.1996]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.8987]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.9507]], device='cuda:0')
------ ------
delta_t: tensor([[0.7126]], device='cuda:0')
rewards[i]: 0.7131
values[i+1]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0520]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.8987]], device='cuda:0')
delta_t: tensor([[0.7126]], device='cuda:0')
------ ------
policy_loss: 31.88965606689453
log_probs[i]: tensor([[-2.3966]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.8987]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[29.0800]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[20.8195]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.5628]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.6149]], device='cuda:0')
------ ------
delta_t: tensor([[0.7032]], device='cuda:0')
rewards[i]: 0.7037
values[i+1]: tensor([[0.0520]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.5628]], device='cuda:0')
delta_t: tensor([[0.7032]], device='cuda:0')
------ ------
policy_loss: 42.80632400512695
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.5628]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[42.7130]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[27.2660]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2217]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.2738]], device='cuda:0')
------ ------
delta_t: tensor([[0.7045]], device='cuda:0')
rewards[i]: 0.705
values[i+1]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2217]], device='cuda:0')
delta_t: tensor([[0.7045]], device='cuda:0')
------ ------
policy_loss: 55.306297302246094
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2217]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[59.9853]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.5445]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8775]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.9295]], device='cuda:0')
------ ------
delta_t: tensor([[0.7080]], device='cuda:0')
rewards[i]: 0.7085
values[i+1]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8775]], device='cuda:0')
delta_t: tensor([[0.7080]], device='cuda:0')
------ ------
policy_loss: 69.37911224365234
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8775]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[81.2597]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[42.5488]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.5229]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.5749]], device='cuda:0')
------ ------
delta_t: tensor([[0.7043]], device='cuda:0')
rewards[i]: 0.7047
values[i+1]: tensor([[0.0521]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0520]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.5229]], device='cuda:0')
delta_t: tensor([[0.7043]], device='cuda:0')
------ ------
policy_loss: 85.00009155273438
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.5229]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[107.0730]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[51.6266]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.1852]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2369]], device='cuda:0')
------ ------
delta_t: tensor([[0.7275]], device='cuda:0')
rewards[i]: 0.7277
values[i+1]: tensor([[0.0520]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0517]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.1852]], device='cuda:0')
delta_t: tensor([[0.7275]], device='cuda:0')
------ ------
policy_loss: 102.21247863769531
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.1852]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[137.7255]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[61.3050]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.8298]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.8814]], device='cuda:0')
------ ------
delta_t: tensor([[0.7164]], device='cuda:0')
rewards[i]: 0.7169
values[i+1]: tensor([[0.0517]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0516]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.8298]], device='cuda:0')
delta_t: tensor([[0.7164]], device='cuda:0')
------ ------
policy_loss: 120.96588897705078
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.8298]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[173.4976]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[71.5441]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.4584]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.5099]], device='cuda:0')
------ ------
delta_t: tensor([[0.7069]], device='cuda:0')
rewards[i]: 0.7073
values[i+1]: tensor([[0.0516]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0515]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.4584]], device='cuda:0')
delta_t: tensor([[0.7069]], device='cuda:0')
------ ------
policy_loss: 141.22499084472656
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.4584]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[214.6724]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[82.3496]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.0747]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.1259]], device='cuda:0')
------ ------
delta_t: tensor([[0.7009]], device='cuda:0')
rewards[i]: 0.7011
values[i+1]: tensor([[0.0515]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0512]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.0747]], device='cuda:0')
delta_t: tensor([[0.7009]], device='cuda:0')
------ ------
policy_loss: 162.9608917236328
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.0747]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[261.6152]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[93.8856]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.6895]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.7401]], device='cuda:0')
------ ------
delta_t: tensor([[0.7055]], device='cuda:0')
rewards[i]: 0.7055
values[i+1]: tensor([[0.0512]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0507]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.6895]], device='cuda:0')
delta_t: tensor([[0.7055]], device='cuda:0')
------ ------
policy_loss: 186.16769409179688
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.6895]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 186.16769409179688
value_loss: 261.61517333984375
loss: 316.97528076171875



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-5.2521e-03, -2.5757e-05, -2.3182e-05, -3.2774e-05, -3.1747e-05],
        [ 9.1686e-02,  4.4134e-04,  4.0441e-04,  5.7669e-04,  5.4405e-04],
        [-1.7547e-03, -8.7405e-06, -7.7450e-06, -1.0888e-05, -1.0759e-05],
        [ 3.8358e-01,  1.8494e-03,  1.6912e-03,  2.4116e-03,  2.2797e-03],
        [ 1.8958e+00,  9.1716e-03,  8.3512e-03,  1.1911e-02,  1.1298e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 4.6812e-05,  4.6245e-05, -1.5283e-05, -4.6586e-05,  3.9861e-05],
        [-8.1532e-04, -8.0798e-04,  2.6370e-04,  8.1459e-04, -6.9398e-04],
        [ 1.5662e-05,  1.5427e-05, -5.1562e-06, -1.5530e-05,  1.3340e-05],
        [-3.4103e-03, -3.3783e-03,  1.1042e-03,  3.4057e-03, -2.9027e-03],
        [-1.6850e-02, -1.6689e-02,  5.4607e-03,  1.6823e-02, -1.4342e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.5344, -0.5221,  0.5336, -0.4642,  0.2911],
        [-0.1353,  0.1322, -0.1351,  0.1178, -0.0740],
        [-0.2766,  0.2703, -0.2762,  0.2401, -0.1505],
        [-0.0075,  0.0073, -0.0075,  0.0065, -0.0041],
        [-0.1150,  0.1123, -0.1148,  0.0998, -0.0626]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0392, -0.0379,  0.0135,  0.0381, -0.0334],
        [-0.0179, -0.0174,  0.0058,  0.0175, -0.0153],
        [ 0.0323,  0.0320, -0.0100, -0.0322,  0.0270],
        [ 0.0005,  0.0003, -0.0003, -0.0003,  0.0007],
        [-0.0249, -0.0239,  0.0085,  0.0240, -0.0213],
        [ 0.0040,  0.0039, -0.0018, -0.0038,  0.0033],
        [ 0.0075,  0.0071, -0.0024, -0.0073,  0.0066],
        [ 0.0110,  0.0107, -0.0033, -0.0109,  0.0094],
        [-0.0386, -0.0374,  0.0133,  0.0375, -0.0329],
        [ 0.0577,  0.0553, -0.0207, -0.0552,  0.0491],
        [ 0.0076,  0.0071, -0.0025, -0.0073,  0.0067]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.3616,  2.2845, -0.8113, -2.2920,  2.0112]], device='cuda:0')
--> [loss] 316.97528076171875

---------------------------------- [[#16 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     4.0      |     5.0     | 0.6209 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([704.,   3.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.3573394774475975 acc: 0.6173
[Epoch 7] loss: 3.2942145554458393 acc: 0.6549
[Epoch 11] loss: 1.7081879181477724 acc: 0.6726
[Epoch 15] loss: 0.733046876638175 acc: 0.6685
[Epoch 19] loss: 0.4288711005159656 acc: 0.6752
[Epoch 23] loss: 0.32758019406281774 acc: 0.6631
[Epoch 27] loss: 0.26640792289401033 acc: 0.654
[Epoch 31] loss: 0.24087832546304636 acc: 0.6617
[Epoch 35] loss: 0.2196142472603056 acc: 0.6689
[Epoch 39] loss: 0.19911248178299887 acc: 0.6645
[Epoch 43] loss: 0.1833466273974246 acc: 0.663
[Epoch 47] loss: 0.17249961322902338 acc: 0.6681
[Epoch 51] loss: 0.1590331652799569 acc: 0.6658
[Epoch 55] loss: 0.1542851312724335 acc: 0.6661
[Epoch 59] loss: 0.1368165554983727 acc: 0.6634
[Epoch 63] loss: 0.13557870359853139 acc: 0.6774
[Epoch 67] loss: 0.12658511489377264 acc: 0.6567
[Epoch 71] loss: 0.12959150778715645 acc: 0.6535
--> [test] acc: 0.6709
--> [accuracy] finished 0.6709
new state: tensor([704.,   3.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.6709
--> [reward] 0.6709
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     2.0     |     4.0      |     5.0     | 0.6709 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   3.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.598010227198491 acc: 0.5801
[Epoch 7] loss: 3.6236922678435244 acc: 0.6251
[Epoch 11] loss: 2.0438617040662814 acc: 0.6418
[Epoch 15] loss: 0.9486322287670181 acc: 0.6347
[Epoch 19] loss: 0.5481670261896632 acc: 0.6351
[Epoch 23] loss: 0.3788475704205501 acc: 0.6303
[Epoch 27] loss: 0.3223341652680465 acc: 0.6296
[Epoch 31] loss: 0.2776561291659694 acc: 0.6328
[Epoch 35] loss: 0.23827886941802243 acc: 0.6301
[Epoch 39] loss: 0.23008371441794173 acc: 0.6226
[Epoch 43] loss: 0.21624117218138997 acc: 0.6303
[Epoch 47] loss: 0.18907869112132417 acc: 0.6301
[Epoch 51] loss: 0.1873106956541481 acc: 0.624
[Epoch 55] loss: 0.17391526710261088 acc: 0.6303
[Epoch 59] loss: 0.16321835657129127 acc: 0.6257
[Epoch 63] loss: 0.144984197960047 acc: 0.6276
[Epoch 67] loss: 0.15342900092901704 acc: 0.6348
[Epoch 71] loss: 0.1304334486760866 acc: 0.6337
--> [test] acc: 0.6225
--> [accuracy] finished 0.6225
new state: tensor([704.,   3.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6225
--> [reward] 0.6225
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     2.0     |     4.0      |     6.0     | 0.6225 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([704.,   3.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.962520003013903 acc: 0.545
[Epoch 7] loss: 4.292900143986773 acc: 0.5998
[Epoch 11] loss: 2.9663110446289678 acc: 0.5975
[Epoch 15] loss: 1.701885126130965 acc: 0.5929
[Epoch 19] loss: 0.9143587536633472 acc: 0.5953
[Epoch 23] loss: 0.6048665901507868 acc: 0.5868
[Epoch 27] loss: 0.4797651166444087 acc: 0.5854
[Epoch 31] loss: 0.39680334856100097 acc: 0.5786
[Epoch 35] loss: 0.35152513779166256 acc: 0.5753
[Epoch 39] loss: 0.332519458659241 acc: 0.5796
[Epoch 43] loss: 0.3044602027391572 acc: 0.586
[Epoch 47] loss: 0.2751885856313588 acc: 0.5843
[Epoch 51] loss: 0.2620227539511707 acc: 0.5817
[Epoch 55] loss: 0.24708821247462803 acc: 0.579
[Epoch 59] loss: 0.23594866582737936 acc: 0.5881
[Epoch 63] loss: 0.23242218781005392 acc: 0.581
[Epoch 67] loss: 0.21381419394617837 acc: 0.5872
[Epoch 71] loss: 0.20630830484549598 acc: 0.5734
--> [test] acc: 0.5784
--> [accuracy] finished 0.5784
new state: tensor([704.,   3.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.5784
--> [reward] 0.5784
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     1.0     |     4.0      |     6.0     | 0.5784 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([704.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.7112229004540405 acc: 0.552
[Epoch 7] loss: 3.9529840211429255 acc: 0.621
[Epoch 11] loss: 2.4760489702758277 acc: 0.6199
[Epoch 15] loss: 1.2211347875730765 acc: 0.6096
[Epoch 19] loss: 0.6647989687312137 acc: 0.6011
[Epoch 23] loss: 0.46244025599602084 acc: 0.5988
[Epoch 27] loss: 0.3790226405262566 acc: 0.6005
[Epoch 31] loss: 0.3221059517501413 acc: 0.6057
[Epoch 35] loss: 0.2903351957321434 acc: 0.598
[Epoch 39] loss: 0.2678709752843394 acc: 0.5992
[Epoch 43] loss: 0.2502856960072351 acc: 0.5908
[Epoch 47] loss: 0.23647826273813652 acc: 0.5952
[Epoch 51] loss: 0.21619075366183924 acc: 0.6023
[Epoch 55] loss: 0.20557642242659235 acc: 0.603
[Epoch 59] loss: 0.19224988812309168 acc: 0.5911
[Epoch 63] loss: 0.17988529076318607 acc: 0.597
[Epoch 67] loss: 0.17494869405103614 acc: 0.6006
[Epoch 71] loss: 0.16794216898186584 acc: 0.6059
--> [test] acc: 0.5936
--> [accuracy] finished 0.5936
new state: tensor([704.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.5936
--> [reward] 0.5936
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.5936 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.725173335855879 acc: 0.5456
[Epoch 7] loss: 3.9548490706002317 acc: 0.6252
[Epoch 11] loss: 2.4456434503693103 acc: 0.6079
[Epoch 15] loss: 1.2031068294821188 acc: 0.6062
[Epoch 19] loss: 0.6642634572980501 acc: 0.6009
[Epoch 23] loss: 0.4617432677389487 acc: 0.5998
[Epoch 27] loss: 0.3674812716458117 acc: 0.6008
[Epoch 31] loss: 0.3160849528010849 acc: 0.5972
[Epoch 35] loss: 0.2860022800238541 acc: 0.6011
[Epoch 39] loss: 0.26136126904212453 acc: 0.5937
[Epoch 43] loss: 0.25012493720683066 acc: 0.6158
[Epoch 47] loss: 0.22678049573022158 acc: 0.5978
[Epoch 51] loss: 0.20465870245891002 acc: 0.5922
[Epoch 55] loss: 0.20990256781277755 acc: 0.5989
[Epoch 59] loss: 0.1858718491664342 acc: 0.5962
[Epoch 63] loss: 0.18595319202698557 acc: 0.5978
[Epoch 67] loss: 0.175081497799043 acc: 0.6002
[Epoch 71] loss: 0.167791060417357 acc: 0.6056
--> [test] acc: 0.5994
--> [accuracy] finished 0.5994
new state: tensor([736.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.5994
--> [reward] 0.5994
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.5994 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.479813860048114 acc: 0.5731
[Epoch 7] loss: 3.562146812448721 acc: 0.6257
[Epoch 11] loss: 2.0586479498678463 acc: 0.6414
[Epoch 15] loss: 0.96286801930012 acc: 0.6367
[Epoch 19] loss: 0.5413531612395249 acc: 0.6281
[Epoch 23] loss: 0.38782305507670584 acc: 0.6259
[Epoch 27] loss: 0.3173590034456052 acc: 0.6418
[Epoch 31] loss: 0.27847552898785344 acc: 0.6383
[Epoch 35] loss: 0.2548420398693789 acc: 0.6346
[Epoch 39] loss: 0.23870039297758466 acc: 0.6285
[Epoch 43] loss: 0.2116308794535525 acc: 0.6338
[Epoch 47] loss: 0.20071506737004918 acc: 0.6393
[Epoch 51] loss: 0.1934798112891786 acc: 0.6354
[Epoch 55] loss: 0.18175428700597618 acc: 0.6332
[Epoch 59] loss: 0.16816460444262046 acc: 0.6273
[Epoch 63] loss: 0.16310348620166634 acc: 0.6364
[Epoch 67] loss: 0.14772996443140385 acc: 0.6248
[Epoch 71] loss: 0.1471081011716629 acc: 0.6368
--> [test] acc: 0.6299
--> [accuracy] finished 0.6299
new state: tensor([736.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6299
--> [reward] 0.6299
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6299 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.46514851190245 acc: 0.5845
[Epoch 7] loss: 3.5543989999519896 acc: 0.6527
[Epoch 11] loss: 2.026335742505615 acc: 0.651
[Epoch 15] loss: 0.9262751598682855 acc: 0.6203
[Epoch 19] loss: 0.5175698552628422 acc: 0.622
[Epoch 23] loss: 0.3944350238413076 acc: 0.6379
[Epoch 27] loss: 0.3177828707058183 acc: 0.6246
[Epoch 31] loss: 0.2812624103496866 acc: 0.6344
[Epoch 35] loss: 0.24445402862556526 acc: 0.6349
[Epoch 39] loss: 0.23072192161236807 acc: 0.6373
[Epoch 43] loss: 0.21917708389356236 acc: 0.6403
[Epoch 47] loss: 0.19958194240908642 acc: 0.6289
[Epoch 51] loss: 0.1892811015457906 acc: 0.6372
[Epoch 55] loss: 0.17986669107113995 acc: 0.636
[Epoch 59] loss: 0.1711616140166226 acc: 0.6371
[Epoch 63] loss: 0.1710811264841529 acc: 0.6374
[Epoch 67] loss: 0.14572887702623522 acc: 0.6246
[Epoch 71] loss: 0.15777635901494672 acc: 0.6314
--> [test] acc: 0.6272
--> [accuracy] finished 0.6272
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6272
--> [reward] 0.6272
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6272 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.46032449427773 acc: 0.5994
[Epoch 7] loss: 3.543616344937888 acc: 0.6462
[Epoch 11] loss: 2.0364251315136395 acc: 0.6329
[Epoch 15] loss: 0.9494222253179916 acc: 0.6447
[Epoch 19] loss: 0.5258701510174805 acc: 0.6443
[Epoch 23] loss: 0.39276607511112527 acc: 0.6421
[Epoch 27] loss: 0.3165924002766571 acc: 0.6359
[Epoch 31] loss: 0.2783735810000154 acc: 0.6359
[Epoch 35] loss: 0.25282611453052983 acc: 0.6202
[Epoch 39] loss: 0.23885402955410198 acc: 0.6419
[Epoch 43] loss: 0.21575179507436654 acc: 0.6312
[Epoch 47] loss: 0.20513061083176787 acc: 0.6403
[Epoch 51] loss: 0.17698773286124345 acc: 0.6394
[Epoch 55] loss: 0.18565245469331818 acc: 0.6279
[Epoch 59] loss: 0.1675855629265194 acc: 0.6389
[Epoch 63] loss: 0.1602516612681129 acc: 0.6426
[Epoch 67] loss: 0.15298285934349995 acc: 0.6334
[Epoch 71] loss: 0.15122118187220315 acc: 0.6335
--> [test] acc: 0.6208
--> [accuracy] finished 0.6208
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6208
--> [reward] 0.6208
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6208 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.471780257914072 acc: 0.5921
[Epoch 7] loss: 3.575566103848655 acc: 0.6433
[Epoch 11] loss: 2.0432723748409534 acc: 0.6471
[Epoch 15] loss: 0.9399629215831342 acc: 0.6261
[Epoch 19] loss: 0.521088047286548 acc: 0.637
[Epoch 23] loss: 0.3838216893975159 acc: 0.631
[Epoch 27] loss: 0.32144572660136406 acc: 0.6308
[Epoch 31] loss: 0.2789658934301923 acc: 0.633
[Epoch 35] loss: 0.25654917457819826 acc: 0.6328
[Epoch 39] loss: 0.22449384961048585 acc: 0.6326
[Epoch 43] loss: 0.21686057564671463 acc: 0.6337
[Epoch 47] loss: 0.19505869242234056 acc: 0.6325
[Epoch 51] loss: 0.19350888415970993 acc: 0.6324
[Epoch 55] loss: 0.17788126509246008 acc: 0.6283
[Epoch 59] loss: 0.16637344509803825 acc: 0.6217
[Epoch 63] loss: 0.15795068834882106 acc: 0.6314
[Epoch 67] loss: 0.15042869299126174 acc: 0.6328
[Epoch 71] loss: 0.15060382823595572 acc: 0.63
--> [test] acc: 0.6257
--> [accuracy] finished 0.6257
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6257
--> [reward] 0.6257
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6257 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.450647639953877 acc: 0.5747
[Epoch 7] loss: 3.5566490815423637 acc: 0.6378
[Epoch 11] loss: 2.029958768116544 acc: 0.6551
[Epoch 15] loss: 0.9556910820362513 acc: 0.6383
[Epoch 19] loss: 0.5379869363144459 acc: 0.6375
[Epoch 23] loss: 0.38747492215603285 acc: 0.6291
[Epoch 27] loss: 0.3189835104605426 acc: 0.6224
[Epoch 31] loss: 0.2917367934947238 acc: 0.6253
[Epoch 35] loss: 0.2653797499125685 acc: 0.6365
[Epoch 39] loss: 0.2244795329292374 acc: 0.6234
[Epoch 43] loss: 0.218233129114408 acc: 0.6339
[Epoch 47] loss: 0.19858806299479187 acc: 0.6342
[Epoch 51] loss: 0.19467411116432504 acc: 0.6237
[Epoch 55] loss: 0.17854468456095518 acc: 0.6327
[Epoch 59] loss: 0.16368988762631098 acc: 0.6313
[Epoch 63] loss: 0.16806513265184964 acc: 0.6286
[Epoch 67] loss: 0.1529794698473914 acc: 0.6229
[Epoch 71] loss: 0.15723993760772417 acc: 0.6345
--> [test] acc: 0.6254
--> [accuracy] finished 0.6254
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6254
--> [reward] 0.6254
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6254 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   4.,   1.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.392834096308559 acc: 0.5876
[Epoch 7] loss: 3.4202598760957303 acc: 0.6577
[Epoch 11] loss: 1.916415669652812 acc: 0.6625
[Epoch 15] loss: 0.8523617803269182 acc: 0.6583
[Epoch 19] loss: 0.47371672441149154 acc: 0.6526
[Epoch 23] loss: 0.36166992908357964 acc: 0.6567
[Epoch 27] loss: 0.2965508055875597 acc: 0.6493
[Epoch 31] loss: 0.26395401944789815 acc: 0.6474
[Epoch 35] loss: 0.23852173604376023 acc: 0.6516
[Epoch 39] loss: 0.20515381947369374 acc: 0.6484
[Epoch 43] loss: 0.20753364734675572 acc: 0.6515
[Epoch 47] loss: 0.19619032524554703 acc: 0.6492
[Epoch 51] loss: 0.1804646184070088 acc: 0.652
[Epoch 55] loss: 0.16121726457382102 acc: 0.6515
[Epoch 59] loss: 0.16420154616146174 acc: 0.6509
[Epoch 63] loss: 0.15069872083361535 acc: 0.658
[Epoch 67] loss: 0.14234018008298505 acc: 0.6481
[Epoch 71] loss: 0.14751643786097274 acc: 0.6425
--> [test] acc: 0.6481
--> [accuracy] finished 0.6481
new state: tensor([768.,   4.,   1.,   4.,   4.], device='cuda:0')
new reward: 0.6481
--> [reward] 0.6481
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     4.0     | 0.6481 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   4.,   1.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.353552853512337 acc: 0.6085
[Epoch 7] loss: 3.420311924899021 acc: 0.6646
[Epoch 11] loss: 1.9242093568796392 acc: 0.6648
[Epoch 15] loss: 0.8672896334734719 acc: 0.6538
[Epoch 19] loss: 0.4850776881989463 acc: 0.6549
[Epoch 23] loss: 0.3513065779038593 acc: 0.6573
[Epoch 27] loss: 0.3121778609521706 acc: 0.6569
[Epoch 31] loss: 0.26479839434004043 acc: 0.6536
[Epoch 35] loss: 0.24524490176307043 acc: 0.6497
[Epoch 39] loss: 0.22679842881801182 acc: 0.6583
[Epoch 43] loss: 0.20190735276350202 acc: 0.6545
[Epoch 47] loss: 0.1941547674505645 acc: 0.648
[Epoch 51] loss: 0.17980692872201162 acc: 0.6495
[Epoch 55] loss: 0.17617075587801465 acc: 0.6491
[Epoch 59] loss: 0.16065556821136442 acc: 0.6515
[Epoch 63] loss: 0.15846297537426815 acc: 0.654
[Epoch 67] loss: 0.1470782001004995 acc: 0.6511
[Epoch 71] loss: 0.13990553874520065 acc: 0.6529
--> [test] acc: 0.6537
--> [accuracy] finished 0.6537
new state: tensor([768.,   4.,   1.,   4.,   4.], device='cuda:0')
new reward: 0.6537
--> [reward] 0.6537
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     4.0     | 0.6537 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([768.,   4.,   2.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.941674188579745 acc: 0.6588
[Epoch 7] loss: 2.66247292686149 acc: 0.709
[Epoch 11] loss: 1.07726444805141 acc: 0.676
[Epoch 15] loss: 0.4501531654563935 acc: 0.7072
[Epoch 19] loss: 0.2864851683182904 acc: 0.6838
[Epoch 23] loss: 0.24634913633317898 acc: 0.7067
[Epoch 27] loss: 0.20612415680995264 acc: 0.6906
[Epoch 31] loss: 0.18751298774110006 acc: 0.7006
[Epoch 35] loss: 0.1609224349098361 acc: 0.7093
[Epoch 39] loss: 0.14767686876199207 acc: 0.697
[Epoch 43] loss: 0.13875540600770422 acc: 0.709
[Epoch 47] loss: 0.12889365488282212 acc: 0.7005
[Epoch 51] loss: 0.12331555666380545 acc: 0.6976
[Epoch 55] loss: 0.12030770169610343 acc: 0.6982
[Epoch 59] loss: 0.10607808554822536 acc: 0.6933
[Epoch 63] loss: 0.10061456299597717 acc: 0.6988
[Epoch 67] loss: 0.09743169967605866 acc: 0.6901
[Epoch 71] loss: 0.09383537439877153 acc: 0.6942
--> [test] acc: 0.6941
--> [accuracy] finished 0.6941
new state: tensor([768.,   4.,   2.,   4.,   4.], device='cuda:0')
new reward: 0.6941
--> [reward] 0.6941
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     4.0      |     4.0     | 0.6941 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([768.,   4.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.075007339877546 acc: 0.6343
[Epoch 7] loss: 2.8794273631956875 acc: 0.6977
[Epoch 11] loss: 1.2137905807255784 acc: 0.6906
[Epoch 15] loss: 0.4914421271842421 acc: 0.6681
[Epoch 19] loss: 0.3342830740687106 acc: 0.6939
[Epoch 23] loss: 0.2605643902340775 acc: 0.6889
[Epoch 27] loss: 0.2148660842610328 acc: 0.6852
[Epoch 31] loss: 0.19395582856434157 acc: 0.6824
[Epoch 35] loss: 0.17689539559538026 acc: 0.6794
[Epoch 39] loss: 0.15751000182569752 acc: 0.6833
[Epoch 43] loss: 0.13825529367398576 acc: 0.6899
[Epoch 47] loss: 0.1467964063634348 acc: 0.6724
[Epoch 51] loss: 0.13220082724209675 acc: 0.6784
[Epoch 55] loss: 0.11881450832049693 acc: 0.681
[Epoch 59] loss: 0.11088636684555399 acc: 0.6668
[Epoch 63] loss: 0.1112714328498239 acc: 0.6809
[Epoch 67] loss: 0.10736428158562583 acc: 0.6779
[Epoch 71] loss: 0.09780813977294994 acc: 0.6822
--> [test] acc: 0.6821
--> [accuracy] finished 0.6821
new state: tensor([768.,   4.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.6821
--> [reward] 0.6821
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     4.0      |     5.0     | 0.6821 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0923, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3965, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.348913002501973 acc: 0.5932
[Epoch 7] loss: 3.27968327133247 acc: 0.6627
[Epoch 11] loss: 1.5631544402302684 acc: 0.6457
[Epoch 15] loss: 0.6612134215462467 acc: 0.6448
[Epoch 19] loss: 0.394064780241331 acc: 0.6383
[Epoch 23] loss: 0.31284479053733905 acc: 0.6462
[Epoch 27] loss: 0.2546648016804472 acc: 0.6393
[Epoch 31] loss: 0.21933285322974025 acc: 0.6448
[Epoch 35] loss: 0.20685115772897325 acc: 0.6428
[Epoch 39] loss: 0.19015331064229427 acc: 0.6475
[Epoch 43] loss: 0.16379385724511292 acc: 0.643
[Epoch 47] loss: 0.16247522388942195 acc: 0.6423
[Epoch 51] loss: 0.1447804138877803 acc: 0.6453
[Epoch 55] loss: 0.14307507543403017 acc: 0.6479
[Epoch 59] loss: 0.12501804565098565 acc: 0.6491
[Epoch 63] loss: 0.13644356810537828 acc: 0.6452
[Epoch 67] loss: 0.11926394835343142 acc: 0.651
[Epoch 71] loss: 0.11592171192788484 acc: 0.6343
--> [test] acc: 0.6334
--> [accuracy] finished 0.6334
new state: tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6334
--> [reward] 0.6334
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2001]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4002]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6326]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6936]], device='cuda:0')
------ ------
delta_t: tensor([[0.6326]], device='cuda:0')
rewards[i]: 0.6334
values[i+1]: tensor([[0.0608]], device='cuda:0')
values[i]: tensor([[0.0610]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6326]], device='cuda:0')
delta_t: tensor([[0.6326]], device='cuda:0')
------ ------
policy_loss: 1.4933544397354126
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6326]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0551]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.7100]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.3077]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3688]], device='cuda:0')
------ ------
delta_t: tensor([[0.6814]], device='cuda:0')
rewards[i]: 0.6821
values[i+1]: tensor([[0.0610]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0611]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.3077]], device='cuda:0')
delta_t: tensor([[0.6814]], device='cuda:0')
------ ------
policy_loss: 4.605754852294922
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.3077]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.0311]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.9519]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9880]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0492]], device='cuda:0')
------ ------
delta_t: tensor([[0.6934]], device='cuda:0')
rewards[i]: 0.6941
values[i+1]: tensor([[0.0611]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0612]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9880]], device='cuda:0')
delta_t: tensor([[0.6934]], device='cuda:0')
------ ------
policy_loss: 9.348393440246582
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9880]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.4668]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.8716]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.6214]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.6824]], device='cuda:0')
------ ------
delta_t: tensor([[0.6533]], device='cuda:0')
rewards[i]: 0.6537
values[i+1]: tensor([[0.0612]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0610]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.6214]], device='cuda:0')
delta_t: tensor([[0.6533]], device='cuda:0')
------ ------
policy_loss: 15.610010147094727
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.6214]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[11.7251]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.5166]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.2429]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3037]], device='cuda:0')
------ ------
delta_t: tensor([[0.6478]], device='cuda:0')
rewards[i]: 0.6481
values[i+1]: tensor([[0.0610]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0607]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.2429]], device='cuda:0')
delta_t: tensor([[0.6478]], device='cuda:0')
------ ------
policy_loss: 23.365097045898438
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.2429]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[19.0811]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.7119]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.8356]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.8960]], device='cuda:0')
------ ------
delta_t: tensor([[0.6251]], device='cuda:0')
rewards[i]: 0.6254
values[i+1]: tensor([[0.0607]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0604]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.8356]], device='cuda:0')
delta_t: tensor([[0.6251]], device='cuda:0')
------ ------
policy_loss: 32.539974212646484
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.8356]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[28.8615]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.5607]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.4227]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.4828]], device='cuda:0')
------ ------
delta_t: tensor([[0.6255]], device='cuda:0')
rewards[i]: 0.6257
values[i+1]: tensor([[0.0604]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0600]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.4227]], device='cuda:0')
delta_t: tensor([[0.6255]], device='cuda:0')
------ ------
policy_loss: 43.12100601196289
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.4227]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[41.3569]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.9908]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9991]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0587]], device='cuda:0')
------ ------
delta_t: tensor([[0.6206]], device='cuda:0')
rewards[i]: 0.6208
values[i+1]: tensor([[0.0600]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0597]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9991]], device='cuda:0')
delta_t: tensor([[0.6206]], device='cuda:0')
------ ------
policy_loss: 55.08399200439453
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9991]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[56.9041]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[31.0943]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.5762]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.6354]], device='cuda:0')
------ ------
delta_t: tensor([[0.6271]], device='cuda:0')
rewards[i]: 0.6272
values[i+1]: tensor([[0.0597]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0591]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.5762]], device='cuda:0')
delta_t: tensor([[0.6271]], device='cuda:0')
------ ------
policy_loss: 68.42341613769531
log_probs[i]: tensor([[-2.3965]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.5762]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[75.8159]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.8237]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1501]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2089]], device='cuda:0')
------ ------
delta_t: tensor([[0.6296]], device='cuda:0')
rewards[i]: 0.6299
values[i+1]: tensor([[0.0591]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0588]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1501]], device='cuda:0')
delta_t: tensor([[0.6296]], device='cuda:0')
------ ------
policy_loss: 83.1520767211914
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1501]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[98.1800]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[44.7282]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.6879]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7462]], device='cuda:0')
------ ------
delta_t: tensor([[0.5993]], device='cuda:0')
rewards[i]: 0.5994
values[i+1]: tensor([[0.0588]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0583]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.6879]], device='cuda:0')
delta_t: tensor([[0.5993]], device='cuda:0')
------ ------
policy_loss: 99.15570831298828
log_probs[i]: tensor([[-2.3965]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.6879]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[124.2056]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[52.0513]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2147]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2723]], device='cuda:0')
------ ------
delta_t: tensor([[0.5936]], device='cuda:0')
rewards[i]: 0.5936
values[i+1]: tensor([[0.0583]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0577]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2147]], device='cuda:0')
delta_t: tensor([[0.5936]], device='cuda:0')
------ ------
policy_loss: 116.43878936767578
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2147]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[154.0120]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[59.6128]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.7209]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7780]], device='cuda:0')
------ ------
delta_t: tensor([[0.5784]], device='cuda:0')
rewards[i]: 0.5784
values[i+1]: tensor([[0.0577]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0571]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.7209]], device='cuda:0')
delta_t: tensor([[0.5784]], device='cuda:0')
------ ------
policy_loss: 134.9254150390625
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.7209]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[188.1778]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[68.3315]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.2663]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.3227]], device='cuda:0')
------ ------
delta_t: tensor([[0.6226]], device='cuda:0')
rewards[i]: 0.6225
values[i+1]: tensor([[0.0571]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0565]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.2663]], device='cuda:0')
delta_t: tensor([[0.6226]], device='cuda:0')
------ ------
policy_loss: 154.72787475585938
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.2663]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[227.3842]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[78.4129]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.8551]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.9104]], device='cuda:0')
------ ------
delta_t: tensor([[0.6715]], device='cuda:0')
rewards[i]: 0.6709
values[i+1]: tensor([[0.0565]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0553]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.8551]], device='cuda:0')
delta_t: tensor([[0.6715]], device='cuda:0')
------ ------
policy_loss: 175.94619750976562
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.8551]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 175.94619750976562
value_loss: 227.38421630859375
loss: 289.6383056640625



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-4.8199e-03, -2.2273e-05, -1.2257e-05, -2.7125e-05, -3.5689e-05],
        [ 1.0316e-01,  4.7956e-04,  2.5819e-04,  5.7879e-04,  7.5822e-04],
        [-1.3341e-03, -6.1258e-06, -3.2918e-06, -7.4888e-06, -9.8667e-06],
        [ 4.4486e-01,  2.0698e-03,  1.0920e-03,  2.4898e-03,  3.2573e-03],
        [ 2.3368e+00,  1.0936e-02,  5.5971e-03,  1.3049e-02,  1.7091e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 4.3974e-05,  4.1566e-05, -1.4719e-05, -4.1910e-05,  3.6717e-05],
        [-9.4111e-04, -8.8888e-04,  3.1460e-04,  8.9656e-04, -7.8459e-04],
        [ 1.2198e-05,  1.1506e-05, -4.0704e-06, -1.1609e-05,  1.0171e-05],
        [-4.0623e-03, -3.8318e-03,  1.3554e-03,  3.8667e-03, -3.3821e-03],
        [-2.1366e-02, -2.0118e-02,  7.1198e-03,  2.0309e-02, -1.7764e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.6577, -0.6450,  0.6570, -0.5777,  0.3790],
        [-0.2025,  0.1985, -0.2022,  0.1777, -0.1163],
        [-0.3000,  0.2942, -0.2997,  0.2636, -0.1731],
        [-0.0120,  0.0118, -0.0120,  0.0105, -0.0069],
        [-0.1432,  0.1405, -0.1431,  0.1259, -0.0827]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0419, -0.0385,  0.0137,  0.0391, -0.0343],
        [ 0.0350,  0.0317, -0.0122, -0.0319,  0.0285],
        [-0.0419, -0.0385,  0.0137,  0.0391, -0.0342],
        [ 0.0536,  0.0505, -0.0180, -0.0508,  0.0458],
        [ 0.0047,  0.0046, -0.0020, -0.0043,  0.0046],
        [-0.0283, -0.0262,  0.0095,  0.0264, -0.0235],
        [-0.0416, -0.0383,  0.0136,  0.0388, -0.0340],
        [ 0.0376,  0.0340, -0.0113, -0.0352,  0.0292],
        [ 0.0184,  0.0164, -0.0059, -0.0168,  0.0142],
        [ 0.0206,  0.0192, -0.0067, -0.0195,  0.0175],
        [-0.0161, -0.0149,  0.0057,  0.0149, -0.0137]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.5252,  2.3204, -0.8244, -2.3561,  2.0647]], device='cuda:0')
--> [loss] 289.6383056640625

---------------------------------- [[#17 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6334 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.357162364303608 acc: 0.6005
[Epoch 7] loss: 3.2495570243776912 acc: 0.6578
[Epoch 11] loss: 1.5543865845788776 acc: 0.6499
[Epoch 15] loss: 0.6428120456435873 acc: 0.6517
[Epoch 19] loss: 0.396532373421866 acc: 0.6314
[Epoch 23] loss: 0.2933400805915713 acc: 0.6429
[Epoch 27] loss: 0.2655323888615841 acc: 0.6502
[Epoch 31] loss: 0.22371874605079212 acc: 0.6529
[Epoch 35] loss: 0.19484105595873427 acc: 0.6449
[Epoch 39] loss: 0.1951874824278914 acc: 0.6494
[Epoch 43] loss: 0.15647249458991278 acc: 0.6483
[Epoch 47] loss: 0.1590201881740366 acc: 0.646
[Epoch 51] loss: 0.1549879932690345 acc: 0.6433
[Epoch 55] loss: 0.13610396795022442 acc: 0.6391
[Epoch 59] loss: 0.1377010918948847 acc: 0.6427
[Epoch 63] loss: 0.128917267113743 acc: 0.6379
[Epoch 67] loss: 0.11518663277639948 acc: 0.6439
[Epoch 71] loss: 0.12436519739160891 acc: 0.6422
--> [test] acc: 0.6496
--> [accuracy] finished 0.6496
new state: tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6496
--> [reward] 0.6496
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6496 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.372621907297607 acc: 0.6113
[Epoch 7] loss: 3.279837813904828 acc: 0.6617
[Epoch 11] loss: 1.591309578922551 acc: 0.6393
[Epoch 15] loss: 0.6665160193172334 acc: 0.6447
[Epoch 19] loss: 0.3990291076857606 acc: 0.6604
[Epoch 23] loss: 0.3061389236584725 acc: 0.646
[Epoch 27] loss: 0.25439260005617464 acc: 0.6412
[Epoch 31] loss: 0.21752581089411094 acc: 0.6489
[Epoch 35] loss: 0.20536959436876923 acc: 0.6535
[Epoch 39] loss: 0.18564032692262125 acc: 0.6479
[Epoch 43] loss: 0.16788586523845944 acc: 0.6455
[Epoch 47] loss: 0.1620472612304856 acc: 0.6265
[Epoch 51] loss: 0.1506343182709897 acc: 0.6407
[Epoch 55] loss: 0.13984494495427097 acc: 0.6454
[Epoch 59] loss: 0.12568768470272984 acc: 0.6494
[Epoch 63] loss: 0.13248847830323668 acc: 0.638
[Epoch 67] loss: 0.11080895324209061 acc: 0.6501
[Epoch 71] loss: 0.11760091334057357 acc: 0.6352
--> [test] acc: 0.6385
--> [accuracy] finished 0.6385
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6385
--> [reward] 0.6385
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6385 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.328212504953984 acc: 0.613
[Epoch 7] loss: 3.2362092321028797 acc: 0.6726
[Epoch 11] loss: 1.5259233920089423 acc: 0.6557
[Epoch 15] loss: 0.6410880531267742 acc: 0.6435
[Epoch 19] loss: 0.3874900870196655 acc: 0.6526
[Epoch 23] loss: 0.2992401614432673 acc: 0.6531
[Epoch 27] loss: 0.2563435220805085 acc: 0.6417
[Epoch 31] loss: 0.21765742017446882 acc: 0.6426
[Epoch 35] loss: 0.19955443515730523 acc: 0.6503
[Epoch 39] loss: 0.17892647153266783 acc: 0.6419
[Epoch 43] loss: 0.1720896508530392 acc: 0.6429
[Epoch 47] loss: 0.16254886610331037 acc: 0.6474
[Epoch 51] loss: 0.14523343744752049 acc: 0.6417
[Epoch 55] loss: 0.14140785289297114 acc: 0.6414
[Epoch 59] loss: 0.1268924899909007 acc: 0.6402
[Epoch 63] loss: 0.1348497015650353 acc: 0.6464
[Epoch 67] loss: 0.11299586199584853 acc: 0.6411
[Epoch 71] loss: 0.11957438717908261 acc: 0.6375
--> [test] acc: 0.6494
--> [accuracy] finished 0.6494
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6494
--> [reward] 0.6494
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6494 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.357888542325296 acc: 0.6009
[Epoch 7] loss: 3.260644691679484 acc: 0.6394
[Epoch 11] loss: 1.5509599479644194 acc: 0.6618
[Epoch 15] loss: 0.6473774888440776 acc: 0.6506
[Epoch 19] loss: 0.38591640194415894 acc: 0.6496
[Epoch 23] loss: 0.297070744142527 acc: 0.652
[Epoch 27] loss: 0.25553713084848795 acc: 0.6437
[Epoch 31] loss: 0.22288622672352798 acc: 0.6448
[Epoch 35] loss: 0.19858600658452724 acc: 0.6499
[Epoch 39] loss: 0.1964742001479544 acc: 0.6436
[Epoch 43] loss: 0.16106077683065326 acc: 0.6466
[Epoch 47] loss: 0.16524865834847513 acc: 0.6466
[Epoch 51] loss: 0.14261116644920177 acc: 0.6481
[Epoch 55] loss: 0.13776616127613714 acc: 0.6392
[Epoch 59] loss: 0.13401663395673838 acc: 0.6483
[Epoch 63] loss: 0.1272935421513322 acc: 0.6476
[Epoch 67] loss: 0.1185082047022498 acc: 0.6499
[Epoch 71] loss: 0.1178589419008035 acc: 0.6446
--> [test] acc: 0.6436
--> [accuracy] finished 0.6436
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6436
--> [reward] 0.6436
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6436 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.405544623542015 acc: 0.5833
[Epoch 7] loss: 3.3006139550062703 acc: 0.6374
[Epoch 11] loss: 1.5861651926394313 acc: 0.641
[Epoch 15] loss: 0.6699053786237679 acc: 0.648
[Epoch 19] loss: 0.4130305460061106 acc: 0.6567
[Epoch 23] loss: 0.30247047058094645 acc: 0.6392
[Epoch 27] loss: 0.25309949337154664 acc: 0.643
[Epoch 31] loss: 0.23306881598032572 acc: 0.648
[Epoch 35] loss: 0.206871558897807 acc: 0.6471
[Epoch 39] loss: 0.1801401845374814 acc: 0.6346
[Epoch 43] loss: 0.1728873276520911 acc: 0.6433
[Epoch 47] loss: 0.15701223732403402 acc: 0.6422
[Epoch 51] loss: 0.1430091526063964 acc: 0.6431
[Epoch 55] loss: 0.14138749382063232 acc: 0.6465
[Epoch 59] loss: 0.13473069353643663 acc: 0.6453
[Epoch 63] loss: 0.12611782881776656 acc: 0.6375
[Epoch 67] loss: 0.12358413591249691 acc: 0.6427
[Epoch 71] loss: 0.10792609242692022 acc: 0.6406
--> [test] acc: 0.6293
--> [accuracy] finished 0.6293
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6293
--> [reward] 0.6293
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6293 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.725821725852654 acc: 0.5639
[Epoch 7] loss: 3.9432459496476158 acc: 0.6291
[Epoch 11] loss: 2.4113556275053707 acc: 0.6224
[Epoch 15] loss: 1.1699125231684322 acc: 0.6175
[Epoch 19] loss: 0.6287844194875806 acc: 0.6115
[Epoch 23] loss: 0.4529077640979949 acc: 0.6109
[Epoch 27] loss: 0.3585287647945878 acc: 0.6117
[Epoch 31] loss: 0.31693341406276615 acc: 0.6117
[Epoch 35] loss: 0.29051927954692136 acc: 0.6084
[Epoch 39] loss: 0.25430168697367544 acc: 0.5987
[Epoch 43] loss: 0.23835845517418575 acc: 0.5936
[Epoch 47] loss: 0.22616063600972944 acc: 0.5996
[Epoch 51] loss: 0.2099256766293093 acc: 0.6052
[Epoch 55] loss: 0.19795162233826527 acc: 0.6093
[Epoch 59] loss: 0.18708015748006684 acc: 0.6087
[Epoch 63] loss: 0.1661114792120369 acc: 0.6007
[Epoch 67] loss: 0.17994712873497773 acc: 0.5961
[Epoch 71] loss: 0.16009503748753798 acc: 0.5998
--> [test] acc: 0.6031
--> [accuracy] finished 0.6031
new state: tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.6031
--> [reward] 0.6031
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.6031 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([800.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.836028627117576 acc: 0.5492
[Epoch 7] loss: 4.043214777241582 acc: 0.6063
[Epoch 11] loss: 2.494175977490442 acc: 0.608
[Epoch 15] loss: 1.2211422805705339 acc: 0.5936
[Epoch 19] loss: 0.6766787369156737 acc: 0.5916
[Epoch 23] loss: 0.4659097723548522 acc: 0.5814
[Epoch 27] loss: 0.37872719836642826 acc: 0.5897
[Epoch 31] loss: 0.3360728483237421 acc: 0.6026
[Epoch 35] loss: 0.29435768171601817 acc: 0.5863
[Epoch 39] loss: 0.28304230404155484 acc: 0.5843
[Epoch 43] loss: 0.24636040616046895 acc: 0.5983
[Epoch 47] loss: 0.22827902844752115 acc: 0.5859
[Epoch 51] loss: 0.22427575011044512 acc: 0.5883
[Epoch 55] loss: 0.20680162128623183 acc: 0.5889
[Epoch 59] loss: 0.1961289317730595 acc: 0.5691
[Epoch 63] loss: 0.19398437476838412 acc: 0.593
[Epoch 67] loss: 0.1749586170608335 acc: 0.5817
[Epoch 71] loss: 0.17304370511094552 acc: 0.5876
--> [test] acc: 0.5845
--> [accuracy] finished 0.5845
new state: tensor([800.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.5845
--> [reward] 0.5845
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.5845 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.739544687063797 acc: 0.5432
[Epoch 7] loss: 3.9341424319445326 acc: 0.6103
[Epoch 11] loss: 2.444833429649358 acc: 0.6184
[Epoch 15] loss: 1.2098914204389237 acc: 0.6135
[Epoch 19] loss: 0.6552149081683677 acc: 0.6047
[Epoch 23] loss: 0.4587754591480088 acc: 0.6074
[Epoch 27] loss: 0.3734900539960055 acc: 0.6013
[Epoch 31] loss: 0.32618298696454073 acc: 0.5971
[Epoch 35] loss: 0.28970464289455156 acc: 0.6003
[Epoch 39] loss: 0.26729175875968564 acc: 0.6017
[Epoch 43] loss: 0.2429213976306493 acc: 0.5971
[Epoch 47] loss: 0.21138784268875713 acc: 0.6053
[Epoch 51] loss: 0.22536947112828207 acc: 0.6001
[Epoch 55] loss: 0.19508789862384615 acc: 0.6024
[Epoch 59] loss: 0.19008561024618575 acc: 0.5956
[Epoch 63] loss: 0.1852224980852545 acc: 0.5968
[Epoch 67] loss: 0.1620585565262324 acc: 0.5983
[Epoch 71] loss: 0.1685018406483009 acc: 0.5973
--> [test] acc: 0.5992
--> [accuracy] finished 0.5992
new state: tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.5992
--> [reward] 0.5992
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.5992 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.734400672985769 acc: 0.5462
[Epoch 7] loss: 3.921628636319924 acc: 0.6078
[Epoch 11] loss: 2.371659244684612 acc: 0.6185
[Epoch 15] loss: 1.1496624641520592 acc: 0.599
[Epoch 19] loss: 0.6165785565829415 acc: 0.6122
[Epoch 23] loss: 0.4419029626490362 acc: 0.6143
[Epoch 27] loss: 0.37067084109691706 acc: 0.6164
[Epoch 31] loss: 0.309478176055033 acc: 0.5947
[Epoch 35] loss: 0.2825937963798261 acc: 0.5923
[Epoch 39] loss: 0.25788252707332604 acc: 0.5993
[Epoch 43] loss: 0.23244281203421713 acc: 0.6065
[Epoch 47] loss: 0.22169939069139302 acc: 0.6041
[Epoch 51] loss: 0.20741964719446418 acc: 0.6071
[Epoch 55] loss: 0.20110108035490337 acc: 0.5982
[Epoch 59] loss: 0.19000172887064154 acc: 0.6019
[Epoch 63] loss: 0.177150060789531 acc: 0.5981
[Epoch 67] loss: 0.17270958884124932 acc: 0.6063
[Epoch 71] loss: 0.16156485824562286 acc: 0.6052
--> [test] acc: 0.5959
--> [accuracy] finished 0.5959
new state: tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.5959
--> [reward] 0.5959
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.5959 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.693638192723169 acc: 0.5681
[Epoch 7] loss: 3.8953518295836878 acc: 0.612
[Epoch 11] loss: 2.3926866882292512 acc: 0.6117
[Epoch 15] loss: 1.1696998826454363 acc: 0.6162
[Epoch 19] loss: 0.6377614074436676 acc: 0.618
[Epoch 23] loss: 0.4500153984236138 acc: 0.6182
[Epoch 27] loss: 0.36664279218039014 acc: 0.6039
[Epoch 31] loss: 0.3159414721352746 acc: 0.6137
[Epoch 35] loss: 0.28494813847248357 acc: 0.6016
[Epoch 39] loss: 0.26079171851201133 acc: 0.594
[Epoch 43] loss: 0.2505616532290912 acc: 0.6092
[Epoch 47] loss: 0.22094865175216552 acc: 0.6024
[Epoch 51] loss: 0.2204249184364286 acc: 0.6054
[Epoch 55] loss: 0.19647794331678803 acc: 0.5979
[Epoch 59] loss: 0.17871437331928355 acc: 0.6025
[Epoch 63] loss: 0.17697884734777158 acc: 0.6034
[Epoch 67] loss: 0.17461365815299704 acc: 0.6067
[Epoch 71] loss: 0.16520393612053808 acc: 0.6037
--> [test] acc: 0.6083
--> [accuracy] finished 0.6083
new state: tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.6083
--> [reward] 0.6083
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.6083 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.708275944985393 acc: 0.569
[Epoch 7] loss: 3.9362092917532565 acc: 0.6125
[Epoch 11] loss: 2.4180342537515305 acc: 0.6122
[Epoch 15] loss: 1.155063300600747 acc: 0.6027
[Epoch 19] loss: 0.6362461799379352 acc: 0.6038
[Epoch 23] loss: 0.43928836453991854 acc: 0.6012
[Epoch 27] loss: 0.3660808126430225 acc: 0.5956
[Epoch 31] loss: 0.31504031365303814 acc: 0.6039
[Epoch 35] loss: 0.2905862169945255 acc: 0.6001
[Epoch 39] loss: 0.2594245685183484 acc: 0.6063
[Epoch 43] loss: 0.23569511679832436 acc: 0.6035
[Epoch 47] loss: 0.22874680052146965 acc: 0.5989
[Epoch 51] loss: 0.21391426556317322 acc: 0.6038
[Epoch 55] loss: 0.19391147453395075 acc: 0.6031
[Epoch 59] loss: 0.18740210562820553 acc: 0.5959
[Epoch 63] loss: 0.18940205711757055 acc: 0.5985
[Epoch 67] loss: 0.16330985660972003 acc: 0.5962
[Epoch 71] loss: 0.1713148558112171 acc: 0.6008
--> [test] acc: 0.5992
--> [accuracy] finished 0.5992
new state: tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.5992
--> [reward] 0.5992
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.5992 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.713982173853823 acc: 0.5624
[Epoch 7] loss: 3.932634947245078 acc: 0.6176
[Epoch 11] loss: 2.48301114824117 acc: 0.6188
[Epoch 15] loss: 1.213779597121584 acc: 0.6069
[Epoch 19] loss: 0.6661171899903613 acc: 0.6087
[Epoch 23] loss: 0.44565208510155113 acc: 0.6068
[Epoch 27] loss: 0.37261636814583676 acc: 0.5987
[Epoch 31] loss: 0.32220881594025796 acc: 0.608
[Epoch 35] loss: 0.29173874017565754 acc: 0.6009
[Epoch 39] loss: 0.26148863179404336 acc: 0.6014
[Epoch 43] loss: 0.23509201463168997 acc: 0.6069
[Epoch 47] loss: 0.22575177851102085 acc: 0.6031
[Epoch 51] loss: 0.21288189733319957 acc: 0.6063
[Epoch 55] loss: 0.21356727030661787 acc: 0.6088
[Epoch 59] loss: 0.17722959778107264 acc: 0.6016
[Epoch 63] loss: 0.18878654370922834 acc: 0.6047
[Epoch 67] loss: 0.17092065313530852 acc: 0.6158
[Epoch 71] loss: 0.1598701150242306 acc: 0.6
--> [test] acc: 0.6054
--> [accuracy] finished 0.6054
new state: tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.6054
--> [reward] 0.6054
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.6054 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.34212269045203 acc: 0.6045
[Epoch 7] loss: 3.2794951788147393 acc: 0.6495
[Epoch 11] loss: 1.5574589997644315 acc: 0.6586
[Epoch 15] loss: 0.6512080250770006 acc: 0.6512
[Epoch 19] loss: 0.39012944075228917 acc: 0.6528
[Epoch 23] loss: 0.2944730720851961 acc: 0.6542
[Epoch 27] loss: 0.24876617116953634 acc: 0.6476
[Epoch 31] loss: 0.2208825474095238 acc: 0.6451
[Epoch 35] loss: 0.2054776289736104 acc: 0.6483
[Epoch 39] loss: 0.1834171074025733 acc: 0.652
[Epoch 43] loss: 0.16046351921754173 acc: 0.6532
[Epoch 47] loss: 0.1634879668440927 acc: 0.651
[Epoch 51] loss: 0.14107674775484955 acc: 0.6461
[Epoch 55] loss: 0.14456984485306626 acc: 0.6462
[Epoch 59] loss: 0.13169747072121943 acc: 0.6475
[Epoch 63] loss: 0.12913394763129657 acc: 0.6387
[Epoch 67] loss: 0.11544038408347751 acc: 0.6446
[Epoch 71] loss: 0.11266626923343838 acc: 0.6542
--> [test] acc: 0.6467
--> [accuracy] finished 0.6467
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6467
--> [reward] 0.6467
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6467 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.361006580197903 acc: 0.6058
[Epoch 7] loss: 3.261264818022623 acc: 0.6583
[Epoch 11] loss: 1.5661152889928245 acc: 0.6556
[Epoch 15] loss: 0.653682435107658 acc: 0.6422
[Epoch 19] loss: 0.39308586578501764 acc: 0.6579
[Epoch 23] loss: 0.2975131334794109 acc: 0.64
[Epoch 27] loss: 0.26179838861765153 acc: 0.6455
[Epoch 31] loss: 0.21410583914793513 acc: 0.648
[Epoch 35] loss: 0.21079105315277416 acc: 0.6381
[Epoch 39] loss: 0.1786661463601472 acc: 0.6355
[Epoch 43] loss: 0.16518509678025742 acc: 0.6465
[Epoch 47] loss: 0.16071548810481187 acc: 0.637
[Epoch 51] loss: 0.14850053007004643 acc: 0.6457
[Epoch 55] loss: 0.1419816245765561 acc: 0.6448
[Epoch 59] loss: 0.13623173609185402 acc: 0.6418
[Epoch 63] loss: 0.12770865967883097 acc: 0.6339
[Epoch 67] loss: 0.12124264501201946 acc: 0.647
[Epoch 71] loss: 0.11338071244777136 acc: 0.651
--> [test] acc: 0.6498
--> [accuracy] finished 0.6498
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6498
--> [reward] 0.6498
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6498 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0899, 0.0913, 0.0910, 0.0908, 0.0910, 0.0900,
         0.0903, 0.0905]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3989, -2.3975, -2.3978, -2.3980, -2.3978,
         -2.3988, -2.3985, -2.3983]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.3650576300023465 acc: 0.6055
[Epoch 7] loss: 3.2669411294752986 acc: 0.6482
[Epoch 11] loss: 1.5451080091202352 acc: 0.6548
[Epoch 15] loss: 0.6526822995137224 acc: 0.6431
[Epoch 19] loss: 0.3918218293544048 acc: 0.6507
[Epoch 23] loss: 0.29783236347091246 acc: 0.6325
[Epoch 27] loss: 0.2569370497413494 acc: 0.6501
[Epoch 31] loss: 0.2228290579946297 acc: 0.6441
[Epoch 35] loss: 0.20292244755122288 acc: 0.6486
[Epoch 39] loss: 0.18422674384835125 acc: 0.6477
[Epoch 43] loss: 0.16776902600170096 acc: 0.6521
[Epoch 47] loss: 0.1531347323070897 acc: 0.6485
[Epoch 51] loss: 0.14896929159622324 acc: 0.637
[Epoch 55] loss: 0.13811788212536547 acc: 0.6413
[Epoch 59] loss: 0.1337133153899075 acc: 0.6425
[Epoch 63] loss: 0.12415932482846863 acc: 0.6407
[Epoch 67] loss: 0.11980694916356555 acc: 0.6441
[Epoch 71] loss: 0.11910541801828929 acc: 0.6425
--> [test] acc: 0.6342
--> [accuracy] finished 0.6342
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6342
--> [reward] 0.6342
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2006]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4012]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6334]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7007]], device='cuda:0')
------ ------
delta_t: tensor([[0.6334]], device='cuda:0')
rewards[i]: 0.6342
values[i+1]: tensor([[0.0672]], device='cuda:0')
values[i]: tensor([[0.0673]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6334]], device='cuda:0')
delta_t: tensor([[0.6334]], device='cuda:0')
------ ------
policy_loss: 1.494774341583252
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6334]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0147]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6283]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2760]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3435]], device='cuda:0')
------ ------
delta_t: tensor([[0.6490]], device='cuda:0')
rewards[i]: 0.6498
values[i+1]: tensor([[0.0673]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0675]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2760]], device='cuda:0')
delta_t: tensor([[0.6490]], device='cuda:0')
------ ------
policy_loss: 4.530416965484619
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2760]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.8370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.6445]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9091]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9768]], device='cuda:0')
------ ------
delta_t: tensor([[0.6458]], device='cuda:0')
rewards[i]: 0.6467
values[i+1]: tensor([[0.0675]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0677]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9091]], device='cuda:0')
delta_t: tensor([[0.6458]], device='cuda:0')
------ ------
policy_loss: 9.083892822265625
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9091]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.9488]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.2236]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.4947]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5624]], device='cuda:0')
------ ------
delta_t: tensor([[0.6048]], device='cuda:0')
rewards[i]: 0.6054
values[i+1]: tensor([[0.0677]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0677]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.4947]], device='cuda:0')
delta_t: tensor([[0.6048]], device='cuda:0')
------ ------
policy_loss: 15.040908813476562
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.4947]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[10.6562]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.4148]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.0684]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1360]], device='cuda:0')
------ ------
delta_t: tensor([[0.5986]], device='cuda:0')
rewards[i]: 0.5992
values[i+1]: tensor([[0.0677]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0676]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.0684]], device='cuda:0')
delta_t: tensor([[0.5986]], device='cuda:0')
------ ------
policy_loss: 22.373188018798828
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.0684]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[17.3006]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.2889]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6454]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7129]], device='cuda:0')
------ ------
delta_t: tensor([[0.6077]], device='cuda:0')
rewards[i]: 0.6083
values[i+1]: tensor([[0.0676]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0675]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6454]], device='cuda:0')
delta_t: tensor([[0.6077]], device='cuda:0')
------ ------
policy_loss: 31.090065002441406
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6454]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[26.1387]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.6763]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2043]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2717]], device='cuda:0')
------ ------
delta_t: tensor([[0.5954]], device='cuda:0')
rewards[i]: 0.5959
values[i+1]: tensor([[0.0675]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0674]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2043]], device='cuda:0')
delta_t: tensor([[0.5954]], device='cuda:0')
------ ------
policy_loss: 41.149444580078125
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2043]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.4723]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.6672]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7610]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8282]], device='cuda:0')
------ ------
delta_t: tensor([[0.5987]], device='cuda:0')
rewards[i]: 0.5992
values[i+1]: tensor([[0.0674]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0672]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7610]], device='cuda:0')
delta_t: tensor([[0.5987]], device='cuda:0')
------ ------
policy_loss: 52.54133224487305
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7610]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[51.5043]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[28.0640]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2975]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3644]], device='cuda:0')
------ ------
delta_t: tensor([[0.5841]], device='cuda:0')
rewards[i]: 0.5845
values[i+1]: tensor([[0.0672]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0669]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2975]], device='cuda:0')
delta_t: tensor([[0.5841]], device='cuda:0')
------ ------
policy_loss: 65.22090148925781
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2975]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[68.6008]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.1929]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8475]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.9139]], device='cuda:0')
------ ------
delta_t: tensor([[0.6029]], device='cuda:0')
rewards[i]: 0.6031
values[i+1]: tensor([[0.0669]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0664]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8475]], device='cuda:0')
delta_t: tensor([[0.6029]], device='cuda:0')
------ ------
policy_loss: 79.21600341796875
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8475]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[89.1957]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[41.1898]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.4179]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4840]], device='cuda:0')
------ ------
delta_t: tensor([[0.6289]], device='cuda:0')
rewards[i]: 0.6293
values[i+1]: tensor([[0.0664]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0661]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.4179]], device='cuda:0')
delta_t: tensor([[0.6289]], device='cuda:0')
------ ------
policy_loss: 94.5853042602539
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.4179]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[113.6751]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[48.9588]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9971]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.0628]], device='cuda:0')
------ ------
delta_t: tensor([[0.6433]], device='cuda:0')
rewards[i]: 0.6436
values[i+1]: tensor([[0.0661]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0657]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9971]], device='cuda:0')
delta_t: tensor([[0.6433]], device='cuda:0')
------ ------
policy_loss: 111.34363555908203
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9971]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[142.3746]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[57.3991]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.5762]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.6416]], device='cuda:0')
------ ------
delta_t: tensor([[0.6491]], device='cuda:0')
rewards[i]: 0.6494
values[i+1]: tensor([[0.0657]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0653]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.5762]], device='cuda:0')
delta_t: tensor([[0.6491]], device='cuda:0')
------ ------
policy_loss: 129.4857940673828
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.5762]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[175.4967]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[66.2441]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.1390]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.2036]], device='cuda:0')
------ ------
delta_t: tensor([[0.6386]], device='cuda:0')
rewards[i]: 0.6385
values[i+1]: tensor([[0.0653]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0646]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.1390]], device='cuda:0')
delta_t: tensor([[0.6386]], device='cuda:0')
------ ------
policy_loss: 148.96656799316406
log_probs[i]: tensor([[-2.3964]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.1390]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[213.4061]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[75.8188]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.7074]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7712]], device='cuda:0')
------ ------
delta_t: tensor([[0.6497]], device='cuda:0')
rewards[i]: 0.6496
values[i+1]: tensor([[0.0646]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0638]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.7074]], device='cuda:0')
delta_t: tensor([[0.6497]], device='cuda:0')
------ ------
policy_loss: 169.82711791992188
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.7074]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 169.82711791992188
value_loss: 213.40609741210938
loss: 276.5301513671875



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-3.2199e-03, -1.6473e-05, -6.7318e-06, -1.6330e-05, -2.2562e-05],
        [ 1.0017e-01,  5.1201e-04,  2.0726e-04,  5.0713e-04,  7.0021e-04],
        [-9.3876e-04, -4.7966e-06, -1.9872e-06, -4.7528e-06, -6.6421e-06],
        [ 5.3024e-01,  2.7072e-03,  1.1001e-03,  2.6798e-03,  3.7259e-03],
        [ 3.0308e+00,  1.5461e-02,  6.2957e-03,  1.5297e-02,  2.1390e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 3.1175e-05,  2.7698e-05, -9.7832e-06, -2.8542e-05,  2.4503e-05],
        [-9.7002e-04, -8.6135e-04,  3.0438e-04,  8.8763e-04, -7.6209e-04],
        [ 9.0961e-06,  8.0705e-06, -2.8555e-06, -8.3176e-06,  7.1442e-06],
        [-5.1371e-03, -4.5570e-03,  1.6125e-03,  4.6963e-03, -4.0339e-03],
        [-2.9376e-02, -2.6038e-02,  9.2230e-03,  2.6836e-02, -2.3058e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.8633, -0.8521,  0.8627, -0.7792,  0.5394],
        [-0.2801,  0.2765, -0.2799,  0.2527, -0.1748],
        [-0.3924,  0.3874, -0.3922,  0.3542, -0.2453],
        [-0.0171,  0.0169, -0.0171,  0.0154, -0.0107],
        [-0.1736,  0.1714, -0.1735,  0.1567, -0.1086]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0460, -0.0402,  0.0145,  0.0414, -0.0358],
        [ 0.0101,  0.0091, -0.0035, -0.0091,  0.0084],
        [-0.0460, -0.0401,  0.0145,  0.0414, -0.0358],
        [-0.0452, -0.0395,  0.0143,  0.0407, -0.0352],
        [ 0.0368,  0.0316, -0.0115, -0.0329,  0.0282],
        [-0.0180, -0.0160,  0.0057,  0.0164, -0.0143],
        [-0.0076, -0.0067,  0.0024,  0.0069, -0.0061],
        [ 0.0684,  0.0594, -0.0214, -0.0613,  0.0530],
        [-0.0453, -0.0395,  0.0143,  0.0407, -0.0352],
        [ 0.1079,  0.0953, -0.0341, -0.0979,  0.0849],
        [-0.0151, -0.0134,  0.0049,  0.0137, -0.0120]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.7671,  2.4145, -0.8731, -2.4913,  2.1547]], device='cuda:0')
--> [loss] 276.5301513671875

---------------------------------- [[#18 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6342 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.364765445137268 acc: 0.6082
[Epoch 7] loss: 3.254607778909566 acc: 0.6551
[Epoch 11] loss: 1.5530124404622465 acc: 0.6447
[Epoch 15] loss: 0.6615665352062496 acc: 0.6455
[Epoch 19] loss: 0.39686246008119164 acc: 0.6445
[Epoch 23] loss: 0.30330272091081 acc: 0.6463
[Epoch 27] loss: 0.2632541382766288 acc: 0.6523
[Epoch 31] loss: 0.21870470866608574 acc: 0.6493
[Epoch 35] loss: 0.19542897885898725 acc: 0.6437
[Epoch 39] loss: 0.18268418000043962 acc: 0.6428
[Epoch 43] loss: 0.1786389233939864 acc: 0.6515
[Epoch 47] loss: 0.15315190178182575 acc: 0.6459
[Epoch 51] loss: 0.14386676594167186 acc: 0.6529
[Epoch 55] loss: 0.15048572138342484 acc: 0.6256
[Epoch 59] loss: 0.13354872573343346 acc: 0.6374
[Epoch 63] loss: 0.12347682121345568 acc: 0.6538
[Epoch 67] loss: 0.12794725509072222 acc: 0.6405
[Epoch 71] loss: 0.11433277467040635 acc: 0.6512
--> [test] acc: 0.6367
--> [accuracy] finished 0.6367
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6367
--> [reward] 0.6367
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6367 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.317117971227602 acc: 0.5899
[Epoch 7] loss: 3.232990748513385 acc: 0.6664
[Epoch 11] loss: 1.5061937223386277 acc: 0.6583
[Epoch 15] loss: 0.6350058777700833 acc: 0.6513
[Epoch 19] loss: 0.3759378637699291 acc: 0.6504
[Epoch 23] loss: 0.30062078707911016 acc: 0.6474
[Epoch 27] loss: 0.2451326775976726 acc: 0.6515
[Epoch 31] loss: 0.23561054995269193 acc: 0.6456
[Epoch 35] loss: 0.2039819250616919 acc: 0.6459
[Epoch 39] loss: 0.18321214360244514 acc: 0.6339
[Epoch 43] loss: 0.15834928857153066 acc: 0.6332
[Epoch 47] loss: 0.16583846221723214 acc: 0.6493
[Epoch 51] loss: 0.15277081641995008 acc: 0.6401
[Epoch 55] loss: 0.14303449543235858 acc: 0.6406
[Epoch 59] loss: 0.11934112244800134 acc: 0.6382
[Epoch 63] loss: 0.13303981199050727 acc: 0.6389
[Epoch 67] loss: 0.11949663395252641 acc: 0.6306
[Epoch 71] loss: 0.11109238938378442 acc: 0.6395
--> [test] acc: 0.6494
--> [accuracy] finished 0.6494
new state: tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6494
--> [reward] 0.6494
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6494 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.330801417303207 acc: 0.5892
[Epoch 7] loss: 3.2297546023602988 acc: 0.6624
[Epoch 11] loss: 1.5336629387057956 acc: 0.6382
[Epoch 15] loss: 0.6539387203648191 acc: 0.6501
[Epoch 19] loss: 0.38280766663115345 acc: 0.6405
[Epoch 23] loss: 0.3065491199536282 acc: 0.644
[Epoch 27] loss: 0.24569266359262226 acc: 0.6398
[Epoch 31] loss: 0.22215138122086864 acc: 0.6426
[Epoch 35] loss: 0.20165315024850086 acc: 0.6454
[Epoch 39] loss: 0.18220651321008305 acc: 0.6509
[Epoch 43] loss: 0.1787809827722266 acc: 0.6495
[Epoch 47] loss: 0.1589428851931163 acc: 0.644
[Epoch 51] loss: 0.15041786994751724 acc: 0.6449
[Epoch 55] loss: 0.12875595320816463 acc: 0.6396
[Epoch 59] loss: 0.1409701965201427 acc: 0.6482
[Epoch 63] loss: 0.11586272747844191 acc: 0.6443
[Epoch 67] loss: 0.13167101948324333 acc: 0.6403
[Epoch 71] loss: 0.1143973135875295 acc: 0.6379
--> [test] acc: 0.6471
--> [accuracy] finished 0.6471
new state: tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.6471
--> [reward] 0.6471
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.6471 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.317020408180364 acc: 0.6177
[Epoch 7] loss: 3.240883177198717 acc: 0.6692
[Epoch 11] loss: 1.5495950921493418 acc: 0.656
[Epoch 15] loss: 0.6600379323220009 acc: 0.6476
[Epoch 19] loss: 0.39861660279914773 acc: 0.6518
[Epoch 23] loss: 0.31003933997296007 acc: 0.6509
[Epoch 27] loss: 0.2595814676631404 acc: 0.6406
[Epoch 31] loss: 0.23367338945322177 acc: 0.6354
[Epoch 35] loss: 0.20525076799571057 acc: 0.6465
[Epoch 39] loss: 0.18929195229459525 acc: 0.6327
[Epoch 43] loss: 0.17229027890116738 acc: 0.6496
[Epoch 47] loss: 0.15863890008634085 acc: 0.6352
[Epoch 51] loss: 0.15938744652073097 acc: 0.6438
[Epoch 55] loss: 0.13734428933047502 acc: 0.6387
[Epoch 59] loss: 0.13919411315237318 acc: 0.6476
[Epoch 63] loss: 0.12826806370197508 acc: 0.643
[Epoch 67] loss: 0.1260959713867463 acc: 0.6388
[Epoch 71] loss: 0.11818291256061333 acc: 0.6471
--> [test] acc: 0.634
--> [accuracy] finished 0.634
new state: tensor([736.,   4.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.634
--> [reward] 0.634
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     4.0      |     6.0     | 0.634  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.737048035692376 acc: 0.5541
[Epoch 7] loss: 3.9825467224925983 acc: 0.6144
[Epoch 11] loss: 2.4951582257552527 acc: 0.6066
[Epoch 15] loss: 1.2528027619432915 acc: 0.6111
[Epoch 19] loss: 0.6635608363544088 acc: 0.6061
[Epoch 23] loss: 0.45822731046306203 acc: 0.6102
[Epoch 27] loss: 0.3748195119907179 acc: 0.6013
[Epoch 31] loss: 0.3327082352893775 acc: 0.5964
[Epoch 35] loss: 0.28386107108572406 acc: 0.6021
[Epoch 39] loss: 0.26605714115140305 acc: 0.594
[Epoch 43] loss: 0.24129364662625066 acc: 0.5976
[Epoch 47] loss: 0.22434856340674983 acc: 0.5985
[Epoch 51] loss: 0.21700049449911202 acc: 0.5963
[Epoch 55] loss: 0.20406502149427486 acc: 0.5977
[Epoch 59] loss: 0.19542441447086803 acc: 0.6028
[Epoch 63] loss: 0.17895774254241906 acc: 0.5999
[Epoch 67] loss: 0.17729581123851526 acc: 0.6027
[Epoch 71] loss: 0.16877345209155242 acc: 0.6053
--> [test] acc: 0.6016
--> [accuracy] finished 0.6016
new state: tensor([736.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.6016
--> [reward] 0.6016
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.6016 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3988, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.746149824708319 acc: 0.5542
[Epoch 7] loss: 3.9675049115629757 acc: 0.6146
[Epoch 11] loss: 2.4736599526594363 acc: 0.6172
[Epoch 15] loss: 1.2168664005787477 acc: 0.5931
[Epoch 19] loss: 0.6750480324372916 acc: 0.6065
[Epoch 23] loss: 0.4457998201108116 acc: 0.6046
[Epoch 27] loss: 0.3825237313714212 acc: 0.5905
[Epoch 31] loss: 0.32240155540035126 acc: 0.6058
[Epoch 35] loss: 0.2882233201299825 acc: 0.6008
[Epoch 39] loss: 0.2681740682472087 acc: 0.6007
[Epoch 43] loss: 0.2431916014274673 acc: 0.604
[Epoch 47] loss: 0.22143346294665428 acc: 0.604
[Epoch 51] loss: 0.2258678844896481 acc: 0.5985
[Epoch 55] loss: 0.19523563512179362 acc: 0.6011
[Epoch 59] loss: 0.2002057295073481 acc: 0.5966
[Epoch 63] loss: 0.1758663133870515 acc: 0.5981
[Epoch 67] loss: 0.18247946827312755 acc: 0.6022
[Epoch 71] loss: 0.1590521422874592 acc: 0.6047
--> [test] acc: 0.6016
--> [accuracy] finished 0.6016
new state: tensor([704.,   4.,   1.,   4.,   6.], device='cuda:0')
new reward: 0.6016
--> [reward] 0.6016
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     1.0     |     4.0      |     6.0     | 0.6016 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3988, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   1.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.829409445490678 acc: 0.5438
[Epoch 7] loss: 4.08900158332132 acc: 0.5966
[Epoch 11] loss: 2.6019507324909004 acc: 0.603
[Epoch 15] loss: 1.3133957360292334 acc: 0.5875
[Epoch 19] loss: 0.7193822413206558 acc: 0.5958
[Epoch 23] loss: 0.4951442516530338 acc: 0.5947
[Epoch 27] loss: 0.39589461894310496 acc: 0.589
[Epoch 31] loss: 0.33714338667843197 acc: 0.5861
[Epoch 35] loss: 0.3047216032962779 acc: 0.5991
[Epoch 39] loss: 0.28317929822189347 acc: 0.5799
[Epoch 43] loss: 0.25754454870567756 acc: 0.5868
[Epoch 47] loss: 0.23269823947183008 acc: 0.5908
[Epoch 51] loss: 0.225501737547824 acc: 0.5903
[Epoch 55] loss: 0.21511377604997448 acc: 0.5863
[Epoch 59] loss: 0.20462651416907074 acc: 0.5934
[Epoch 63] loss: 0.18698028307420003 acc: 0.5918
[Epoch 67] loss: 0.19224558291894853 acc: 0.5876
[Epoch 71] loss: 0.1640948101930568 acc: 0.5934
--> [test] acc: 0.5955
--> [accuracy] finished 0.5955
new state: tensor([704.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.5955
--> [reward] 0.5955
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.5955 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3988, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.846214012721616 acc: 0.5455
[Epoch 7] loss: 4.057988966669877 acc: 0.6095
[Epoch 11] loss: 2.527384788712577 acc: 0.6131
[Epoch 15] loss: 1.2583180974282877 acc: 0.5945
[Epoch 19] loss: 0.6726732566914595 acc: 0.594
[Epoch 23] loss: 0.49283674303108777 acc: 0.589
[Epoch 27] loss: 0.38599727535739425 acc: 0.5905
[Epoch 31] loss: 0.3480792236716851 acc: 0.5986
[Epoch 35] loss: 0.2910301676520225 acc: 0.5892
[Epoch 39] loss: 0.2778289743539546 acc: 0.598
[Epoch 43] loss: 0.2554715016089818 acc: 0.5899
[Epoch 47] loss: 0.237591620615643 acc: 0.5954
[Epoch 51] loss: 0.2161510338782883 acc: 0.5962
[Epoch 55] loss: 0.21661321615652585 acc: 0.5864
[Epoch 59] loss: 0.1913981541188534 acc: 0.5917
[Epoch 63] loss: 0.19748637641963485 acc: 0.5942
[Epoch 67] loss: 0.17257435824198034 acc: 0.5854
[Epoch 71] loss: 0.1812081600472693 acc: 0.5964
--> [test] acc: 0.584
--> [accuracy] finished 0.584
new state: tensor([736.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.584
--> [reward] 0.584
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.584  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3988, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.816661990207175 acc: 0.5592
[Epoch 7] loss: 4.029738816611298 acc: 0.6
[Epoch 11] loss: 2.5126095272391042 acc: 0.6034
[Epoch 15] loss: 1.238067319059311 acc: 0.6058
[Epoch 19] loss: 0.6642946163549677 acc: 0.5976
[Epoch 23] loss: 0.47478793626722626 acc: 0.5967
[Epoch 27] loss: 0.3810890359503915 acc: 0.5919
[Epoch 31] loss: 0.3329573453754148 acc: 0.5901
[Epoch 35] loss: 0.29636178628477217 acc: 0.59
[Epoch 39] loss: 0.27540687962299415 acc: 0.5865
[Epoch 43] loss: 0.2417174436537849 acc: 0.5894
[Epoch 47] loss: 0.23786812571479993 acc: 0.5862
[Epoch 51] loss: 0.21283276271327492 acc: 0.5999
[Epoch 55] loss: 0.19782513964092335 acc: 0.5891
[Epoch 59] loss: 0.212138168368479 acc: 0.5923
[Epoch 63] loss: 0.18216427037125582 acc: 0.5953
[Epoch 67] loss: 0.1807209073858874 acc: 0.5889
[Epoch 71] loss: 0.1709507801510451 acc: 0.5922
--> [test] acc: 0.581
--> [accuracy] finished 0.581
new state: tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.581
--> [reward] 0.581
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.581  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3988, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.83009342586293 acc: 0.5483
[Epoch 7] loss: 4.075477419454423 acc: 0.5981
[Epoch 11] loss: 2.5648606033123973 acc: 0.6029
[Epoch 15] loss: 1.2942222661488807 acc: 0.5779
[Epoch 19] loss: 0.7041661273973906 acc: 0.5984
[Epoch 23] loss: 0.4753783543158294 acc: 0.5912
[Epoch 27] loss: 0.3890710197427236 acc: 0.5927
[Epoch 31] loss: 0.3446939543289754 acc: 0.5875
[Epoch 35] loss: 0.2941973159003936 acc: 0.5995
[Epoch 39] loss: 0.2735600062143391 acc: 0.5981
[Epoch 43] loss: 0.24947337425597335 acc: 0.5948
[Epoch 47] loss: 0.2332380360606915 acc: 0.5921
[Epoch 51] loss: 0.21972871820925904 acc: 0.5976
[Epoch 55] loss: 0.2111843148284518 acc: 0.5962
[Epoch 59] loss: 0.19766028909086514 acc: 0.5905
[Epoch 63] loss: 0.18156960707805728 acc: 0.5991
[Epoch 67] loss: 0.1850278470200746 acc: 0.5927
[Epoch 71] loss: 0.1739632707908559 acc: 0.5912
--> [test] acc: 0.5951
--> [accuracy] finished 0.5951
new state: tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.5951
--> [reward] 0.5951
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.5951 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   4.,   1.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.569980407919725 acc: 0.5761
[Epoch 7] loss: 3.6864074807032905 acc: 0.6352
[Epoch 11] loss: 2.120873296657182 acc: 0.6261
[Epoch 15] loss: 0.9774992703972265 acc: 0.6255
[Epoch 19] loss: 0.5339060298850774 acc: 0.6286
[Epoch 23] loss: 0.3914456593368174 acc: 0.6204
[Epoch 27] loss: 0.33035721834939535 acc: 0.6254
[Epoch 31] loss: 0.2838274919763779 acc: 0.6217
[Epoch 35] loss: 0.25847990636992485 acc: 0.6258
[Epoch 39] loss: 0.23974956304925826 acc: 0.6232
[Epoch 43] loss: 0.2198166952818594 acc: 0.6284
[Epoch 47] loss: 0.20028341131623062 acc: 0.6229
[Epoch 51] loss: 0.1839126924843625 acc: 0.6225
[Epoch 55] loss: 0.1906429969055383 acc: 0.6199
[Epoch 59] loss: 0.1705724597822808 acc: 0.622
[Epoch 63] loss: 0.16701679673198316 acc: 0.6172
[Epoch 67] loss: 0.16232123138988033 acc: 0.6257
[Epoch 71] loss: 0.1477347482210192 acc: 0.6092
--> [test] acc: 0.6149
--> [accuracy] finished 0.6149
new state: tensor([768.,   4.,   1.,   3.,   5.], device='cuda:0')
new reward: 0.6149
--> [reward] 0.6149
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     3.0      |     5.0     | 0.6149 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0900,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.483259525292975 acc: 0.5863
[Epoch 7] loss: 3.591645340785346 acc: 0.6224
[Epoch 11] loss: 2.072288296678487 acc: 0.6465
[Epoch 15] loss: 0.9361762217319834 acc: 0.645
[Epoch 19] loss: 0.515866013348598 acc: 0.643
[Epoch 23] loss: 0.3796617070598828 acc: 0.6359
[Epoch 27] loss: 0.31842446028519317 acc: 0.6396
[Epoch 31] loss: 0.2823326876553733 acc: 0.6248
[Epoch 35] loss: 0.2415573876184385 acc: 0.6351
[Epoch 39] loss: 0.23617526879081566 acc: 0.6252
[Epoch 43] loss: 0.21840272960550797 acc: 0.6334
[Epoch 47] loss: 0.19746981292029323 acc: 0.6322
[Epoch 51] loss: 0.20172012989621257 acc: 0.6317
[Epoch 55] loss: 0.1713791413142172 acc: 0.6346
[Epoch 59] loss: 0.16892393937458278 acc: 0.6313
[Epoch 63] loss: 0.15786300848007126 acc: 0.6256
[Epoch 67] loss: 0.16079146647825837 acc: 0.6317
[Epoch 71] loss: 0.15296408670771 acc: 0.6415
--> [test] acc: 0.6448
--> [accuracy] finished 0.6448
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6448
--> [reward] 0.6448
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6448 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.499725797292217 acc: 0.5681
[Epoch 7] loss: 3.600448549632221 acc: 0.6356
[Epoch 11] loss: 2.0496253317121957 acc: 0.6508
[Epoch 15] loss: 0.950002408431619 acc: 0.6454
[Epoch 19] loss: 0.5067529580162843 acc: 0.6383
[Epoch 23] loss: 0.37662833713023636 acc: 0.6406
[Epoch 27] loss: 0.3286860844982631 acc: 0.6369
[Epoch 31] loss: 0.27264838248653256 acc: 0.6293
[Epoch 35] loss: 0.24486991200510347 acc: 0.6433
[Epoch 39] loss: 0.23140472802035797 acc: 0.6458
[Epoch 43] loss: 0.20393170185549103 acc: 0.6371
[Epoch 47] loss: 0.19719504495687268 acc: 0.63
[Epoch 51] loss: 0.19034434280946583 acc: 0.6331
[Epoch 55] loss: 0.1667162974262634 acc: 0.6325
[Epoch 59] loss: 0.1784703501790781 acc: 0.6343
[Epoch 63] loss: 0.16180673035342827 acc: 0.6364
[Epoch 67] loss: 0.1438781976606935 acc: 0.6375
[Epoch 71] loss: 0.15180803166435616 acc: 0.6287
--> [test] acc: 0.6367
--> [accuracy] finished 0.6367
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6367
--> [reward] 0.6367
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6367 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.465822331130962 acc: 0.5901
[Epoch 7] loss: 3.5509845307263572 acc: 0.6364
[Epoch 11] loss: 2.039011464132677 acc: 0.622
[Epoch 15] loss: 0.9279825433116892 acc: 0.6354
[Epoch 19] loss: 0.5291384088823481 acc: 0.6369
[Epoch 23] loss: 0.3752111691476591 acc: 0.6446
[Epoch 27] loss: 0.3083822921613979 acc: 0.6382
[Epoch 31] loss: 0.28375204655882497 acc: 0.6387
[Epoch 35] loss: 0.2573750641725748 acc: 0.6375
[Epoch 39] loss: 0.2251173330463774 acc: 0.6318
[Epoch 43] loss: 0.222975344790141 acc: 0.6305
[Epoch 47] loss: 0.19318894636305167 acc: 0.6332
[Epoch 51] loss: 0.19186990983102975 acc: 0.6374
[Epoch 55] loss: 0.18254206986273722 acc: 0.6261
[Epoch 59] loss: 0.15971332608038546 acc: 0.637
[Epoch 63] loss: 0.15378247093066305 acc: 0.6361
[Epoch 67] loss: 0.15829407421114575 acc: 0.6266
[Epoch 71] loss: 0.14679135762922027 acc: 0.6386
--> [test] acc: 0.636
--> [accuracy] finished 0.636
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.636
--> [reward] 0.636
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.636  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.52474744027228 acc: 0.586
[Epoch 7] loss: 3.592296634336262 acc: 0.6379
[Epoch 11] loss: 2.0413656784674092 acc: 0.6504
[Epoch 15] loss: 0.9144159084581354 acc: 0.6466
[Epoch 19] loss: 0.5211414107505012 acc: 0.6403
[Epoch 23] loss: 0.3766012603197904 acc: 0.6349
[Epoch 27] loss: 0.3210860818286267 acc: 0.6475
[Epoch 31] loss: 0.26708564044350325 acc: 0.6382
[Epoch 35] loss: 0.2480108441946947 acc: 0.6353
[Epoch 39] loss: 0.23118874550942342 acc: 0.6409
[Epoch 43] loss: 0.21395909191464144 acc: 0.6296
[Epoch 47] loss: 0.19292746334934555 acc: 0.6336
[Epoch 51] loss: 0.1910650472006644 acc: 0.6286
[Epoch 55] loss: 0.1764423795499365 acc: 0.6314
[Epoch 59] loss: 0.1737588013824828 acc: 0.6354
[Epoch 63] loss: 0.1646597465914686 acc: 0.6323
[Epoch 67] loss: 0.1492282593637095 acc: 0.6335
[Epoch 71] loss: 0.1536508889837891 acc: 0.63
--> [test] acc: 0.627
--> [accuracy] finished 0.627
new state: tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.627
--> [reward] 0.627
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1962]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3924]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6264]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6982]], device='cuda:0')
------ ------
delta_t: tensor([[0.6264]], device='cuda:0')
rewards[i]: 0.627
values[i+1]: tensor([[0.0720]], device='cuda:0')
values[i]: tensor([[0.0719]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6264]], device='cuda:0')
delta_t: tensor([[0.6264]], device='cuda:0')
------ ------
policy_loss: 1.4786221981048584
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6264]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.9844]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.5765]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2556]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3273]], device='cuda:0')
------ ------
delta_t: tensor([[0.6354]], device='cuda:0')
rewards[i]: 0.636
values[i+1]: tensor([[0.0719]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0717]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2556]], device='cuda:0')
delta_t: tensor([[0.6354]], device='cuda:0')
------ ------
policy_loss: 4.465153694152832
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2556]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.7502]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.5316]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8792]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9507]], device='cuda:0')
------ ------
delta_t: tensor([[0.6362]], device='cuda:0')
rewards[i]: 0.6367
values[i+1]: tensor([[0.0717]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0714]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8792]], device='cuda:0')
delta_t: tensor([[0.6362]], device='cuda:0')
------ ------
policy_loss: 8.948341369628906
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8792]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.8873]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.2743]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5048]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5760]], device='cuda:0')
------ ------
delta_t: tensor([[0.6444]], device='cuda:0')
rewards[i]: 0.6448
values[i+1]: tensor([[0.0714]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0711]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5048]], device='cuda:0')
delta_t: tensor([[0.6444]], device='cuda:0')
------ ------
policy_loss: 14.930292129516602
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5048]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[10.6747]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.5748]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.0943]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1651]], device='cuda:0')
------ ------
delta_t: tensor([[0.6145]], device='cuda:0')
rewards[i]: 0.6149
values[i+1]: tensor([[0.0711]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0708]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.0943]], device='cuda:0')
delta_t: tensor([[0.6145]], device='cuda:0')
------ ------
policy_loss: 22.329120635986328
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.0943]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[17.3648]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.3801]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6579]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7286]], device='cuda:0')
------ ------
delta_t: tensor([[0.5945]], device='cuda:0')
rewards[i]: 0.5951
values[i+1]: tensor([[0.0708]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0707]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6579]], device='cuda:0')
delta_t: tensor([[0.5945]], device='cuda:0')
------ ------
policy_loss: 31.078155517578125
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6579]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[26.1918]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.6540]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2017]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2723]], device='cuda:0')
------ ------
delta_t: tensor([[0.5804]], device='cuda:0')
rewards[i]: 0.581
values[i+1]: tensor([[0.0707]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0706]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2017]], device='cuda:0')
delta_t: tensor([[0.5804]], device='cuda:0')
------ ------
policy_loss: 41.12313461303711
log_probs[i]: tensor([[-2.3964]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2017]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.4389]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.4942]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7428]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8136]], device='cuda:0')
------ ------
delta_t: tensor([[0.5832]], device='cuda:0')
rewards[i]: 0.584
values[i+1]: tensor([[0.0706]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0708]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7428]], device='cuda:0')
delta_t: tensor([[0.5832]], device='cuda:0')
------ ------
policy_loss: 52.464935302734375
log_probs[i]: tensor([[-2.3964]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7428]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[51.4302]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[27.9827]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2899]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3609]], device='cuda:0')
------ ------
delta_t: tensor([[0.5945]], device='cuda:0')
rewards[i]: 0.5955
values[i+1]: tensor([[0.0708]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0711]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2899]], device='cuda:0')
delta_t: tensor([[0.5945]], device='cuda:0')
------ ------
policy_loss: 65.1261978149414
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2899]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[68.4679]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.0753]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8374]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.9089]], device='cuda:0')
------ ------
delta_t: tensor([[0.6004]], device='cuda:0')
rewards[i]: 0.6016
values[i+1]: tensor([[0.0711]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0715]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8374]], device='cuda:0')
delta_t: tensor([[0.6004]], device='cuda:0')
------ ------
policy_loss: 79.09703063964844
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8374]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[88.8197]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[40.7037]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3799]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4514]], device='cuda:0')
------ ------
delta_t: tensor([[0.6009]], device='cuda:0')
rewards[i]: 0.6016
values[i+1]: tensor([[0.0715]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0715]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3799]], device='cuda:0')
delta_t: tensor([[0.6009]], device='cuda:0')
------ ------
policy_loss: 94.36864471435547
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3799]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[112.9655]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[48.2914]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9492]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.0209]], device='cuda:0')
------ ------
delta_t: tensor([[0.6331]], device='cuda:0')
rewards[i]: 0.634
values[i+1]: tensor([[0.0715]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0717]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9492]], device='cuda:0')
delta_t: tensor([[0.6331]], device='cuda:0')
------ ------
policy_loss: 111.00492095947266
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9492]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[141.2865]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.6420]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.5261]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5978]], device='cuda:0')
------ ------
delta_t: tensor([[0.6464]], device='cuda:0')
rewards[i]: 0.6471
values[i+1]: tensor([[0.0717]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0717]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.5261]], device='cuda:0')
delta_t: tensor([[0.6464]], device='cuda:0')
------ ------
policy_loss: 129.02423095703125
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.5261]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[174.0919]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[65.6109]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.1001]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1712]], device='cuda:0')
------ ------
delta_t: tensor([[0.6492]], device='cuda:0')
rewards[i]: 0.6494
values[i+1]: tensor([[0.0717]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0712]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.1001]], device='cuda:0')
delta_t: tensor([[0.6492]], device='cuda:0')
------ ------
policy_loss: 148.43104553222656
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.1001]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[211.5545]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.9251]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6559]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7262]], device='cuda:0')
------ ------
delta_t: tensor([[0.6369]], device='cuda:0')
rewards[i]: 0.6367
values[i+1]: tensor([[0.0712]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0703]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6559]], device='cuda:0')
delta_t: tensor([[0.6369]], device='cuda:0')
------ ------
policy_loss: 169.1620635986328
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6559]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 169.1620635986328
value_loss: 211.5544891357422
loss: 274.9393005371094



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-3.7951e-03, -1.9888e-05, -7.4155e-06, -1.8867e-05, -2.9673e-05],
        [ 1.0984e-01,  5.7003e-04,  2.1887e-04,  5.4587e-04,  8.5067e-04],
        [-1.0549e-03, -5.5211e-06, -2.0939e-06, -5.2467e-06, -8.2325e-06],
        [ 5.6738e-01,  2.9302e-03,  1.1478e-03,  2.8195e-03,  4.3732e-03],
        [ 3.1432e+00,  1.6204e-02,  6.3933e-03,  1.5608e-02,  2.4182e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 3.9442e-05,  3.2911e-05, -1.3356e-05, -3.4079e-05,  3.0629e-05],
        [-1.1337e-03, -9.5050e-04,  3.7902e-04,  9.8460e-04, -8.7893e-04],
        [ 1.0958e-05,  9.1446e-06, -3.7060e-06, -9.4694e-06,  8.5079e-06],
        [-5.8366e-03, -4.9045e-03,  1.9389e-03,  5.0814e-03, -4.5213e-03],
        [-3.2288e-02, -2.7152e-02,  1.0705e-02,  2.8132e-02, -2.5006e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 0.9021, -0.8896,  0.9015, -0.8096,  0.5645],
        [-0.2959,  0.2918, -0.2957,  0.2658, -0.1854],
        [-0.4055,  0.3998, -0.4052,  0.3638, -0.2536],
        [-0.0185,  0.0182, -0.0184,  0.0166, -0.0116],
        [-0.1824,  0.1798, -0.1822,  0.1635, -0.1139]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.1026,  0.0846, -0.0347, -0.0877,  0.0794],
        [ 0.0153,  0.0115, -0.0072, -0.0117,  0.0126],
        [-0.0466, -0.0384,  0.0162,  0.0397, -0.0363],
        [ 0.0181,  0.0156, -0.0049, -0.0162,  0.0136],
        [-0.0007, -0.0008, -0.0002,  0.0008, -0.0004],
        [ 0.0173,  0.0156, -0.0043, -0.0162,  0.0130],
        [-0.0096, -0.0086,  0.0023,  0.0089, -0.0072],
        [-0.0193, -0.0159,  0.0066,  0.0165, -0.0150],
        [-0.0241, -0.0198,  0.0082,  0.0205, -0.0186],
        [-0.0460, -0.0380,  0.0160,  0.0393, -0.0359],
        [-0.0070, -0.0059,  0.0020,  0.0062, -0.0052]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.7994,  2.3073, -0.9713, -2.3863,  2.1822]], device='cuda:0')
--> [loss] 274.9393005371094

---------------------------------- [[#19 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.627  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([800.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.474557881160161 acc: 0.5998
[Epoch 7] loss: 3.5583333351727946 acc: 0.6408
[Epoch 11] loss: 2.030628185214289 acc: 0.6524
[Epoch 15] loss: 0.9153186208699517 acc: 0.6481
[Epoch 19] loss: 0.5109273839570448 acc: 0.6359
[Epoch 23] loss: 0.38676107469755594 acc: 0.6392
[Epoch 27] loss: 0.3037748337030182 acc: 0.6334
[Epoch 31] loss: 0.28439543107310145 acc: 0.6313
[Epoch 35] loss: 0.2477976568233784 acc: 0.6304
[Epoch 39] loss: 0.2187154541301834 acc: 0.6287
[Epoch 43] loss: 0.21962467218270462 acc: 0.6218
[Epoch 47] loss: 0.19270221031535312 acc: 0.6293
[Epoch 51] loss: 0.17962960210388235 acc: 0.6375
[Epoch 55] loss: 0.18124807043396451 acc: 0.6273
[Epoch 59] loss: 0.1571710231168198 acc: 0.6219
[Epoch 63] loss: 0.16339426973948964 acc: 0.6261
[Epoch 67] loss: 0.14147886133793255 acc: 0.6139
[Epoch 71] loss: 0.13932996290400051 acc: 0.631
--> [test] acc: 0.6309
--> [accuracy] finished 0.6309
new state: tensor([800.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6309
--> [reward] 0.6309
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6309 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([800.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.483449344287443 acc: 0.5893
[Epoch 7] loss: 3.522837592284088 acc: 0.6427
[Epoch 11] loss: 2.0008163830584578 acc: 0.6543
[Epoch 15] loss: 0.9264050631514748 acc: 0.6407
[Epoch 19] loss: 0.5209843965861803 acc: 0.6369
[Epoch 23] loss: 0.37610058763714704 acc: 0.6329
[Epoch 27] loss: 0.32692607269024526 acc: 0.6407
[Epoch 31] loss: 0.2770028167601932 acc: 0.6477
[Epoch 35] loss: 0.2483627913850824 acc: 0.6414
[Epoch 39] loss: 0.2320165930058607 acc: 0.6367
[Epoch 43] loss: 0.21254412756989832 acc: 0.6306
[Epoch 47] loss: 0.1952936225273001 acc: 0.6344
[Epoch 51] loss: 0.188724991088957 acc: 0.629
[Epoch 55] loss: 0.1700473481627262 acc: 0.6382
[Epoch 59] loss: 0.17138968002827615 acc: 0.636
[Epoch 63] loss: 0.16254042116972758 acc: 0.6249
[Epoch 67] loss: 0.15383089515511567 acc: 0.6396
[Epoch 71] loss: 0.14162869746183449 acc: 0.6374
--> [test] acc: 0.6384
--> [accuracy] finished 0.6384
new state: tensor([800.,   4.,   1.,   4.,   5.], device='cuda:0')
new reward: 0.6384
--> [reward] 0.6384
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     4.0      |     5.0     | 0.6384 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([800.,   4.,   1.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.540529972604474 acc: 0.569
[Epoch 7] loss: 3.648506080555489 acc: 0.6286
[Epoch 11] loss: 2.086814007871901 acc: 0.6369
[Epoch 15] loss: 0.9469300243631958 acc: 0.6378
[Epoch 19] loss: 0.525274227561472 acc: 0.6288
[Epoch 23] loss: 0.384764729191542 acc: 0.6269
[Epoch 27] loss: 0.32869555990037785 acc: 0.6284
[Epoch 31] loss: 0.277853876592882 acc: 0.6333
[Epoch 35] loss: 0.2555121092859398 acc: 0.6253
[Epoch 39] loss: 0.24343407396084207 acc: 0.6293
[Epoch 43] loss: 0.2201934366432183 acc: 0.6344
[Epoch 47] loss: 0.20078424544399961 acc: 0.6256
[Epoch 51] loss: 0.18850607657090515 acc: 0.6265
[Epoch 55] loss: 0.1853653084179458 acc: 0.62
[Epoch 59] loss: 0.16900836076120587 acc: 0.6192
[Epoch 63] loss: 0.16051045143166962 acc: 0.6279
[Epoch 67] loss: 0.15989621832390385 acc: 0.6245
[Epoch 71] loss: 0.15043637349539438 acc: 0.6368
--> [test] acc: 0.6325
--> [accuracy] finished 0.6325
new state: tensor([800.,   4.,   1.,   3.,   5.], device='cuda:0')
new reward: 0.6325
--> [reward] 0.6325
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     3.0      |     5.0     | 0.6325 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([800.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.840414732313522 acc: 0.5457
[Epoch 7] loss: 4.039493943876622 acc: 0.6218
[Epoch 11] loss: 2.49125747790422 acc: 0.6052
[Epoch 15] loss: 1.2105773970332292 acc: 0.6074
[Epoch 19] loss: 0.6636291627612565 acc: 0.5986
[Epoch 23] loss: 0.45545549089532067 acc: 0.5963
[Epoch 27] loss: 0.3860171963639386 acc: 0.5932
[Epoch 31] loss: 0.32310429660369977 acc: 0.5906
[Epoch 35] loss: 0.28087906537534635 acc: 0.5991
[Epoch 39] loss: 0.26433609695414373 acc: 0.5906
[Epoch 43] loss: 0.2506254392335444 acc: 0.5968
[Epoch 47] loss: 0.22709980323824966 acc: 0.5789
[Epoch 51] loss: 0.212011165201159 acc: 0.5828
[Epoch 55] loss: 0.20149936488903392 acc: 0.5919
[Epoch 59] loss: 0.19159622734074322 acc: 0.59
[Epoch 63] loss: 0.17780959413594108 acc: 0.5957
[Epoch 67] loss: 0.1759250926239716 acc: 0.5879
[Epoch 71] loss: 0.17050735318028104 acc: 0.5839
--> [test] acc: 0.5931
--> [accuracy] finished 0.5931
new state: tensor([800.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.5931
--> [reward] 0.5931
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.5931 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([800.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.804511217509999 acc: 0.5626
[Epoch 7] loss: 4.042675616186293 acc: 0.595
[Epoch 11] loss: 2.5170084956052055 acc: 0.6076
[Epoch 15] loss: 1.2655129366747253 acc: 0.5961
[Epoch 19] loss: 0.6821355472611802 acc: 0.5928
[Epoch 23] loss: 0.4797367174226 acc: 0.5947
[Epoch 27] loss: 0.38844040336797153 acc: 0.5876
[Epoch 31] loss: 0.3396272378952226 acc: 0.6002
[Epoch 35] loss: 0.3041494682412166 acc: 0.5928
[Epoch 39] loss: 0.27462702138049294 acc: 0.5956
[Epoch 43] loss: 0.23818452226336273 acc: 0.5917
[Epoch 47] loss: 0.23620535821541952 acc: 0.5919
[Epoch 51] loss: 0.22491364306329614 acc: 0.5979
[Epoch 55] loss: 0.21552984777580747 acc: 0.587
[Epoch 59] loss: 0.20087640824944467 acc: 0.5893
[Epoch 63] loss: 0.1868065862232805 acc: 0.5835
[Epoch 67] loss: 0.179607820762631 acc: 0.5956
[Epoch 71] loss: 0.17177688671946478 acc: 0.5918
--> [test] acc: 0.5877
--> [accuracy] finished 0.5877
new state: tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
new reward: 0.5877
--> [reward] 0.5877
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     3.0      |     6.0     | 0.5877 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([768.,   4.,   2.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.422346135082147 acc: 0.5887
[Epoch 7] loss: 3.343136818665068 acc: 0.6412
[Epoch 11] loss: 1.6343501347028995 acc: 0.6469
[Epoch 15] loss: 0.7032230653611901 acc: 0.6366
[Epoch 19] loss: 0.4192825830672556 acc: 0.6466
[Epoch 23] loss: 0.30488683808061395 acc: 0.6479
[Epoch 27] loss: 0.27701028381638665 acc: 0.6356
[Epoch 31] loss: 0.22964945724686545 acc: 0.6378
[Epoch 35] loss: 0.20691805811184447 acc: 0.6452
[Epoch 39] loss: 0.20213066183430764 acc: 0.6415
[Epoch 43] loss: 0.1720052274239376 acc: 0.6392
[Epoch 47] loss: 0.16688568125028744 acc: 0.6289
[Epoch 51] loss: 0.16154316644948882 acc: 0.6317
[Epoch 55] loss: 0.14252144535955832 acc: 0.6345
[Epoch 59] loss: 0.13273972085834293 acc: 0.6248
[Epoch 63] loss: 0.12928614332852767 acc: 0.6277
[Epoch 67] loss: 0.12150198963882826 acc: 0.6339
[Epoch 71] loss: 0.12902202744044655 acc: 0.6386
--> [test] acc: 0.6328
--> [accuracy] finished 0.6328
new state: tensor([768.,   4.,   2.,   3.,   6.], device='cuda:0')
new reward: 0.6328
--> [reward] 0.6328
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     3.0      |     6.0     | 0.6328 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([768.,   4.,   2.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.566729888129418 acc: 0.5806
[Epoch 7] loss: 3.4826553879339066 acc: 0.6168
[Epoch 11] loss: 1.7093306358360574 acc: 0.6313
[Epoch 15] loss: 0.7419663299437221 acc: 0.6267
[Epoch 19] loss: 0.4530932370982969 acc: 0.62
[Epoch 23] loss: 0.3409291014713628 acc: 0.6216
[Epoch 27] loss: 0.295534075204464 acc: 0.6238
[Epoch 31] loss: 0.24739799111881447 acc: 0.6216
[Epoch 35] loss: 0.221859596006315 acc: 0.6277
[Epoch 39] loss: 0.20620053103360372 acc: 0.6281
[Epoch 43] loss: 0.18575595268060255 acc: 0.6324
[Epoch 47] loss: 0.1702473150614454 acc: 0.6334
[Epoch 51] loss: 0.16830744292668026 acc: 0.6216
[Epoch 55] loss: 0.14826094053085426 acc: 0.6202
[Epoch 59] loss: 0.14597004361907998 acc: 0.6238
[Epoch 63] loss: 0.14334358974143177 acc: 0.629
[Epoch 67] loss: 0.13832378072445006 acc: 0.6351
[Epoch 71] loss: 0.12725772628860782 acc: 0.6186
--> [test] acc: 0.6292
--> [accuracy] finished 0.6292
new state: tensor([768.,   4.,   2.,   2.,   6.], device='cuda:0')
new reward: 0.6292
--> [reward] 0.6292
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     2.0      |     6.0     | 0.6292 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([768.,   4.,   1.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.968769219220447 acc: 0.5315
[Epoch 7] loss: 4.253132009750132 acc: 0.5871
[Epoch 11] loss: 2.642739540018389 acc: 0.5849
[Epoch 15] loss: 1.3075301123740117 acc: 0.5727
[Epoch 19] loss: 0.7188510129995206 acc: 0.5638
[Epoch 23] loss: 0.5072486025788595 acc: 0.5724
[Epoch 27] loss: 0.4290176059221825 acc: 0.5608
[Epoch 31] loss: 0.36494749670376636 acc: 0.5798
[Epoch 35] loss: 0.3215969659893028 acc: 0.57
[Epoch 39] loss: 0.3028136680707755 acc: 0.5626
[Epoch 43] loss: 0.2633191199920824 acc: 0.5689
[Epoch 47] loss: 0.25554866636233864 acc: 0.5598
[Epoch 51] loss: 0.2511961151538488 acc: 0.5657
[Epoch 55] loss: 0.2256245190811241 acc: 0.5634
[Epoch 59] loss: 0.21798497293373126 acc: 0.5691
[Epoch 63] loss: 0.19866384591673836 acc: 0.5701
[Epoch 67] loss: 0.18799879040230838 acc: 0.5696
[Epoch 71] loss: 0.19833553954006156 acc: 0.5721
--> [test] acc: 0.5683
--> [accuracy] finished 0.5683
new state: tensor([768.,   4.,   1.,   2.,   6.], device='cuda:0')
new reward: 0.5683
--> [reward] 0.5683
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     2.0      |     6.0     | 0.5683 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.735506621772981 acc: 0.5646
[Epoch 7] loss: 3.868428959566004 acc: 0.6106
[Epoch 11] loss: 2.260785831248059 acc: 0.6117
[Epoch 15] loss: 1.040089372069101 acc: 0.6008
[Epoch 19] loss: 0.5694073371617766 acc: 0.6101
[Epoch 23] loss: 0.4112251635088259 acc: 0.6047
[Epoch 27] loss: 0.34878388276833405 acc: 0.5958
[Epoch 31] loss: 0.3056654726510005 acc: 0.6023
[Epoch 35] loss: 0.27171089219839295 acc: 0.6071
[Epoch 39] loss: 0.2546199183968251 acc: 0.6009
[Epoch 43] loss: 0.23425153759605896 acc: 0.603
[Epoch 47] loss: 0.2145564023719724 acc: 0.6054
[Epoch 51] loss: 0.20963866811941195 acc: 0.5993
[Epoch 55] loss: 0.1904872092978119 acc: 0.6006
[Epoch 59] loss: 0.18531599565578238 acc: 0.604
[Epoch 63] loss: 0.17715650418525575 acc: 0.6013
[Epoch 67] loss: 0.16645378651111709 acc: 0.6018
[Epoch 71] loss: 0.16184920404354097 acc: 0.6077
--> [test] acc: 0.6011
--> [accuracy] finished 0.6011
new state: tensor([768.,   4.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.6011
--> [reward] 0.6011
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     2.0      |     5.0     | 0.6011 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.725122187143701 acc: 0.5619
[Epoch 7] loss: 3.8781647741642145 acc: 0.6031
[Epoch 11] loss: 2.2527775599066255 acc: 0.6107
[Epoch 15] loss: 1.021908045043726 acc: 0.6055
[Epoch 19] loss: 0.569323090831642 acc: 0.6132
[Epoch 23] loss: 0.40972277609264607 acc: 0.6098
[Epoch 27] loss: 0.35035809394105544 acc: 0.6079
[Epoch 31] loss: 0.2991922206633612 acc: 0.5935
[Epoch 35] loss: 0.27471855704141473 acc: 0.6051
[Epoch 39] loss: 0.25198665060712705 acc: 0.6028
[Epoch 43] loss: 0.2315675986006551 acc: 0.6093
[Epoch 47] loss: 0.2259472171249597 acc: 0.6083
[Epoch 51] loss: 0.21422272966817366 acc: 0.5923
[Epoch 55] loss: 0.18421765440560478 acc: 0.609
[Epoch 59] loss: 0.19391207101390415 acc: 0.6015
[Epoch 63] loss: 0.1716284067882106 acc: 0.6121
[Epoch 67] loss: 0.1692776172558832 acc: 0.6115
[Epoch 71] loss: 0.15774078143979697 acc: 0.5957
--> [test] acc: 0.6035
--> [accuracy] finished 0.6035
new state: tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.6035
--> [reward] 0.6035
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     2.0      |     5.0     | 0.6035 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.715892916140349 acc: 0.5524
[Epoch 7] loss: 3.8562323562324505 acc: 0.6283
[Epoch 11] loss: 2.2427229354219973 acc: 0.6239
[Epoch 15] loss: 1.052774672465556 acc: 0.604
[Epoch 19] loss: 0.5728705460848786 acc: 0.6018
[Epoch 23] loss: 0.4230379251586964 acc: 0.6069
[Epoch 27] loss: 0.3508222985159977 acc: 0.6066
[Epoch 31] loss: 0.303661007546079 acc: 0.6025
[Epoch 35] loss: 0.276227407371792 acc: 0.6
[Epoch 39] loss: 0.25114933339655016 acc: 0.597
[Epoch 43] loss: 0.22378047444450352 acc: 0.6011
[Epoch 47] loss: 0.23067479053526507 acc: 0.5913
[Epoch 51] loss: 0.20255493547271966 acc: 0.6119
[Epoch 55] loss: 0.2005861969307408 acc: 0.6044
[Epoch 59] loss: 0.1849309550610173 acc: 0.5965
[Epoch 63] loss: 0.16936483375056435 acc: 0.6029
[Epoch 67] loss: 0.17121052537161066 acc: 0.605
[Epoch 71] loss: 0.17226157814521542 acc: 0.5967
--> [test] acc: 0.6021
--> [accuracy] finished 0.6021
new state: tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.6021
--> [reward] 0.6021
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     2.0      |     5.0     | 0.6021 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.690243363990199 acc: 0.5713
[Epoch 7] loss: 3.8657427698450015 acc: 0.612
[Epoch 11] loss: 2.255483409099262 acc: 0.624
[Epoch 15] loss: 1.0439985413437762 acc: 0.6054
[Epoch 19] loss: 0.5812502127912496 acc: 0.6163
[Epoch 23] loss: 0.42195976447895206 acc: 0.6114
[Epoch 27] loss: 0.3419477019501883 acc: 0.6175
[Epoch 31] loss: 0.303730935830137 acc: 0.6103
[Epoch 35] loss: 0.27660272384057644 acc: 0.6041
[Epoch 39] loss: 0.25722387934441837 acc: 0.6118
[Epoch 43] loss: 0.2373644895780155 acc: 0.6115
[Epoch 47] loss: 0.2144313448006311 acc: 0.6118
[Epoch 51] loss: 0.20727501428731337 acc: 0.6078
[Epoch 55] loss: 0.1954600632981495 acc: 0.6113
[Epoch 59] loss: 0.1947358353539844 acc: 0.6037
[Epoch 63] loss: 0.1674169519275446 acc: 0.6069
[Epoch 67] loss: 0.16730330489775944 acc: 0.6006
[Epoch 71] loss: 0.15898592844891274 acc: 0.6076
--> [test] acc: 0.6095
--> [accuracy] finished 0.6095
new state: tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.6095
--> [reward] 0.6095
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     2.0      |     5.0     | 0.6095 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.424544067943797 acc: 0.5894
[Epoch 7] loss: 3.4734820984970884 acc: 0.6358
[Epoch 11] loss: 1.7893674198128378 acc: 0.6439
[Epoch 15] loss: 0.7696114166942246 acc: 0.6445
[Epoch 19] loss: 0.4393102711166643 acc: 0.6424
[Epoch 23] loss: 0.3345410555334347 acc: 0.6351
[Epoch 27] loss: 0.2980981098721399 acc: 0.6384
[Epoch 31] loss: 0.2535912278429855 acc: 0.6301
[Epoch 35] loss: 0.24328852574224286 acc: 0.6404
[Epoch 39] loss: 0.21755234789236655 acc: 0.6387
[Epoch 43] loss: 0.1986420171840302 acc: 0.6243
[Epoch 47] loss: 0.1837992264217485 acc: 0.638
[Epoch 51] loss: 0.17678288137659315 acc: 0.6154
[Epoch 55] loss: 0.17297303874779235 acc: 0.6366
[Epoch 59] loss: 0.16845028232166762 acc: 0.6334
[Epoch 63] loss: 0.1518385786232074 acc: 0.6345
[Epoch 67] loss: 0.1474578332506082 acc: 0.6355
[Epoch 71] loss: 0.13066410250387742 acc: 0.6305
--> [test] acc: 0.6312
--> [accuracy] finished 0.6312
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6312
--> [reward] 0.6312
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6312 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.436962044909787 acc: 0.5872
[Epoch 7] loss: 3.480906185286734 acc: 0.6378
[Epoch 11] loss: 1.7868693442562658 acc: 0.6479
[Epoch 15] loss: 0.7417862503825093 acc: 0.6573
[Epoch 19] loss: 0.4428424291798602 acc: 0.6506
[Epoch 23] loss: 0.33795007490231405 acc: 0.6408
[Epoch 27] loss: 0.2863141501803532 acc: 0.639
[Epoch 31] loss: 0.26299658042552604 acc: 0.6396
[Epoch 35] loss: 0.2373243389041413 acc: 0.6394
[Epoch 39] loss: 0.21684899278547223 acc: 0.6366
[Epoch 43] loss: 0.20371744502097597 acc: 0.6365
[Epoch 47] loss: 0.18360543599985826 acc: 0.6395
[Epoch 51] loss: 0.18520805200256998 acc: 0.6396
[Epoch 55] loss: 0.16762026166662458 acc: 0.6452
[Epoch 59] loss: 0.1663121402911518 acc: 0.6366
[Epoch 63] loss: 0.15296443217002864 acc: 0.6379
[Epoch 67] loss: 0.15198727751679508 acc: 0.6398
[Epoch 71] loss: 0.1382451357160364 acc: 0.6354
--> [test] acc: 0.6446
--> [accuracy] finished 0.6446
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6446
--> [reward] 0.6446
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6446 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0924, 0.0913, 0.0900, 0.0914, 0.0910, 0.0908, 0.0911, 0.0899,
         0.0904, 0.0904]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3964, -2.3975, -2.3988, -2.3974, -2.3978, -2.3980, -2.3977,
         -2.3989, -2.3984, -2.3984]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.42030866950979 acc: 0.6072
[Epoch 7] loss: 3.463725647810475 acc: 0.6482
[Epoch 11] loss: 1.7942831390882696 acc: 0.6525
[Epoch 15] loss: 0.7649958514324997 acc: 0.6456
[Epoch 19] loss: 0.4481596021967776 acc: 0.6481
[Epoch 23] loss: 0.34448473978921046 acc: 0.6466
[Epoch 27] loss: 0.29780080065350323 acc: 0.648
[Epoch 31] loss: 0.2613077189206429 acc: 0.643
[Epoch 35] loss: 0.24518901846416846 acc: 0.6363
[Epoch 39] loss: 0.21793816710019584 acc: 0.6526
[Epoch 43] loss: 0.19844062212149582 acc: 0.6354
[Epoch 47] loss: 0.18618827237916724 acc: 0.6361
[Epoch 51] loss: 0.19264593571567398 acc: 0.6385
[Epoch 55] loss: 0.1642466892531175 acc: 0.6386
[Epoch 59] loss: 0.16001979337738412 acc: 0.6411
[Epoch 63] loss: 0.15852226400533526 acc: 0.6353
[Epoch 67] loss: 0.14300575650409056 acc: 0.6434
[Epoch 71] loss: 0.1536110032628507 acc: 0.6404
--> [test] acc: 0.639
--> [accuracy] finished 0.639
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.639
--> [reward] 0.639
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2036]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4072]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6381]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7148]], device='cuda:0')
------ ------
delta_t: tensor([[0.6381]], device='cuda:0')
rewards[i]: 0.639
values[i+1]: tensor([[0.0766]], device='cuda:0')
values[i]: tensor([[0.0767]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6381]], device='cuda:0')
delta_t: tensor([[0.6381]], device='cuda:0')
------ ------
policy_loss: 1.5058987140655518
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6381]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0170]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6267]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2754]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3522]], device='cuda:0')
------ ------
delta_t: tensor([[0.6437]], device='cuda:0')
rewards[i]: 0.6446
values[i+1]: tensor([[0.0767]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0768]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2754]], device='cuda:0')
delta_t: tensor([[0.6437]], device='cuda:0')
------ ------
policy_loss: 4.539666652679443
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2754]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.8085]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.5831]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8929]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9699]], device='cuda:0')
------ ------
delta_t: tensor([[0.6302]], device='cuda:0')
rewards[i]: 0.6312
values[i+1]: tensor([[0.0768]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0770]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8929]], device='cuda:0')
delta_t: tensor([[0.6302]], device='cuda:0')
------ ------
policy_loss: 9.05495834350586
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8929]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.8899]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.1628]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.4825]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5597]], device='cuda:0')
------ ------
delta_t: tensor([[0.6085]], device='cuda:0')
rewards[i]: 0.6095
values[i+1]: tensor([[0.0770]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0772]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.4825]], device='cuda:0')
delta_t: tensor([[0.6085]], device='cuda:0')
------ ------
policy_loss: 14.982589721679688
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.4825]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[10.5682]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.3567]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.0589]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1362]], device='cuda:0')
------ ------
delta_t: tensor([[0.6012]], device='cuda:0')
rewards[i]: 0.6021
values[i+1]: tensor([[0.0772]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0773]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.0589]], device='cuda:0')
delta_t: tensor([[0.6012]], device='cuda:0')
------ ------
policy_loss: 22.29517364501953
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.0589]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[17.1592]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.1820]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6307]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7084]], device='cuda:0')
------ ------
delta_t: tensor([[0.6024]], device='cuda:0')
rewards[i]: 0.6035
values[i+1]: tensor([[0.0773]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0777]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6307]], device='cuda:0')
delta_t: tensor([[0.6024]], device='cuda:0')
------ ------
policy_loss: 30.975513458251953
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6307]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[25.9571]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.5958]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1947]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2724]], device='cuda:0')
------ ------
delta_t: tensor([[0.6003]], device='cuda:0')
rewards[i]: 0.6011
values[i+1]: tensor([[0.0777]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0777]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1947]], device='cuda:0')
delta_t: tensor([[0.6003]], device='cuda:0')
------ ------
policy_loss: 41.014305114746094
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1947]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.0977]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.2810]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7203]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7980]], device='cuda:0')
------ ------
delta_t: tensor([[0.5675]], device='cuda:0')
rewards[i]: 0.5683
values[i+1]: tensor([[0.0777]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0777]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7203]], device='cuda:0')
delta_t: tensor([[0.5675]], device='cuda:0')
------ ------
policy_loss: 52.3068962097168
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7203]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[51.1490]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[28.1027]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.3012]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3792]], device='cuda:0')
------ ------
delta_t: tensor([[0.6281]], device='cuda:0')
rewards[i]: 0.6292
values[i+1]: tensor([[0.0777]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0780]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.3012]], device='cuda:0')
delta_t: tensor([[0.6281]], device='cuda:0')
------ ------
policy_loss: 64.99544525146484
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.3012]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[68.4360]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.5740]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8800]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.9582]], device='cuda:0')
------ ------
delta_t: tensor([[0.6318]], device='cuda:0')
rewards[i]: 0.6328
values[i+1]: tensor([[0.0780]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0782]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8800]], device='cuda:0')
delta_t: tensor([[0.6318]], device='cuda:0')
------ ------
policy_loss: 79.07035827636719
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8800]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[88.9670]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[41.0620]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.4080]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4863]], device='cuda:0')
------ ------
delta_t: tensor([[0.5868]], device='cuda:0')
rewards[i]: 0.5877
values[i+1]: tensor([[0.0782]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0783]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.4080]], device='cuda:0')
delta_t: tensor([[0.5868]], device='cuda:0')
------ ------
policy_loss: 94.40895080566406
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.4080]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[113.0254]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[48.1167]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9366]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.0145]], device='cuda:0')
------ ------
delta_t: tensor([[0.5927]], device='cuda:0')
rewards[i]: 0.5931
values[i+1]: tensor([[0.0783]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0779]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9366]], device='cuda:0')
delta_t: tensor([[0.5927]], device='cuda:0')
------ ------
policy_loss: 111.02198791503906
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9366]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[141.1476]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.2444]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4996]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5769]], device='cuda:0')
------ ------
delta_t: tensor([[0.6324]], device='cuda:0')
rewards[i]: 0.6325
values[i+1]: tensor([[0.0779]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0773]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4996]], device='cuda:0')
delta_t: tensor([[0.6324]], device='cuda:0')
------ ------
policy_loss: 128.98248291015625
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4996]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[173.6541]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[65.0131]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0631]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1395]], device='cuda:0')
------ ------
delta_t: tensor([[0.6384]], device='cuda:0')
rewards[i]: 0.6384
values[i+1]: tensor([[0.0773]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0765]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0631]], device='cuda:0')
delta_t: tensor([[0.6384]], device='cuda:0')
------ ------
policy_loss: 148.2891845703125
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0631]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[210.7544]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.2006]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6140]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6890]], device='cuda:0')
------ ------
delta_t: tensor([[0.6315]], device='cuda:0')
rewards[i]: 0.6309
values[i+1]: tensor([[0.0765]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0751]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6140]], device='cuda:0')
delta_t: tensor([[0.6315]], device='cuda:0')
------ ------
policy_loss: 168.90737915039062
log_probs[i]: tensor([[-2.3964]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6140]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 168.90737915039062
value_loss: 210.75442504882812
loss: 274.28460693359375



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-4.1299e-03, -2.1403e-05, -5.9307e-06, -1.7396e-05, -2.8616e-05],
        [ 1.2738e-01,  6.5836e-04,  1.8105e-04,  5.4327e-04,  8.7796e-04],
        [-1.0661e-03, -5.5191e-06, -1.5388e-06, -4.4742e-06, -7.3730e-06],
        [ 6.4877e-01,  3.3481e-03,  9.2151e-04,  2.7722e-03,  4.4639e-03],
        [ 3.4330e+00,  1.7703e-02,  4.8703e-03,  1.4684e-02,  2.3591e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 4.3785e-05,  3.4974e-05, -1.5447e-05, -3.6336e-05,  3.3915e-05],
        [-1.3474e-03, -1.0770e-03,  4.7423e-04,  1.1189e-03, -1.0435e-03],
        [ 1.1315e-05,  9.0271e-06, -3.9890e-06, -9.3816e-06,  8.7589e-06],
        [-6.8601e-03, -5.4812e-03,  2.4116e-03,  5.6951e-03, -5.3111e-03],
        [-3.6295e-02, -2.8993e-02,  1.2752e-02,  3.0126e-02, -2.8095e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.0077, -0.9934,  1.0070, -0.9023,  0.6406],
        [-0.3448,  0.3399, -0.3446,  0.3088, -0.2191],
        [-0.4430,  0.4368, -0.4427,  0.3967, -0.2817],
        [-0.0224,  0.0221, -0.0224,  0.0201, -0.0143],
        [-0.1974,  0.1946, -0.1973,  0.1767, -0.1255]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0289,  0.0225, -0.0100, -0.0235,  0.0218],
        [ 0.0150,  0.0127, -0.0050, -0.0130,  0.0120],
        [-0.0492, -0.0385,  0.0174,  0.0402, -0.0379],
        [-0.0484, -0.0379,  0.0172,  0.0395, -0.0373],
        [ 0.0810,  0.0632, -0.0292, -0.0658,  0.0626],
        [-0.0035, -0.0029,  0.0012,  0.0030, -0.0029],
        [ 0.0632,  0.0494, -0.0224, -0.0515,  0.0486],
        [-0.0491, -0.0385,  0.0174,  0.0401, -0.0378],
        [-0.0170, -0.0134,  0.0057,  0.0140, -0.0130],
        [ 0.0051,  0.0041, -0.0011, -0.0044,  0.0036],
        [-0.0260, -0.0205,  0.0087,  0.0214, -0.0198]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 2.9573,  2.3161, -1.0472, -2.4136,  2.2757]], device='cuda:0')
--> [loss] 274.28460693359375

---------------------------------- [[#20 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.639  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.396098631879558 acc: 0.5962
[Epoch 7] loss: 3.454722048452748 acc: 0.6527
[Epoch 11] loss: 1.7665072222195013 acc: 0.6515
[Epoch 15] loss: 0.7320725602690902 acc: 0.6483
[Epoch 19] loss: 0.4440046693996319 acc: 0.6364
[Epoch 23] loss: 0.33822159756861075 acc: 0.6421
[Epoch 27] loss: 0.2916723774993778 acc: 0.6376
[Epoch 31] loss: 0.2646266067248133 acc: 0.6422
[Epoch 35] loss: 0.22490360923683095 acc: 0.6377
[Epoch 39] loss: 0.22786578086092876 acc: 0.6298
[Epoch 43] loss: 0.20140050588618688 acc: 0.6302
[Epoch 47] loss: 0.1919629134504539 acc: 0.6377
[Epoch 51] loss: 0.17843911389503486 acc: 0.6412
[Epoch 55] loss: 0.1543991513421659 acc: 0.6366
[Epoch 59] loss: 0.16456051560921495 acc: 0.6294
[Epoch 63] loss: 0.15585582424193392 acc: 0.6436
[Epoch 67] loss: 0.14570957692602024 acc: 0.6316
[Epoch 71] loss: 0.14546661513507406 acc: 0.6325
--> [test] acc: 0.6352
--> [accuracy] finished 0.6352
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6352
--> [reward] 0.6352
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6352 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.007207814080026 acc: 0.6476
[Epoch 7] loss: 2.7394273809307372 acc: 0.6924
[Epoch 11] loss: 1.0617006068186992 acc: 0.6989
[Epoch 15] loss: 0.44191797479720374 acc: 0.6845
[Epoch 19] loss: 0.29544942834607474 acc: 0.694
[Epoch 23] loss: 0.24262920383106718 acc: 0.6854
[Epoch 27] loss: 0.20418689368040208 acc: 0.6838
[Epoch 31] loss: 0.18673415249094483 acc: 0.6882
[Epoch 35] loss: 0.16622010084426464 acc: 0.6897
[Epoch 39] loss: 0.15721010263113644 acc: 0.6869
[Epoch 43] loss: 0.14016053545981874 acc: 0.6904
[Epoch 47] loss: 0.14643608343661252 acc: 0.6894
[Epoch 51] loss: 0.1198781748057243 acc: 0.689
[Epoch 55] loss: 0.12481533728904375 acc: 0.6935
[Epoch 59] loss: 0.11197840187298444 acc: 0.6758
[Epoch 63] loss: 0.10417367572826736 acc: 0.6869
[Epoch 67] loss: 0.1151377133336728 acc: 0.6934
[Epoch 71] loss: 0.10333108410710835 acc: 0.6854
--> [test] acc: 0.6837
--> [accuracy] finished 0.6837
new state: tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.6837
--> [reward] 0.6837
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     5.0     | 0.6837 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.962179174661027 acc: 0.6246
[Epoch 7] loss: 2.7483269923635762 acc: 0.7031
[Epoch 11] loss: 1.0722956737250928 acc: 0.6866
[Epoch 15] loss: 0.45305265126573613 acc: 0.6981
[Epoch 19] loss: 0.2904861806070103 acc: 0.6884
[Epoch 23] loss: 0.250982722279418 acc: 0.687
[Epoch 27] loss: 0.2055008588525493 acc: 0.6834
[Epoch 31] loss: 0.1982744745314693 acc: 0.6948
[Epoch 35] loss: 0.1694522562682095 acc: 0.6789
[Epoch 39] loss: 0.15736426539627402 acc: 0.6961
[Epoch 43] loss: 0.143515297321274 acc: 0.69
[Epoch 47] loss: 0.12404694402461772 acc: 0.6825
[Epoch 51] loss: 0.13371204204030832 acc: 0.691
[Epoch 55] loss: 0.1189729689770495 acc: 0.6928
[Epoch 59] loss: 0.11637850002536927 acc: 0.6915
[Epoch 63] loss: 0.11530394432113489 acc: 0.6883
[Epoch 67] loss: 0.0968828969816317 acc: 0.6963
[Epoch 71] loss: 0.10812172449975158 acc: 0.6925
--> [test] acc: 0.6917
--> [accuracy] finished 0.6917
new state: tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.6917
--> [reward] 0.6917
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     5.0     | 0.6917 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   5.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.771011712301113 acc: 0.6605
[Epoch 7] loss: 2.5425377617711606 acc: 0.7123
[Epoch 11] loss: 0.9071084378796923 acc: 0.7069
[Epoch 15] loss: 0.3900546785682211 acc: 0.7069
[Epoch 19] loss: 0.2692039795815373 acc: 0.7005
[Epoch 23] loss: 0.22006446315104242 acc: 0.6941
[Epoch 27] loss: 0.19573906466336277 acc: 0.7081
[Epoch 31] loss: 0.164022533906638 acc: 0.7013
[Epoch 35] loss: 0.15115093971457322 acc: 0.7057
[Epoch 39] loss: 0.1462950292746048 acc: 0.707
[Epoch 43] loss: 0.12493565995746371 acc: 0.7059
[Epoch 47] loss: 0.12088065689472514 acc: 0.6983
[Epoch 51] loss: 0.11764881541938199 acc: 0.7018
[Epoch 55] loss: 0.11507826517818405 acc: 0.7064
[Epoch 59] loss: 0.10618372619464098 acc: 0.6965
[Epoch 63] loss: 0.09607132650879652 acc: 0.7025
[Epoch 67] loss: 0.10235276299006546 acc: 0.7061
[Epoch 71] loss: 0.09364068610123251 acc: 0.7038
--> [test] acc: 0.7033
--> [accuracy] finished 0.7033
new state: tensor([736.,   5.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.7033
--> [reward] 0.7033
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     2.0     |     1.0      |     5.0     | 0.7033 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.264149987484183 acc: 0.6181
[Epoch 7] loss: 3.241360671379987 acc: 0.668
[Epoch 11] loss: 1.5529124513459023 acc: 0.6586
[Epoch 15] loss: 0.6231833205313024 acc: 0.6572
[Epoch 19] loss: 0.39346944618388974 acc: 0.6516
[Epoch 23] loss: 0.3182631697972565 acc: 0.6627
[Epoch 27] loss: 0.2612222653463521 acc: 0.6602
[Epoch 31] loss: 0.23614252402740138 acc: 0.6519
[Epoch 35] loss: 0.21033533576571042 acc: 0.6543
[Epoch 39] loss: 0.2037662154663821 acc: 0.6494
[Epoch 43] loss: 0.18450498437304097 acc: 0.6543
[Epoch 47] loss: 0.16927578558435524 acc: 0.6445
[Epoch 51] loss: 0.16630763292093487 acc: 0.6466
[Epoch 55] loss: 0.15217027224986182 acc: 0.6484
[Epoch 59] loss: 0.14714070535300638 acc: 0.6484
[Epoch 63] loss: 0.1376388885469064 acc: 0.6475
[Epoch 67] loss: 0.13629427651756817 acc: 0.6442
[Epoch 71] loss: 0.12286831822086726 acc: 0.6441
--> [test] acc: 0.6503
--> [accuracy] finished 0.6503
new state: tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6503
--> [reward] 0.6503
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     5.0     | 0.6503 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.244310684063855 acc: 0.6038
[Epoch 7] loss: 3.258363196002248 acc: 0.658
[Epoch 11] loss: 1.5284152766094183 acc: 0.6528
[Epoch 15] loss: 0.6190577916505621 acc: 0.6577
[Epoch 19] loss: 0.3861675203999366 acc: 0.6525
[Epoch 23] loss: 0.29912420621622937 acc: 0.6555
[Epoch 27] loss: 0.24948601934420483 acc: 0.651
[Epoch 31] loss: 0.23504402583984235 acc: 0.654
[Epoch 35] loss: 0.20890047404286274 acc: 0.6494
[Epoch 39] loss: 0.19251508421271735 acc: 0.6514
[Epoch 43] loss: 0.17831709175644553 acc: 0.6447
[Epoch 47] loss: 0.17898554144822576 acc: 0.648
[Epoch 51] loss: 0.14554081539459088 acc: 0.6506
[Epoch 55] loss: 0.15654427866639611 acc: 0.6486
[Epoch 59] loss: 0.13572581689757154 acc: 0.6429
[Epoch 63] loss: 0.13954527274815037 acc: 0.6498
[Epoch 67] loss: 0.13009860959616215 acc: 0.6492
[Epoch 71] loss: 0.12750849775943782 acc: 0.6547
--> [test] acc: 0.6431
--> [accuracy] finished 0.6431
new state: tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6431
--> [reward] 0.6431
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     5.0     | 0.6431 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.200511252483748 acc: 0.6158
[Epoch 7] loss: 3.203794693626711 acc: 0.6625
[Epoch 11] loss: 1.5189008713149659 acc: 0.6754
[Epoch 15] loss: 0.6201168597030365 acc: 0.6607
[Epoch 19] loss: 0.3814502044526093 acc: 0.6552
[Epoch 23] loss: 0.29911216171196353 acc: 0.6483
[Epoch 27] loss: 0.2643605054682478 acc: 0.6546
[Epoch 31] loss: 0.2371805460618623 acc: 0.6604
[Epoch 35] loss: 0.2059674466367039 acc: 0.6607
[Epoch 39] loss: 0.19729499787311344 acc: 0.6507
[Epoch 43] loss: 0.17845510027211758 acc: 0.6548
[Epoch 47] loss: 0.17626500627993014 acc: 0.6575
[Epoch 51] loss: 0.15486109532478987 acc: 0.6537
[Epoch 55] loss: 0.15675755496829977 acc: 0.6497
[Epoch 59] loss: 0.14669471511812618 acc: 0.6522
[Epoch 63] loss: 0.13932835199820148 acc: 0.6535
[Epoch 67] loss: 0.13624558570828108 acc: 0.65
[Epoch 71] loss: 0.13812346625334734 acc: 0.6502
--> [test] acc: 0.6502
--> [accuracy] finished 0.6502
new state: tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6502
--> [reward] 0.6502
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     5.0     | 0.6502 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.431589799463901 acc: 0.5845
[Epoch 7] loss: 3.491005436691177 acc: 0.6577
[Epoch 11] loss: 1.813813935460337 acc: 0.6421
[Epoch 15] loss: 0.7685807598445117 acc: 0.641
[Epoch 19] loss: 0.4515220875211079 acc: 0.6431
[Epoch 23] loss: 0.34236406360793376 acc: 0.6318
[Epoch 27] loss: 0.2966306847913186 acc: 0.6344
[Epoch 31] loss: 0.27682789133818786 acc: 0.6306
[Epoch 35] loss: 0.23170816679925793 acc: 0.6368
[Epoch 39] loss: 0.2314820147650626 acc: 0.6346
[Epoch 43] loss: 0.1979759001742353 acc: 0.6247
[Epoch 47] loss: 0.18987391093540984 acc: 0.6373
[Epoch 51] loss: 0.17709011803655064 acc: 0.6274
[Epoch 55] loss: 0.18078728507289574 acc: 0.6251
[Epoch 59] loss: 0.16229867449749613 acc: 0.6254
[Epoch 63] loss: 0.16050127585111257 acc: 0.6315
[Epoch 67] loss: 0.1390592492759571 acc: 0.6237
[Epoch 71] loss: 0.15566596044632403 acc: 0.6257
--> [test] acc: 0.6399
--> [accuracy] finished 0.6399
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6399
--> [reward] 0.6399
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6399 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.342090728642691 acc: 0.6192
[Epoch 7] loss: 3.340291426004961 acc: 0.6593
[Epoch 11] loss: 1.7466135759411565 acc: 0.6566
[Epoch 15] loss: 0.7473791289855453 acc: 0.6652
[Epoch 19] loss: 0.43772760641229963 acc: 0.6601
[Epoch 23] loss: 0.34666569260856533 acc: 0.6528
[Epoch 27] loss: 0.29532388482681093 acc: 0.657
[Epoch 31] loss: 0.2628158935657738 acc: 0.661
[Epoch 35] loss: 0.2279409295724481 acc: 0.6535
[Epoch 39] loss: 0.21423150731436433 acc: 0.6571
[Epoch 43] loss: 0.21433246257188526 acc: 0.6538
[Epoch 47] loss: 0.193395707281092 acc: 0.6596
[Epoch 51] loss: 0.1718605625243081 acc: 0.6571
[Epoch 55] loss: 0.1810660122434044 acc: 0.6515
[Epoch 59] loss: 0.15524095738463847 acc: 0.6506
[Epoch 63] loss: 0.15929427184968414 acc: 0.6541
[Epoch 67] loss: 0.14538523504901155 acc: 0.6549
[Epoch 71] loss: 0.14937288370197804 acc: 0.6563
--> [test] acc: 0.6488
--> [accuracy] finished 0.6488
new state: tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6488
--> [reward] 0.6488
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     1.0     |     1.0      |     5.0     | 0.6488 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.362014360440051 acc: 0.6112
[Epoch 7] loss: 3.3742715749136933 acc: 0.6636
[Epoch 11] loss: 1.7280527013723197 acc: 0.6763
[Epoch 15] loss: 0.7272873806393207 acc: 0.6589
[Epoch 19] loss: 0.4353160417884055 acc: 0.6446
[Epoch 23] loss: 0.34228330242978716 acc: 0.6559
[Epoch 27] loss: 0.2849065323867609 acc: 0.6488
[Epoch 31] loss: 0.26232174212527476 acc: 0.6592
[Epoch 35] loss: 0.2314033534906595 acc: 0.6569
[Epoch 39] loss: 0.2232491380161108 acc: 0.6547
[Epoch 43] loss: 0.206719329147159 acc: 0.6595
[Epoch 47] loss: 0.19499863075542614 acc: 0.6488
[Epoch 51] loss: 0.1717551948426439 acc: 0.6555
[Epoch 55] loss: 0.1834829176616524 acc: 0.6563
[Epoch 59] loss: 0.159752272319107 acc: 0.6531
[Epoch 63] loss: 0.15353989321023911 acc: 0.6373
[Epoch 67] loss: 0.14769353659804482 acc: 0.6571
[Epoch 71] loss: 0.1487960817549578 acc: 0.6564
--> [test] acc: 0.6583
--> [accuracy] finished 0.6583
new state: tensor([768.,   3.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6583
--> [reward] 0.6583
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     3.0      |     1.0     |     1.0      |     5.0     | 0.6583 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.36040151103988 acc: 0.6013
[Epoch 7] loss: 3.333465236074784 acc: 0.6686
[Epoch 11] loss: 1.738624817651251 acc: 0.6711
[Epoch 15] loss: 0.747270171952141 acc: 0.6575
[Epoch 19] loss: 0.4342442882506896 acc: 0.6617
[Epoch 23] loss: 0.3440000792307889 acc: 0.6629
[Epoch 27] loss: 0.29422641062484983 acc: 0.6515
[Epoch 31] loss: 0.25653119324742224 acc: 0.6524
[Epoch 35] loss: 0.2377457443500876 acc: 0.659
[Epoch 39] loss: 0.2176367171928096 acc: 0.6577
[Epoch 43] loss: 0.20738407649585736 acc: 0.6567
[Epoch 47] loss: 0.1911601489767089 acc: 0.654
[Epoch 51] loss: 0.18273316714865015 acc: 0.6588
[Epoch 55] loss: 0.17204408822795542 acc: 0.657
[Epoch 59] loss: 0.15985128929948106 acc: 0.6442
[Epoch 63] loss: 0.1537732546973397 acc: 0.6565
[Epoch 67] loss: 0.15153188110255372 acc: 0.6617
[Epoch 71] loss: 0.14726850428663746 acc: 0.6519
--> [test] acc: 0.6555
--> [accuracy] finished 0.6555
new state: tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6555
--> [reward] 0.6555
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     1.0     |     1.0      |     5.0     | 0.6555 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.438090056562058 acc: 0.5934
[Epoch 7] loss: 3.4559490871246514 acc: 0.6519
[Epoch 11] loss: 1.7658154544661113 acc: 0.6489
[Epoch 15] loss: 0.7410803182655589 acc: 0.6437
[Epoch 19] loss: 0.4378829128716303 acc: 0.6405
[Epoch 23] loss: 0.3409191495060082 acc: 0.6438
[Epoch 27] loss: 0.29199135350658917 acc: 0.6364
[Epoch 31] loss: 0.2578958777015281 acc: 0.6378
[Epoch 35] loss: 0.23167013880722892 acc: 0.6313
[Epoch 39] loss: 0.21750273263019979 acc: 0.6365
[Epoch 43] loss: 0.20112061172323611 acc: 0.6397
[Epoch 47] loss: 0.1897996952697692 acc: 0.6315
[Epoch 51] loss: 0.181396905562657 acc: 0.6321
[Epoch 55] loss: 0.16445593471707934 acc: 0.6438
[Epoch 59] loss: 0.15682879173139685 acc: 0.6337
[Epoch 63] loss: 0.15803612361583488 acc: 0.6318
[Epoch 67] loss: 0.14834379466593534 acc: 0.634
[Epoch 71] loss: 0.14092603710758717 acc: 0.6262
--> [test] acc: 0.6444
--> [accuracy] finished 0.6444
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6444
--> [reward] 0.6444
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6444 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.3255531917447625 acc: 0.593
[Epoch 7] loss: 3.3364763657760133 acc: 0.6598
[Epoch 11] loss: 1.710770169601721 acc: 0.6662
[Epoch 15] loss: 0.734454438595287 acc: 0.6541
[Epoch 19] loss: 0.43680554413524886 acc: 0.6496
[Epoch 23] loss: 0.34017526808306764 acc: 0.6616
[Epoch 27] loss: 0.28362501226841946 acc: 0.6663
[Epoch 31] loss: 0.254708772720507 acc: 0.6417
[Epoch 35] loss: 0.240862291122851 acc: 0.6483
[Epoch 39] loss: 0.22016254867381796 acc: 0.6591
[Epoch 43] loss: 0.19512574993254964 acc: 0.6575
[Epoch 47] loss: 0.19180658052835015 acc: 0.6542
[Epoch 51] loss: 0.1882845232908702 acc: 0.6567
[Epoch 55] loss: 0.16801550852618946 acc: 0.6503
[Epoch 59] loss: 0.1514990555093912 acc: 0.6602
[Epoch 63] loss: 0.16363873840440685 acc: 0.6507
[Epoch 67] loss: 0.1451890825179627 acc: 0.6519
[Epoch 71] loss: 0.14798354763356622 acc: 0.6559
--> [test] acc: 0.6576
--> [accuracy] finished 0.6576
new state: tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6576
--> [reward] 0.6576
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     1.0     |     1.0      |     5.0     | 0.6576 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.394647249480343 acc: 0.606
[Epoch 7] loss: 3.3838057892249367 acc: 0.6336
[Epoch 11] loss: 1.761234599763475 acc: 0.6634
[Epoch 15] loss: 0.7427335584398045 acc: 0.6554
[Epoch 19] loss: 0.4408338211734048 acc: 0.6622
[Epoch 23] loss: 0.34090135538059735 acc: 0.6541
[Epoch 27] loss: 0.2882761987964706 acc: 0.6596
[Epoch 31] loss: 0.26906562340981743 acc: 0.649
[Epoch 35] loss: 0.22585824890838713 acc: 0.6561
[Epoch 39] loss: 0.2340319893463417 acc: 0.6551
[Epoch 43] loss: 0.20130301409346216 acc: 0.6542
[Epoch 47] loss: 0.19720944754131461 acc: 0.6529
[Epoch 51] loss: 0.16924750354126705 acc: 0.652
[Epoch 55] loss: 0.17405909416087142 acc: 0.6512
[Epoch 59] loss: 0.1634250848858958 acc: 0.6461
[Epoch 63] loss: 0.1575588441973128 acc: 0.6504
[Epoch 67] loss: 0.15185243587421676 acc: 0.654
[Epoch 71] loss: 0.14973808590279858 acc: 0.6527
--> [test] acc: 0.6539
--> [accuracy] finished 0.6539
new state: tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6539
--> [reward] 0.6539
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     1.0     |     1.0      |     5.0     | 0.6539 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0914, 0.0910, 0.0908, 0.0912, 0.0899,
         0.0904, 0.0903]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3974, -2.3978, -2.3980, -2.3976,
         -2.3989, -2.3984, -2.3985]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   3.,   1.,   1.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.608122222716241 acc: 0.5817
[Epoch 7] loss: 3.762140279535747 acc: 0.6327
[Epoch 11] loss: 2.1491405354131516 acc: 0.6419
[Epoch 15] loss: 0.9490714937238894 acc: 0.626
[Epoch 19] loss: 0.5240791248838843 acc: 0.6155
[Epoch 23] loss: 0.40472588581902447 acc: 0.6228
[Epoch 27] loss: 0.33571466886679 acc: 0.6295
[Epoch 31] loss: 0.3090318622415328 acc: 0.6158
[Epoch 35] loss: 0.2664526343736274 acc: 0.6252
[Epoch 39] loss: 0.23424803709749448 acc: 0.6183
[Epoch 43] loss: 0.2406165914829163 acc: 0.6144
[Epoch 47] loss: 0.21085523233375014 acc: 0.6108
[Epoch 51] loss: 0.21526678047759834 acc: 0.619
[Epoch 55] loss: 0.17602357510691674 acc: 0.6167
[Epoch 59] loss: 0.19480646117244993 acc: 0.6191
[Epoch 63] loss: 0.16345455877773482 acc: 0.625
[Epoch 67] loss: 0.18243639917610704 acc: 0.6188
[Epoch 71] loss: 0.15201248474242857 acc: 0.6194
--> [test] acc: 0.6178
--> [accuracy] finished 0.6178
new state: tensor([736.,   3.,   1.,   1.,   6.], device='cuda:0')
new reward: 0.6178
--> [reward] 0.6178
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1902]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3805]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6168]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6985]], device='cuda:0')
------ ------
delta_t: tensor([[0.6168]], device='cuda:0')
rewards[i]: 0.6178
values[i+1]: tensor([[0.0816]], device='cuda:0')
values[i]: tensor([[0.0817]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6168]], device='cuda:0')
delta_t: tensor([[0.6168]], device='cuda:0')
------ ------
policy_loss: 1.4554667472839355
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6168]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.9886]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.5968]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2636]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3455]], device='cuda:0')
------ ------
delta_t: tensor([[0.6530]], device='cuda:0')
rewards[i]: 0.6539
values[i+1]: tensor([[0.0817]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0818]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2636]], device='cuda:0')
delta_t: tensor([[0.6530]], device='cuda:0')
------ ------
policy_loss: 4.462310314178467
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2636]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.8081]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.6390]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9076]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9896]], device='cuda:0')
------ ------
delta_t: tensor([[0.6566]], device='cuda:0')
rewards[i]: 0.6576
values[i+1]: tensor([[0.0818]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0820]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9076]], device='cuda:0')
delta_t: tensor([[0.6566]], device='cuda:0')
------ ------
policy_loss: 9.011960983276367
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9076]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.0147]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.4132]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5324]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.6141]], device='cuda:0')
------ ------
delta_t: tensor([[0.6439]], device='cuda:0')
rewards[i]: 0.6444
values[i+1]: tensor([[0.0820]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0817]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5324]], device='cuda:0')
delta_t: tensor([[0.6439]], device='cuda:0')
------ ------
policy_loss: 15.062755584716797
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5324]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[11.0122]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.9951]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.1615]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2435]], device='cuda:0')
------ ------
delta_t: tensor([[0.6544]], device='cuda:0')
rewards[i]: 0.6555
values[i+1]: tensor([[0.0817]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0820]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.1615]], device='cuda:0')
delta_t: tensor([[0.6544]], device='cuda:0')
------ ------
policy_loss: 22.61817169189453
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.1615]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[18.1851]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.3457]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.7876]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.8693]], device='cuda:0')
------ ------
delta_t: tensor([[0.6577]], device='cuda:0')
rewards[i]: 0.6583
values[i+1]: tensor([[0.0820]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0818]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.7876]], device='cuda:0')
delta_t: tensor([[0.6577]], device='cuda:0')
------ ------
policy_loss: 31.670459747314453
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.7876]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[27.8535]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.3367]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3974]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.4794]], device='cuda:0')
------ ------
delta_t: tensor([[0.6477]], device='cuda:0')
rewards[i]: 0.6488
values[i+1]: tensor([[0.0818]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0821]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3974]], device='cuda:0')
delta_t: tensor([[0.6477]], device='cuda:0')
------ ------
policy_loss: 42.18947982788086
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3974]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[40.3150]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.9231]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9923]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0745]], device='cuda:0')
------ ------
delta_t: tensor([[0.6389]], device='cuda:0')
rewards[i]: 0.6399
values[i+1]: tensor([[0.0821]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0822]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9923]], device='cuda:0')
delta_t: tensor([[0.6389]], device='cuda:0')
------ ------
policy_loss: 54.13493728637695
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9923]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[55.9498]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[31.2696]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.5919]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.6740]], device='cuda:0')
------ ------
delta_t: tensor([[0.6495]], device='cuda:0')
rewards[i]: 0.6502
values[i+1]: tensor([[0.0822]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0821]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.5919]], device='cuda:0')
delta_t: tensor([[0.6495]], device='cuda:0')
------ ------
policy_loss: 67.52064514160156
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.5919]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[75.0370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[38.1743]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1785]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2604]], device='cuda:0')
------ ------
delta_t: tensor([[0.6425]], device='cuda:0')
rewards[i]: 0.6431
values[i+1]: tensor([[0.0821]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0818]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1785]], device='cuda:0')
delta_t: tensor([[0.6425]], device='cuda:0')
------ ------
policy_loss: 82.3159408569336
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1785]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[97.9309]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.7877]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7667]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8481]], device='cuda:0')
------ ------
delta_t: tensor([[0.6499]], device='cuda:0')
rewards[i]: 0.6503
values[i+1]: tensor([[0.0818]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0814]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7667]], device='cuda:0')
delta_t: tensor([[0.6499]], device='cuda:0')
------ ------
policy_loss: 98.51435089111328
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7667]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[125.3243]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[54.7870]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4018]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.4829]], device='cuda:0')
------ ------
delta_t: tensor([[0.7028]], device='cuda:0')
rewards[i]: 0.7033
values[i+1]: tensor([[0.0814]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0811]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4018]], device='cuda:0')
delta_t: tensor([[0.7028]], device='cuda:0')
------ ------
policy_loss: 116.24580383300781
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4018]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[157.4755]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[64.3023]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0189]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.0998]], device='cuda:0')
------ ------
delta_t: tensor([[0.6911]], device='cuda:0')
rewards[i]: 0.6917
values[i+1]: tensor([[0.0811]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0809]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0189]], device='cuda:0')
delta_t: tensor([[0.6911]], device='cuda:0')
------ ------
policy_loss: 135.45143127441406
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0189]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[194.6439]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.3369]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6219]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7025]], device='cuda:0')
------ ------
delta_t: tensor([[0.6832]], device='cuda:0')
rewards[i]: 0.6837
values[i+1]: tensor([[0.0809]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0806]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6219]], device='cuda:0')
delta_t: tensor([[0.6832]], device='cuda:0')
------ ------
policy_loss: 156.1009979248047
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6219]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[236.6973]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[84.1067]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.1710]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2506]], device='cuda:0')
------ ------
delta_t: tensor([[0.6353]], device='cuda:0')
rewards[i]: 0.6352
values[i+1]: tensor([[0.0806]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0797]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.1710]], device='cuda:0')
delta_t: tensor([[0.6353]], device='cuda:0')
------ ------
policy_loss: 178.06349182128906
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.1710]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 178.06349182128906
value_loss: 236.6973114013672
loss: 296.4121398925781



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-6.3300e-03, -3.5730e-05, -1.0644e-05, -9.8472e-06, -4.3056e-05],
        [ 1.6123e-01,  9.0724e-04,  2.6929e-04,  2.5452e-04,  1.0970e-03],
        [-1.4548e-03, -8.2312e-06, -2.4791e-06, -2.2307e-06, -9.8930e-06],
        [ 7.2880e-01,  4.0966e-03,  1.2212e-03,  1.1536e-03,  4.9600e-03],
        [ 3.5042e+00,  1.9722e-02,  5.8771e-03,  5.5339e-03,  2.3849e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 7.0536e-05,  5.3260e-05, -2.7255e-05, -5.5350e-05,  5.5061e-05],
        [-1.7958e-03, -1.3569e-03,  6.9312e-04,  1.4103e-03, -1.4016e-03],
        [ 1.6217e-05,  1.2232e-05, -6.2745e-06, -1.2711e-05,  1.2662e-05],
        [-8.1166e-03, -6.1329e-03,  3.1322e-03,  6.3743e-03, -6.3346e-03],
        [-3.9026e-02, -2.9482e-02,  1.5065e-02,  3.0642e-02, -3.0459e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.0502, -1.0311,  1.0492, -0.9222,  0.6570],
        [-0.3525,  0.3461, -0.3522,  0.3096, -0.2205],
        [-0.4629,  0.4545, -0.4625,  0.4064, -0.2896],
        [-0.0273,  0.0268, -0.0273,  0.0240, -0.0171],
        [-0.2075,  0.2037, -0.2073,  0.1822, -0.1298]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0269, -0.0200,  0.0108,  0.0207, -0.0210],
        [-0.0228, -0.0171,  0.0089,  0.0177, -0.0178],
        [ 0.0349,  0.0254, -0.0142, -0.0265,  0.0269],
        [ 0.0235,  0.0175, -0.0093, -0.0182,  0.0187],
        [ 0.0684,  0.0516, -0.0264, -0.0534,  0.0536],
        [ 0.0138,  0.0105, -0.0049, -0.0109,  0.0108],
        [ 0.0514,  0.0381, -0.0204, -0.0395,  0.0401],
        [-0.0511, -0.0380,  0.0201,  0.0395, -0.0400],
        [-0.0503, -0.0374,  0.0198,  0.0389, -0.0393],
        [-0.0460, -0.0342,  0.0181,  0.0356, -0.0359],
        [ 0.0053,  0.0037, -0.0024, -0.0038,  0.0040]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.0826,  2.2918, -1.2108, -2.3808,  2.4085]], device='cuda:0')
--> [loss] 296.4121398925781

---------------------------------- [[#21 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     1.0     |     1.0      |     6.0     | 0.6178 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   1.,   1.,   6.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   4.,   1.,   1.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.739008299834893 acc: 0.5535
[Epoch 7] loss: 3.890728770619463 acc: 0.6187
[Epoch 11] loss: 2.2114271675534263 acc: 0.6103
[Epoch 15] loss: 0.9979610881098854 acc: 0.5994
[Epoch 19] loss: 0.5621356040267916 acc: 0.6106
[Epoch 23] loss: 0.4082606874044289 acc: 0.5984
[Epoch 27] loss: 0.3469027583737431 acc: 0.6012
[Epoch 31] loss: 0.30869985317997156 acc: 0.5938
[Epoch 35] loss: 0.2707801053700659 acc: 0.6066
[Epoch 39] loss: 0.247196034742686 acc: 0.6004
[Epoch 43] loss: 0.23095417556965062 acc: 0.6034
[Epoch 47] loss: 0.22425450839083214 acc: 0.6008
[Epoch 51] loss: 0.19051889735786126 acc: 0.6075
[Epoch 55] loss: 0.20174182323105347 acc: 0.594
[Epoch 59] loss: 0.19329532573614128 acc: 0.6042
[Epoch 63] loss: 0.1732264812523618 acc: 0.6003
[Epoch 67] loss: 0.16544438684877255 acc: 0.606
[Epoch 71] loss: 0.158493610170415 acc: 0.5954
--> [test] acc: 0.5935
--> [accuracy] finished 0.5935
new state: tensor([736.,   4.,   1.,   1.,   6.], device='cuda:0')
new reward: 0.5935
--> [reward] 0.5935
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     6.0     | 0.5935 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   6.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   5.,   1.,   1.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.511798000091787 acc: 0.5897
[Epoch 7] loss: 3.619146304514707 acc: 0.632
[Epoch 11] loss: 1.883015667957723 acc: 0.6351
[Epoch 15] loss: 0.7798127451592394 acc: 0.6298
[Epoch 19] loss: 0.46991005933383845 acc: 0.6196
[Epoch 23] loss: 0.358208617007317 acc: 0.6088
[Epoch 27] loss: 0.30375547752098736 acc: 0.63
[Epoch 31] loss: 0.2623011791707038 acc: 0.6189
[Epoch 35] loss: 0.23900553214904444 acc: 0.6239
[Epoch 39] loss: 0.22622638260064376 acc: 0.6121
[Epoch 43] loss: 0.19627127440913064 acc: 0.6106
[Epoch 47] loss: 0.19048554652377658 acc: 0.6199
[Epoch 51] loss: 0.18077408200691994 acc: 0.6042
[Epoch 55] loss: 0.16794455345824857 acc: 0.6077
[Epoch 59] loss: 0.1545879381704037 acc: 0.6193
[Epoch 63] loss: 0.15843824909814178 acc: 0.6182
[Epoch 67] loss: 0.15214589408948978 acc: 0.6154
[Epoch 71] loss: 0.13673920612877516 acc: 0.6068
--> [test] acc: 0.6156
--> [accuracy] finished 0.6156
new state: tensor([736.,   5.,   1.,   1.,   6.], device='cuda:0')
new reward: 0.6156
--> [reward] 0.6156
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     6.0     | 0.6156 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   6.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   5.,   1.,   1.,   7.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.913003081555867 acc: 0.5269
[Epoch 7] loss: 4.2041996757087805 acc: 0.5931
[Epoch 11] loss: 2.494273484972737 acc: 0.5741
[Epoch 15] loss: 1.1640917547142413 acc: 0.5712
[Epoch 19] loss: 0.6485764579585446 acc: 0.5623
[Epoch 23] loss: 0.465859987942116 acc: 0.5679
[Epoch 27] loss: 0.382860135368031 acc: 0.5637
[Epoch 31] loss: 0.34467176367264346 acc: 0.5694
[Epoch 35] loss: 0.3030236420385978 acc: 0.5648
[Epoch 39] loss: 0.2723726875522672 acc: 0.5688
[Epoch 43] loss: 0.26435327567541234 acc: 0.5649
[Epoch 47] loss: 0.22571715969077843 acc: 0.5597
[Epoch 51] loss: 0.2395613350550575 acc: 0.5631
[Epoch 55] loss: 0.21131483862733902 acc: 0.5551
[Epoch 59] loss: 0.19900400317190667 acc: 0.5568
[Epoch 63] loss: 0.19876683640854714 acc: 0.5571
[Epoch 67] loss: 0.19625574805061607 acc: 0.5569
[Epoch 71] loss: 0.173428833027325 acc: 0.5683
--> [test] acc: 0.5652
--> [accuracy] finished 0.5652
new state: tensor([736.,   5.,   1.,   1.,   7.], device='cuda:0')
new reward: 0.5652
--> [reward] 0.5652
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     7.0     | 0.5652 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   7.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([736.,   5.,   1.,   1.,   7.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.934516909482229 acc: 0.533
[Epoch 7] loss: 4.270024030562252 acc: 0.574
[Epoch 11] loss: 2.585842808646619 acc: 0.5603
[Epoch 15] loss: 1.2071957354198026 acc: 0.5723
[Epoch 19] loss: 0.6497512145010788 acc: 0.5669
[Epoch 23] loss: 0.46592530579118013 acc: 0.564
[Epoch 27] loss: 0.39635940110001266 acc: 0.5671
[Epoch 31] loss: 0.3311772426909498 acc: 0.5615
[Epoch 35] loss: 0.30676245785144435 acc: 0.5559
[Epoch 39] loss: 0.27663158993844106 acc: 0.5637
[Epoch 43] loss: 0.2562924548709655 acc: 0.5654
[Epoch 47] loss: 0.24215697480932527 acc: 0.5631
[Epoch 51] loss: 0.2271904825564007 acc: 0.5665
[Epoch 55] loss: 0.2092416375503897 acc: 0.5679
[Epoch 59] loss: 0.1952759603043194 acc: 0.5672
[Epoch 63] loss: 0.20275136715758715 acc: 0.5649
[Epoch 67] loss: 0.18227652187013757 acc: 0.5646
[Epoch 71] loss: 0.17119063904790013 acc: 0.5429
--> [test] acc: 0.5621
--> [accuracy] finished 0.5621
new state: tensor([736.,   5.,   1.,   1.,   7.], device='cuda:0')
new reward: 0.5621
--> [reward] 0.5621
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     7.0     | 0.5621 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   7.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   5.,   1.,   1.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.495212083887261 acc: 0.575
[Epoch 7] loss: 3.6349778447462167 acc: 0.6302
[Epoch 11] loss: 1.9006841150696014 acc: 0.6242
[Epoch 15] loss: 0.7911673762723613 acc: 0.6267
[Epoch 19] loss: 0.47563436505434764 acc: 0.623
[Epoch 23] loss: 0.35224685269643735 acc: 0.6197
[Epoch 27] loss: 0.30568216330445636 acc: 0.6279
[Epoch 31] loss: 0.26431849426435083 acc: 0.6201
[Epoch 35] loss: 0.23683101629071376 acc: 0.6176
[Epoch 39] loss: 0.22078376503951866 acc: 0.6108
[Epoch 43] loss: 0.20029713613602818 acc: 0.6172
[Epoch 47] loss: 0.19258091940789882 acc: 0.6049
[Epoch 51] loss: 0.18176125205548294 acc: 0.6239
[Epoch 55] loss: 0.1663908088005736 acc: 0.6152
[Epoch 59] loss: 0.16319249220826018 acc: 0.6121
[Epoch 63] loss: 0.15243536670265906 acc: 0.6143
[Epoch 67] loss: 0.14159391675849475 acc: 0.6178
[Epoch 71] loss: 0.15024109753539494 acc: 0.611
--> [test] acc: 0.6159
--> [accuracy] finished 0.6159
new state: tensor([736.,   5.,   1.,   1.,   6.], device='cuda:0')
new reward: 0.6159
--> [reward] 0.6159
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     1.0      |     6.0     | 0.6159 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   1.,   6.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.705063150057097 acc: 0.5574
[Epoch 7] loss: 3.8805501140901804 acc: 0.6191
[Epoch 11] loss: 2.2214879538397043 acc: 0.6149
[Epoch 15] loss: 1.0072334526616442 acc: 0.6172
[Epoch 19] loss: 0.5578864753065284 acc: 0.6064
[Epoch 23] loss: 0.41589753722290856 acc: 0.6078
[Epoch 27] loss: 0.3582033175603508 acc: 0.604
[Epoch 31] loss: 0.3003736894382426 acc: 0.6043
[Epoch 35] loss: 0.2689313855846329 acc: 0.5977
[Epoch 39] loss: 0.24892682787697867 acc: 0.6025
[Epoch 43] loss: 0.24035420010576164 acc: 0.6004
[Epoch 47] loss: 0.22145633130093745 acc: 0.6087
[Epoch 51] loss: 0.19898837162396105 acc: 0.606
[Epoch 55] loss: 0.20304331585379018 acc: 0.5981
[Epoch 59] loss: 0.17851832463784748 acc: 0.6022
[Epoch 63] loss: 0.17642525449404708 acc: 0.6021
[Epoch 67] loss: 0.17135350321612472 acc: 0.5999
[Epoch 71] loss: 0.1668673021423504 acc: 0.611
--> [test] acc: 0.6004
--> [accuracy] finished 0.6004
new state: tensor([736.,   4.,   1.,   1.,   6.], device='cuda:0')
new reward: 0.6004
--> [reward] 0.6004
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     6.0     | 0.6004 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   6.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.462035377617077 acc: 0.5846
[Epoch 7] loss: 3.500261662713707 acc: 0.6334
[Epoch 11] loss: 1.8298377778447803 acc: 0.6451
[Epoch 15] loss: 0.7822492047763236 acc: 0.6405
[Epoch 19] loss: 0.45273871622655704 acc: 0.6398
[Epoch 23] loss: 0.3499713984063214 acc: 0.6306
[Epoch 27] loss: 0.29564785564323065 acc: 0.6362
[Epoch 31] loss: 0.25634801255829653 acc: 0.6397
[Epoch 35] loss: 0.24782768913480402 acc: 0.6319
[Epoch 39] loss: 0.20688309212027076 acc: 0.6337
[Epoch 43] loss: 0.21352837469114366 acc: 0.6447
[Epoch 47] loss: 0.1804152332613116 acc: 0.6392
[Epoch 51] loss: 0.17231195213635217 acc: 0.6329
[Epoch 55] loss: 0.1793914326726247 acc: 0.6238
[Epoch 59] loss: 0.167598411022826 acc: 0.6334
[Epoch 63] loss: 0.14827686247875546 acc: 0.6237
[Epoch 67] loss: 0.144034750207001 acc: 0.6393
[Epoch 71] loss: 0.13173244355241662 acc: 0.6288
--> [test] acc: 0.6287
--> [accuracy] finished 0.6287
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6287
--> [reward] 0.6287
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6287 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.443045177423131 acc: 0.5937
[Epoch 7] loss: 3.532936594751485 acc: 0.6587
[Epoch 11] loss: 1.8489780897450874 acc: 0.6493
[Epoch 15] loss: 0.7802591402264659 acc: 0.64
[Epoch 19] loss: 0.45444623900629827 acc: 0.6352
[Epoch 23] loss: 0.35756518282091526 acc: 0.6328
[Epoch 27] loss: 0.2983574432837765 acc: 0.6286
[Epoch 31] loss: 0.27105890395705734 acc: 0.6341
[Epoch 35] loss: 0.23394480191142586 acc: 0.6446
[Epoch 39] loss: 0.22618855372109375 acc: 0.6355
[Epoch 43] loss: 0.19960189123144922 acc: 0.6328
[Epoch 47] loss: 0.1974742996978009 acc: 0.6329
[Epoch 51] loss: 0.19025298881718455 acc: 0.6354
[Epoch 55] loss: 0.1578535140704011 acc: 0.6274
[Epoch 59] loss: 0.16773139108972782 acc: 0.6301
[Epoch 63] loss: 0.15840344665491063 acc: 0.631
[Epoch 67] loss: 0.15848224660348328 acc: 0.6267
[Epoch 71] loss: 0.14037796418013437 acc: 0.6277
--> [test] acc: 0.628
--> [accuracy] finished 0.628
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.628
--> [reward] 0.628
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.628  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.433179441162997 acc: 0.5912
[Epoch 7] loss: 3.493719245085631 acc: 0.648
[Epoch 11] loss: 1.8329031277266914 acc: 0.6463
[Epoch 15] loss: 0.767605556075073 acc: 0.6424
[Epoch 19] loss: 0.441740194002114 acc: 0.6345
[Epoch 23] loss: 0.3495056378176374 acc: 0.634
[Epoch 27] loss: 0.2983051065207862 acc: 0.6413
[Epoch 31] loss: 0.2549963346218019 acc: 0.6376
[Epoch 35] loss: 0.24262713059030303 acc: 0.6382
[Epoch 39] loss: 0.2098633632078157 acc: 0.6371
[Epoch 43] loss: 0.2056498620682932 acc: 0.6301
[Epoch 47] loss: 0.19105598556301784 acc: 0.6244
[Epoch 51] loss: 0.17967636934231462 acc: 0.6281
[Epoch 55] loss: 0.16433294429955886 acc: 0.6319
[Epoch 59] loss: 0.16538176931383664 acc: 0.6344
[Epoch 63] loss: 0.1506261172239333 acc: 0.6285
[Epoch 67] loss: 0.15666265811418634 acc: 0.6414
[Epoch 71] loss: 0.14600993632910597 acc: 0.6303
--> [test] acc: 0.6363
--> [accuracy] finished 0.6363
new state: tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
new reward: 0.6363
--> [reward] 0.6363
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     1.0      |     5.0     | 0.6363 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.695989328119761 acc: 0.5437
[Epoch 7] loss: 3.850760664857562 acc: 0.6247
[Epoch 11] loss: 2.255443381036029 acc: 0.6114
[Epoch 15] loss: 1.0618442345096175 acc: 0.6116
[Epoch 19] loss: 0.5681293109703399 acc: 0.6278
[Epoch 23] loss: 0.41193522105131614 acc: 0.6023
[Epoch 27] loss: 0.3545411663782566 acc: 0.606
[Epoch 31] loss: 0.30079481675935066 acc: 0.6013
[Epoch 35] loss: 0.2711129241773044 acc: 0.604
[Epoch 39] loss: 0.2549564528738713 acc: 0.6057
[Epoch 43] loss: 0.23094300450304586 acc: 0.6042
[Epoch 47] loss: 0.21509348554417607 acc: 0.5994
[Epoch 51] loss: 0.20932240010289205 acc: 0.6083
[Epoch 55] loss: 0.19161909518529996 acc: 0.5939
[Epoch 59] loss: 0.1853518953685032 acc: 0.5974
[Epoch 63] loss: 0.17654541750197938 acc: 0.6087
[Epoch 67] loss: 0.16452584102097184 acc: 0.6019
[Epoch 71] loss: 0.16364109603321308 acc: 0.6084
--> [test] acc: 0.6057
--> [accuracy] finished 0.6057
new state: tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.6057
--> [reward] 0.6057
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     2.0      |     5.0     | 0.6057 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   5.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.440994066807925 acc: 0.5926
[Epoch 7] loss: 3.456745820353403 acc: 0.6494
[Epoch 11] loss: 1.7524110826537431 acc: 0.6412
[Epoch 15] loss: 0.7574103690988725 acc: 0.6264
[Epoch 19] loss: 0.451167336757988 acc: 0.6353
[Epoch 23] loss: 0.34011087693092995 acc: 0.6393
[Epoch 27] loss: 0.2975588350787835 acc: 0.6347
[Epoch 31] loss: 0.25930498862910606 acc: 0.6369
[Epoch 35] loss: 0.23761658325953328 acc: 0.6272
[Epoch 39] loss: 0.21038844367093823 acc: 0.6347
[Epoch 43] loss: 0.19023080644152507 acc: 0.6333
[Epoch 47] loss: 0.19476899611132453 acc: 0.6233
[Epoch 51] loss: 0.1790608157453787 acc: 0.6317
[Epoch 55] loss: 0.15386248312657103 acc: 0.6228
[Epoch 59] loss: 0.1593660338736518 acc: 0.6274
[Epoch 63] loss: 0.14349106271677387 acc: 0.639
[Epoch 67] loss: 0.14756005642044803 acc: 0.6312
[Epoch 71] loss: 0.14175524161485456 acc: 0.6315
--> [test] acc: 0.6355
--> [accuracy] finished 0.6355
new state: tensor([736.,   5.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.6355
--> [reward] 0.6355
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     1.0     |     2.0      |     5.0     | 0.6355 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.665766873018211 acc: 0.5568
[Epoch 7] loss: 3.817107367698494 acc: 0.6201
[Epoch 11] loss: 2.21461069305687 acc: 0.6173
[Epoch 15] loss: 1.0358213244287102 acc: 0.6106
[Epoch 19] loss: 0.5656539288104114 acc: 0.6071
[Epoch 23] loss: 0.41681656575359194 acc: 0.5931
[Epoch 27] loss: 0.3565957476777951 acc: 0.6001
[Epoch 31] loss: 0.30135782068247535 acc: 0.6038
[Epoch 35] loss: 0.2744574330108779 acc: 0.61
[Epoch 39] loss: 0.24835777770527792 acc: 0.6
[Epoch 43] loss: 0.23295474069340683 acc: 0.6049
[Epoch 47] loss: 0.22170283830584125 acc: 0.6065
[Epoch 51] loss: 0.2031517067728826 acc: 0.6043
[Epoch 55] loss: 0.1915796643261181 acc: 0.5937
[Epoch 59] loss: 0.1866896570614918 acc: 0.5994
[Epoch 63] loss: 0.18792145372227864 acc: 0.6008
[Epoch 67] loss: 0.15969866357953347 acc: 0.598
[Epoch 71] loss: 0.16550647910467595 acc: 0.5988
--> [test] acc: 0.5973
--> [accuracy] finished 0.5973
new state: tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.5973
--> [reward] 0.5973
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     2.0      |     5.0     | 0.5973 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   4.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.596624340242742 acc: 0.5685
[Epoch 7] loss: 3.7396313949009343 acc: 0.6386
[Epoch 11] loss: 2.167345319791218 acc: 0.6278
[Epoch 15] loss: 1.0126288531305234 acc: 0.6205
[Epoch 19] loss: 0.5539098883100102 acc: 0.6189
[Epoch 23] loss: 0.4044423786365925 acc: 0.6246
[Epoch 27] loss: 0.3358034539438994 acc: 0.6169
[Epoch 31] loss: 0.30303979470678 acc: 0.6171
[Epoch 35] loss: 0.25934301785257696 acc: 0.6205
[Epoch 39] loss: 0.2428537721809981 acc: 0.613
[Epoch 43] loss: 0.23335879992531694 acc: 0.611
[Epoch 47] loss: 0.20851738194284766 acc: 0.6249
[Epoch 51] loss: 0.20181668403646563 acc: 0.6192
[Epoch 55] loss: 0.1809285483525499 acc: 0.6222
[Epoch 59] loss: 0.18274354524052966 acc: 0.6255
[Epoch 63] loss: 0.1736687561878196 acc: 0.6221
[Epoch 67] loss: 0.16283099154901245 acc: 0.6085
[Epoch 71] loss: 0.1506554825633974 acc: 0.6221
--> [test] acc: 0.6174
--> [accuracy] finished 0.6174
new state: tensor([736.,   4.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6174
--> [reward] 0.6174
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     1.0     |     2.0      |     4.0     | 0.6174 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   4.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.561086904667222 acc: 0.5718
[Epoch 7] loss: 3.6687221030902375 acc: 0.6297
[Epoch 11] loss: 2.075674217870778 acc: 0.6279
[Epoch 15] loss: 0.9510654754307873 acc: 0.6335
[Epoch 19] loss: 0.5129841572707495 acc: 0.6242
[Epoch 23] loss: 0.3946515448734431 acc: 0.6238
[Epoch 27] loss: 0.3276653355916443 acc: 0.6242
[Epoch 31] loss: 0.2861619914698479 acc: 0.6194
[Epoch 35] loss: 0.2598412726455561 acc: 0.6251
[Epoch 39] loss: 0.23786493307432097 acc: 0.6207
[Epoch 43] loss: 0.229004177245814 acc: 0.6194
[Epoch 47] loss: 0.20332233553223522 acc: 0.6188
[Epoch 51] loss: 0.19897154773659337 acc: 0.6203
[Epoch 55] loss: 0.18234610472284163 acc: 0.6201
[Epoch 59] loss: 0.1785801843385143 acc: 0.6176
[Epoch 63] loss: 0.15854110822731823 acc: 0.6153
[Epoch 67] loss: 0.1576067185817796 acc: 0.6199
[Epoch 71] loss: 0.15655994898808734 acc: 0.617
--> [test] acc: 0.6111
--> [accuracy] finished 0.6111
new state: tensor([768.,   4.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6111
--> [reward] 0.6111
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     1.0     |     2.0      |     4.0     | 0.6111 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0900, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3988, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([768.,   4.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.201176970206258 acc: 0.6287
[Epoch 7] loss: 2.9277102632443315 acc: 0.676
[Epoch 11] loss: 1.196432462033561 acc: 0.6877
[Epoch 15] loss: 0.5017946450816243 acc: 0.6811
[Epoch 19] loss: 0.32352405413747065 acc: 0.6775
[Epoch 23] loss: 0.26423782566824305 acc: 0.6835
[Epoch 27] loss: 0.21790909638051945 acc: 0.6827
[Epoch 31] loss: 0.197085101814831 acc: 0.6829
[Epoch 35] loss: 0.1776579224606952 acc: 0.6831
[Epoch 39] loss: 0.15800138099638322 acc: 0.6767
[Epoch 43] loss: 0.1501931814819841 acc: 0.674
[Epoch 47] loss: 0.13694088344115055 acc: 0.6736
[Epoch 51] loss: 0.1361053273286623 acc: 0.6689
[Epoch 55] loss: 0.12168207707459969 acc: 0.6701
[Epoch 59] loss: 0.11799250598496679 acc: 0.68
[Epoch 63] loss: 0.12120598463384467 acc: 0.6718
[Epoch 67] loss: 0.11129427403556968 acc: 0.6708
[Epoch 71] loss: 0.10258086384369342 acc: 0.6642
--> [test] acc: 0.6717
--> [accuracy] finished 0.6717
new state: tensor([768.,   4.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6717
--> [reward] 0.6717
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2250]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4499]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6708]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7590]], device='cuda:0')
------ ------
delta_t: tensor([[0.6708]], device='cuda:0')
rewards[i]: 0.6717
values[i+1]: tensor([[0.0882]], device='cuda:0')
values[i]: tensor([[0.0882]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6708]], device='cuda:0')
delta_t: tensor([[0.6708]], device='cuda:0')
------ ------
policy_loss: 1.58437979221344
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6708]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0374]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6249]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2747]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3625]], device='cuda:0')
------ ------
delta_t: tensor([[0.6107]], device='cuda:0')
rewards[i]: 0.6111
values[i+1]: tensor([[0.0882]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0878]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2747]], device='cuda:0')
delta_t: tensor([[0.6107]], device='cuda:0')
------ ------
policy_loss: 4.614972114562988
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2747]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.8017]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.5286]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8785]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9663]], device='cuda:0')
------ ------
delta_t: tensor([[0.6165]], device='cuda:0')
rewards[i]: 0.6174
values[i+1]: tensor([[0.0878]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0878]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8785]], device='cuda:0')
delta_t: tensor([[0.6165]], device='cuda:0')
------ ------
policy_loss: 9.097450256347656
log_probs[i]: tensor([[-2.3990]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8785]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.8176]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.0318]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.4560]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5439]], device='cuda:0')
------ ------
delta_t: tensor([[0.5963]], device='cuda:0')
rewards[i]: 0.5973
values[i+1]: tensor([[0.0878]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0880]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.4560]], device='cuda:0')
delta_t: tensor([[0.5963]], device='cuda:0')
------ ------
policy_loss: 14.961908340454102
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.4560]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[10.5188]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.4023]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.0663]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1540]], device='cuda:0')
------ ------
delta_t: tensor([[0.6349]], device='cuda:0')
rewards[i]: 0.6355
values[i+1]: tensor([[0.0880]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0877]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.0663]], device='cuda:0')
delta_t: tensor([[0.6349]], device='cuda:0')
------ ------
policy_loss: 22.2933349609375
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.0663]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[17.1453]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.2532]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6405]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7282]], device='cuda:0')
------ ------
delta_t: tensor([[0.6048]], device='cuda:0')
rewards[i]: 0.6057
values[i+1]: tensor([[0.0877]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0877]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6405]], device='cuda:0')
delta_t: tensor([[0.6048]], device='cuda:0')
------ ------
policy_loss: 30.997875213623047
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6405]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[26.1322]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.9737]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2395]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3272]], device='cuda:0')
------ ------
delta_t: tensor([[0.6355]], device='cuda:0')
rewards[i]: 0.6363
values[i+1]: tensor([[0.0877]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2395]], device='cuda:0')
delta_t: tensor([[0.6355]], device='cuda:0')
------ ------
policy_loss: 41.140403747558594
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2395]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.7690]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.2738]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8243]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.9119]], device='cuda:0')
------ ------
delta_t: tensor([[0.6271]], device='cuda:0')
rewards[i]: 0.628
values[i+1]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8243]], device='cuda:0')
delta_t: tensor([[0.6271]], device='cuda:0')
------ ------
policy_loss: 52.68779754638672
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8243]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[52.3699]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.2017]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4039]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4915]], device='cuda:0')
------ ------
delta_t: tensor([[0.6278]], device='cuda:0')
rewards[i]: 0.6287
values[i+1]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4039]], device='cuda:0')
delta_t: tensor([[0.6278]], device='cuda:0')
------ ------
policy_loss: 65.62773895263672
log_probs[i]: tensor([[-2.3990]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4039]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[70.0663]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[35.3929]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.9492]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.0370]], device='cuda:0')
------ ------
delta_t: tensor([[0.5994]], device='cuda:0')
rewards[i]: 0.6004
values[i+1]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0878]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.9492]], device='cuda:0')
delta_t: tensor([[0.5994]], device='cuda:0')
------ ------
policy_loss: 79.86754608154297
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.9492]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[91.2233]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[42.3139]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.5049]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.5925]], device='cuda:0')
------ ------
delta_t: tensor([[0.6152]], device='cuda:0')
rewards[i]: 0.6159
values[i+1]: tensor([[0.0878]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.5049]], device='cuda:0')
delta_t: tensor([[0.6152]], device='cuda:0')
------ ------
policy_loss: 95.44892120361328
log_probs[i]: tensor([[-2.3990]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.5049]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[115.7327]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[49.0188]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.0013]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.0887]], device='cuda:0')
------ ------
delta_t: tensor([[0.5615]], device='cuda:0')
rewards[i]: 0.5621
values[i+1]: tensor([[0.0876]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0873]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.0013]], device='cuda:0')
delta_t: tensor([[0.5615]], device='cuda:0')
------ ------
policy_loss: 112.21276092529297
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.0013]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[143.8288]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.1923]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4961]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5830]], device='cuda:0')
------ ------
delta_t: tensor([[0.5648]], device='cuda:0')
rewards[i]: 0.5652
values[i+1]: tensor([[0.0873]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0868]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4961]], device='cuda:0')
delta_t: tensor([[0.5648]], device='cuda:0')
------ ------
policy_loss: 130.1676788330078
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4961]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[176.1245]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[64.5915]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0369]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1228]], device='cuda:0')
------ ------
delta_t: tensor([[0.6157]], device='cuda:0')
rewards[i]: 0.6156
values[i+1]: tensor([[0.0868]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0859]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0369]], device='cuda:0')
delta_t: tensor([[0.6157]], device='cuda:0')
------ ------
policy_loss: 149.4224090576172
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0369]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[212.6785]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[73.1079]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.5503]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6350]], device='cuda:0')
------ ------
delta_t: tensor([[0.5938]], device='cuda:0')
rewards[i]: 0.5935
values[i+1]: tensor([[0.0859]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0847]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.5503]], device='cuda:0')
delta_t: tensor([[0.5938]], device='cuda:0')
------ ------
policy_loss: 169.9087677001953
log_probs[i]: tensor([[-2.3988]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.5503]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 169.9087677001953
value_loss: 212.678466796875
loss: 276.24798583984375



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-6.6987e-03, -3.6598e-05, -9.1151e-06, -9.6043e-06, -5.1795e-05],
        [ 1.6966e-01,  9.2072e-04,  2.3086e-04,  2.4301e-04,  1.3056e-03],
        [-1.4371e-03, -7.9018e-06, -1.9550e-06, -2.0759e-06, -1.1136e-05],
        [ 7.6225e-01,  4.1331e-03,  1.0371e-03,  1.0919e-03,  5.8646e-03],
        [ 3.5838e+00,  1.9524e-02,  4.8755e-03,  5.1412e-03,  2.7641e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 7.7332e-05,  5.5767e-05, -3.1089e-05, -5.8027e-05,  6.0168e-05],
        [-1.9572e-03, -1.4124e-03,  7.8636e-04,  1.4698e-03, -1.5231e-03],
        [ 1.6604e-05,  1.1960e-05, -6.6826e-06, -1.2444e-05,  1.2915e-05],
        [-8.7919e-03, -6.3450e-03,  3.5323e-03,  6.6030e-03, -6.8421e-03],
        [-4.1350e-02, -2.9827e-02,  1.6621e-02,  3.1038e-02, -3.2175e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.0812, -1.0611,  1.0802, -0.9452,  0.6815],
        [-0.3689,  0.3620, -0.3685,  0.3225, -0.2325],
        [-0.4680,  0.4594, -0.4676,  0.4092, -0.2950],
        [-0.0283,  0.0278, -0.0283,  0.0248, -0.0179],
        [-0.2159,  0.2119, -0.2157,  0.1888, -0.1361]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0506, -0.0358,  0.0207,  0.0373, -0.0391],
        [-0.0410, -0.0292,  0.0167,  0.0303, -0.0317],
        [ 0.0160,  0.0110, -0.0070, -0.0115,  0.0120],
        [ 0.1010,  0.0728, -0.0399, -0.0757,  0.0793],
        [-0.0506, -0.0359,  0.0207,  0.0373, -0.0392],
        [ 0.0101,  0.0071, -0.0042, -0.0072,  0.0075],
        [-0.0169, -0.0122,  0.0067,  0.0125, -0.0130],
        [-0.0217, -0.0156,  0.0087,  0.0161, -0.0168],
        [ 0.0575,  0.0404, -0.0241, -0.0419,  0.0441],
        [ 0.0084,  0.0061, -0.0032, -0.0062,  0.0065],
        [-0.0122, -0.0089,  0.0048,  0.0091, -0.0095]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.0562,  2.1663, -1.2492, -2.2531,  2.3666]], device='cuda:0')
--> [loss] 276.24798583984375

---------------------------------- [[#22 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     2.0      |     4.0     | 0.6717 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([768.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.876716563158937 acc: 0.6609
[Epoch 7] loss: 2.5459986628626314 acc: 0.7053
[Epoch 11] loss: 0.8881332464039783 acc: 0.7014
[Epoch 15] loss: 0.3876841380653898 acc: 0.6976
[Epoch 19] loss: 0.26713105387118696 acc: 0.7032
[Epoch 23] loss: 0.21584930188139267 acc: 0.6951
[Epoch 27] loss: 0.19613941316552402 acc: 0.6958
[Epoch 31] loss: 0.16455529567540225 acc: 0.6996
[Epoch 35] loss: 0.1546365133989269 acc: 0.6968
[Epoch 39] loss: 0.13773907648037423 acc: 0.69
[Epoch 43] loss: 0.13169845800085797 acc: 0.6955
[Epoch 47] loss: 0.11826999909232569 acc: 0.6926
[Epoch 51] loss: 0.12041602936237479 acc: 0.7043
[Epoch 55] loss: 0.10115659109238163 acc: 0.7051
[Epoch 59] loss: 0.10758978226091094 acc: 0.693
[Epoch 63] loss: 0.09810633456234431 acc: 0.6998
[Epoch 67] loss: 0.09420635264076273 acc: 0.6925
[Epoch 71] loss: 0.08802876429503684 acc: 0.6904
--> [test] acc: 0.6983
--> [accuracy] finished 0.6983
new state: tensor([768.,   5.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6983
--> [reward] 0.6983
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     5.0      |     2.0     |     2.0      |     4.0     | 0.6983 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.641592532777421 acc: 0.6704
[Epoch 7] loss: 2.277908013223687 acc: 0.7189
[Epoch 11] loss: 0.7313699869396132 acc: 0.7078
[Epoch 15] loss: 0.33852378230379976 acc: 0.7046
[Epoch 19] loss: 0.23917190404013372 acc: 0.7
[Epoch 23] loss: 0.19768020884631693 acc: 0.7171
[Epoch 27] loss: 0.17236893228433856 acc: 0.713
[Epoch 31] loss: 0.1538838059045946 acc: 0.7119
[Epoch 35] loss: 0.1471583259475353 acc: 0.7157
[Epoch 39] loss: 0.12033340234435676 acc: 0.7143
[Epoch 43] loss: 0.11913076349262558 acc: 0.6895
[Epoch 47] loss: 0.10992331598001673 acc: 0.7118
[Epoch 51] loss: 0.1067777981553191 acc: 0.7146
[Epoch 55] loss: 0.10279926321709819 acc: 0.7055
[Epoch 59] loss: 0.09855139611736702 acc: 0.7106
[Epoch 63] loss: 0.08387627968032572 acc: 0.7158
[Epoch 67] loss: 0.08929302240781428 acc: 0.7071
[Epoch 71] loss: 0.08258888227632864 acc: 0.7036
--> [test] acc: 0.7029
--> [accuracy] finished 0.7029
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7029
--> [reward] 0.7029
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7029 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.717581191026341 acc: 0.6708
[Epoch 7] loss: 2.396498421192779 acc: 0.7093
[Epoch 11] loss: 0.8027304115293123 acc: 0.7109
[Epoch 15] loss: 0.35413767150638964 acc: 0.7004
[Epoch 19] loss: 0.2539656588193172 acc: 0.7096
[Epoch 23] loss: 0.200995154574018 acc: 0.704
[Epoch 27] loss: 0.17206233992453313 acc: 0.7033
[Epoch 31] loss: 0.16219482967323956 acc: 0.7114
[Epoch 35] loss: 0.13868015021195307 acc: 0.6991
[Epoch 39] loss: 0.1335959250995856 acc: 0.7041
[Epoch 43] loss: 0.1234308637588111 acc: 0.6953
[Epoch 47] loss: 0.11335003205105815 acc: 0.7035
[Epoch 51] loss: 0.11386129882929803 acc: 0.6994
[Epoch 55] loss: 0.09868507655790014 acc: 0.6895
[Epoch 59] loss: 0.098302115026154 acc: 0.6976
[Epoch 63] loss: 0.10042646680922841 acc: 0.6942
[Epoch 67] loss: 0.08538903356762008 acc: 0.6955
[Epoch 71] loss: 0.08590270966158041 acc: 0.7014
--> [test] acc: 0.6902
--> [accuracy] finished 0.6902
new state: tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.6902
--> [reward] 0.6902
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     1.0      |     4.0     | 0.6902 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([768.,   5.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.631521919651714 acc: 0.6682
[Epoch 7] loss: 2.2909043393171657 acc: 0.7093
[Epoch 11] loss: 0.7517295934479026 acc: 0.7164
[Epoch 15] loss: 0.33706773759897257 acc: 0.7239
[Epoch 19] loss: 0.243982027260982 acc: 0.7167
[Epoch 23] loss: 0.20999792736509573 acc: 0.7203
[Epoch 27] loss: 0.17507831529949974 acc: 0.7107
[Epoch 31] loss: 0.15617130057948173 acc: 0.7218
[Epoch 35] loss: 0.14211530845059686 acc: 0.7099
[Epoch 39] loss: 0.13149131935023134 acc: 0.7187
[Epoch 43] loss: 0.11998623256843127 acc: 0.7155
[Epoch 47] loss: 0.11450631836610263 acc: 0.7187
[Epoch 51] loss: 0.11281583577280155 acc: 0.7105
[Epoch 55] loss: 0.099094632683117 acc: 0.7157
[Epoch 59] loss: 0.09818974499176245 acc: 0.7183
[Epoch 63] loss: 0.09898883589814939 acc: 0.714
[Epoch 67] loss: 0.08047779206879666 acc: 0.7165
[Epoch 71] loss: 0.09299376109034976 acc: 0.7165
--> [test] acc: 0.7186
--> [accuracy] finished 0.7186
new state: tensor([768.,   5.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7186
--> [reward] 0.7186
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     5.0      |     2.0     |     1.0      |     4.0     | 0.7186 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   5.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.749605681127904 acc: 0.6675
[Epoch 7] loss: 2.3892973444193526 acc: 0.7073
[Epoch 11] loss: 0.8005098707240332 acc: 0.7049
[Epoch 15] loss: 0.35576503694800615 acc: 0.7055
[Epoch 19] loss: 0.24425765477082767 acc: 0.7061
[Epoch 23] loss: 0.20247123064115988 acc: 0.6989
[Epoch 27] loss: 0.16971740608468003 acc: 0.6999
[Epoch 31] loss: 0.15931858707819602 acc: 0.7015
[Epoch 35] loss: 0.14382911842468832 acc: 0.7033
[Epoch 39] loss: 0.13072230249567104 acc: 0.6951
[Epoch 43] loss: 0.12270691116218982 acc: 0.6984
[Epoch 47] loss: 0.11088446768886788 acc: 0.7026
[Epoch 51] loss: 0.10384629337477934 acc: 0.6986
[Epoch 55] loss: 0.10150097567847718 acc: 0.7058
[Epoch 59] loss: 0.08968232290646153 acc: 0.6883
[Epoch 63] loss: 0.09172682321714976 acc: 0.6953
[Epoch 67] loss: 0.08742322545567685 acc: 0.7
[Epoch 71] loss: 0.08371995700894834 acc: 0.6971
--> [test] acc: 0.6766
--> [accuracy] finished 0.6766
new state: tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.6766
--> [reward] 0.6766
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     1.0      |     4.0     | 0.6766 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.701240931477998 acc: 0.6669
[Epoch 7] loss: 2.3506114937155447 acc: 0.7108
[Epoch 11] loss: 0.781256459620031 acc: 0.7067
[Epoch 15] loss: 0.3439788958986702 acc: 0.6923
[Epoch 19] loss: 0.2447503065056813 acc: 0.7051
[Epoch 23] loss: 0.2063982709265693 acc: 0.7003
[Epoch 27] loss: 0.1717699953781255 acc: 0.7059
[Epoch 31] loss: 0.15713618075965768 acc: 0.6948
[Epoch 35] loss: 0.14230333920330038 acc: 0.7109
[Epoch 39] loss: 0.13310606002717104 acc: 0.7023
[Epoch 43] loss: 0.11979794137410896 acc: 0.7033
[Epoch 47] loss: 0.11331423117070283 acc: 0.706
[Epoch 51] loss: 0.10585435285754598 acc: 0.7025
[Epoch 55] loss: 0.10230816225342625 acc: 0.7103
[Epoch 59] loss: 0.09265351586092664 acc: 0.7019
[Epoch 63] loss: 0.09203593125444767 acc: 0.7012
[Epoch 67] loss: 0.09419178868771372 acc: 0.6994
[Epoch 71] loss: 0.08462116463691034 acc: 0.6982
--> [test] acc: 0.6923
--> [accuracy] finished 0.6923
new state: tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.6923
--> [reward] 0.6923
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     1.0      |     4.0     | 0.6923 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.747961415049365 acc: 0.6686
[Epoch 7] loss: 2.411382065785815 acc: 0.6956
[Epoch 11] loss: 0.8264088043299935 acc: 0.7023
[Epoch 15] loss: 0.36669732667410465 acc: 0.694
[Epoch 19] loss: 0.25959286200420933 acc: 0.7125
[Epoch 23] loss: 0.21217197516356665 acc: 0.6988
[Epoch 27] loss: 0.1785103188115446 acc: 0.6971
[Epoch 31] loss: 0.16015389978961872 acc: 0.7038
[Epoch 35] loss: 0.14556338149539727 acc: 0.7038
[Epoch 39] loss: 0.13339145867454122 acc: 0.6951
[Epoch 43] loss: 0.12009430217796037 acc: 0.7074
[Epoch 47] loss: 0.12576706611725222 acc: 0.7009
[Epoch 51] loss: 0.10880021377921562 acc: 0.6865
[Epoch 55] loss: 0.09626843395542896 acc: 0.7052
[Epoch 59] loss: 0.09924842370941978 acc: 0.7037
[Epoch 63] loss: 0.0895576500018244 acc: 0.7061
[Epoch 67] loss: 0.0872193959159324 acc: 0.6989
[Epoch 71] loss: 0.09084257130901736 acc: 0.7031
--> [test] acc: 0.7052
--> [accuracy] finished 0.7052
new state: tensor([736.,   6.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7052
--> [reward] 0.7052
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     6.0      |     2.0     |     1.0      |     4.0     | 0.7052 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   6.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.629954376184117 acc: 0.6839
[Epoch 7] loss: 2.272401313380817 acc: 0.7169
[Epoch 11] loss: 0.7286933122007438 acc: 0.7126
[Epoch 15] loss: 0.3332464273690301 acc: 0.718
[Epoch 19] loss: 0.23670553058728844 acc: 0.7153
[Epoch 23] loss: 0.19836238284816826 acc: 0.7127
[Epoch 27] loss: 0.170169999568111 acc: 0.7208
[Epoch 31] loss: 0.15790987813540394 acc: 0.7126
[Epoch 35] loss: 0.13950363432755097 acc: 0.7011
[Epoch 39] loss: 0.12613359781915842 acc: 0.7122
[Epoch 43] loss: 0.12492478305897901 acc: 0.7097
[Epoch 47] loss: 0.11024543308460004 acc: 0.7103
[Epoch 51] loss: 0.09996029730115677 acc: 0.6985
[Epoch 55] loss: 0.10315396078764592 acc: 0.691
[Epoch 59] loss: 0.10011154532556416 acc: 0.7
[Epoch 63] loss: 0.08477619978735614 acc: 0.7098
[Epoch 67] loss: 0.08801605422978702 acc: 0.7018
[Epoch 71] loss: 0.08106916364165656 acc: 0.7039
--> [test] acc: 0.705
--> [accuracy] finished 0.705
new state: tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.705
--> [reward] 0.705
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.705  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.658816338804981 acc: 0.6766
[Epoch 7] loss: 2.2815905806734738 acc: 0.691
[Epoch 11] loss: 0.7345874646769079 acc: 0.697
[Epoch 15] loss: 0.3357889475372365 acc: 0.7087
[Epoch 19] loss: 0.24210867161155128 acc: 0.6981
[Epoch 23] loss: 0.19530793949199454 acc: 0.7074
[Epoch 27] loss: 0.17498055796908296 acc: 0.701
[Epoch 31] loss: 0.148444745749178 acc: 0.7153
[Epoch 35] loss: 0.14157229923474057 acc: 0.6992
[Epoch 39] loss: 0.12936437281701343 acc: 0.7119
[Epoch 43] loss: 0.11531675361868594 acc: 0.707
[Epoch 47] loss: 0.11056078023657136 acc: 0.7035
[Epoch 51] loss: 0.1091190553728796 acc: 0.6872
[Epoch 55] loss: 0.10294755355706867 acc: 0.7031
[Epoch 59] loss: 0.09347558954594862 acc: 0.7014
[Epoch 63] loss: 0.08632420856372246 acc: 0.6959
[Epoch 67] loss: 0.08708322725815239 acc: 0.7011
[Epoch 71] loss: 0.0879024141923646 acc: 0.7007
--> [test] acc: 0.7023
--> [accuracy] finished 0.7023
new state: tensor([704.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7023
--> [reward] 0.7023
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7023 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.6457602334449355 acc: 0.6662
[Epoch 7] loss: 2.264472397049065 acc: 0.6952
[Epoch 11] loss: 0.7295306428523777 acc: 0.707
[Epoch 15] loss: 0.3407031998891012 acc: 0.707
[Epoch 19] loss: 0.2446426672198812 acc: 0.7182
[Epoch 23] loss: 0.19683385619183863 acc: 0.706
[Epoch 27] loss: 0.16977323278727585 acc: 0.7049
[Epoch 31] loss: 0.16153261581581096 acc: 0.7145
[Epoch 35] loss: 0.12913704256686714 acc: 0.703
[Epoch 39] loss: 0.12777657564515083 acc: 0.701
[Epoch 43] loss: 0.11871033451041145 acc: 0.7014
[Epoch 47] loss: 0.10915452362302586 acc: 0.71
[Epoch 51] loss: 0.10296882354461438 acc: 0.6959
[Epoch 55] loss: 0.09667344237177554 acc: 0.7127
[Epoch 59] loss: 0.09513697736507135 acc: 0.7113
[Epoch 63] loss: 0.08543591571720484 acc: 0.6997
[Epoch 67] loss: 0.09336120378751697 acc: 0.7082
[Epoch 71] loss: 0.08094346444235634 acc: 0.7081
--> [test] acc: 0.6997
--> [accuracy] finished 0.6997
new state: tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6997
--> [reward] 0.6997
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.6997 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3990, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.673054657933657 acc: 0.6673
[Epoch 7] loss: 2.266337907946933 acc: 0.6926
[Epoch 11] loss: 0.7245179758409558 acc: 0.7113
[Epoch 15] loss: 0.3350509876918877 acc: 0.7188
[Epoch 19] loss: 0.23921244897786767 acc: 0.7169
[Epoch 23] loss: 0.20211070393690903 acc: 0.7134
[Epoch 27] loss: 0.16772980139116803 acc: 0.716
[Epoch 31] loss: 0.15462628211302068 acc: 0.7186
[Epoch 35] loss: 0.1344721705921928 acc: 0.6981
[Epoch 39] loss: 0.12946472986174934 acc: 0.7082
[Epoch 43] loss: 0.12043596179369846 acc: 0.7078
[Epoch 47] loss: 0.10681251334764845 acc: 0.7074
[Epoch 51] loss: 0.09979925742712291 acc: 0.7107
[Epoch 55] loss: 0.09748910118788814 acc: 0.7052
[Epoch 59] loss: 0.09069387839221017 acc: 0.7112
[Epoch 63] loss: 0.08764994291938803 acc: 0.6985
[Epoch 67] loss: 0.0831004030304104 acc: 0.713
[Epoch 71] loss: 0.08127615056947454 acc: 0.7042
--> [test] acc: 0.7012
--> [accuracy] finished 0.7012
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7012
--> [reward] 0.7012
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7012 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.717701561158271 acc: 0.6636
[Epoch 7] loss: 2.3324277336945007 acc: 0.7076
[Epoch 11] loss: 0.755400934925927 acc: 0.7115
[Epoch 15] loss: 0.3383591122034451 acc: 0.7055
[Epoch 19] loss: 0.24295895841315657 acc: 0.7069
[Epoch 23] loss: 0.1984163923069949 acc: 0.7079
[Epoch 27] loss: 0.17417314094598488 acc: 0.7083
[Epoch 31] loss: 0.15280160921223251 acc: 0.708
[Epoch 35] loss: 0.13329233934797938 acc: 0.7159
[Epoch 39] loss: 0.1263652588085979 acc: 0.7096
[Epoch 43] loss: 0.1115918510023009 acc: 0.7095
[Epoch 47] loss: 0.11580402066912074 acc: 0.7025
[Epoch 51] loss: 0.10580889075217992 acc: 0.7019
[Epoch 55] loss: 0.09934454901343988 acc: 0.6903
[Epoch 59] loss: 0.0898167519506229 acc: 0.7008
[Epoch 63] loss: 0.0917693739042372 acc: 0.7119
[Epoch 67] loss: 0.0880783429497953 acc: 0.7029
[Epoch 71] loss: 0.08024311513255013 acc: 0.7011
--> [test] acc: 0.7132
--> [accuracy] finished 0.7132
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7132
--> [reward] 0.7132
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7132 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.65179228706433 acc: 0.6855
[Epoch 7] loss: 2.271067879655782 acc: 0.7117
[Epoch 11] loss: 0.7330885360212734 acc: 0.7122
[Epoch 15] loss: 0.34184101054592586 acc: 0.7067
[Epoch 19] loss: 0.24048396951907203 acc: 0.7167
[Epoch 23] loss: 0.1978093929341077 acc: 0.7177
[Epoch 27] loss: 0.17312572304340426 acc: 0.7072
[Epoch 31] loss: 0.15396731226797908 acc: 0.709
[Epoch 35] loss: 0.14161179670730553 acc: 0.7051
[Epoch 39] loss: 0.12845506375867521 acc: 0.7104
[Epoch 43] loss: 0.11950666370475307 acc: 0.7134
[Epoch 47] loss: 0.10770294802558734 acc: 0.698
[Epoch 51] loss: 0.10838770582948042 acc: 0.7064
[Epoch 55] loss: 0.09610833851334727 acc: 0.705
[Epoch 59] loss: 0.09179283291830317 acc: 0.7093
[Epoch 63] loss: 0.08463153409023705 acc: 0.7126
[Epoch 67] loss: 0.08516906282853792 acc: 0.7062
[Epoch 71] loss: 0.08394171671031991 acc: 0.7117
--> [test] acc: 0.7103
--> [accuracy] finished 0.7103
new state: tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7103
--> [reward] 0.7103
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7103 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.6948421394733515 acc: 0.6616
[Epoch 7] loss: 2.2814691589235343 acc: 0.7185
[Epoch 11] loss: 0.7293899770483108 acc: 0.7155
[Epoch 15] loss: 0.3330823730324845 acc: 0.7014
[Epoch 19] loss: 0.2340609951378287 acc: 0.7117
[Epoch 23] loss: 0.1874072185200651 acc: 0.7156
[Epoch 27] loss: 0.17874298144377235 acc: 0.7085
[Epoch 31] loss: 0.14651900722557093 acc: 0.7231
[Epoch 35] loss: 0.1436358710007785 acc: 0.7121
[Epoch 39] loss: 0.12916643557417423 acc: 0.7109
[Epoch 43] loss: 0.11150748001606874 acc: 0.7119
[Epoch 47] loss: 0.11210315262497154 acc: 0.7104
[Epoch 51] loss: 0.10284092416391348 acc: 0.7145
[Epoch 55] loss: 0.09581174545497408 acc: 0.7104
[Epoch 59] loss: 0.08965838010667923 acc: 0.7063
[Epoch 63] loss: 0.09113725666266383 acc: 0.6982
[Epoch 67] loss: 0.08616433798795795 acc: 0.699
[Epoch 71] loss: 0.0877323609951269 acc: 0.7138
--> [test] acc: 0.7027
--> [accuracy] finished 0.7027
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7027
--> [reward] 0.7027
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7027 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0925, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0898,
         0.0904, 0.0902]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3986]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([800.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.638621648406739 acc: 0.677
[Epoch 7] loss: 2.2319895561851197 acc: 0.7019
[Epoch 11] loss: 0.7202736912700145 acc: 0.7114
[Epoch 15] loss: 0.3271831290181869 acc: 0.7053
[Epoch 19] loss: 0.23921406644460796 acc: 0.713
[Epoch 23] loss: 0.1967695837082994 acc: 0.7094
[Epoch 27] loss: 0.1644951528500375 acc: 0.7043
[Epoch 31] loss: 0.1502992843427812 acc: 0.7046
[Epoch 35] loss: 0.1440538921086189 acc: 0.7081
[Epoch 39] loss: 0.12276832639928097 acc: 0.7103
[Epoch 43] loss: 0.11881619465746976 acc: 0.698
[Epoch 47] loss: 0.10722019808316875 acc: 0.7024
[Epoch 51] loss: 0.10698072886084924 acc: 0.7049
[Epoch 55] loss: 0.0887242878386465 acc: 0.7044
[Epoch 59] loss: 0.09522741154917633 acc: 0.6962
[Epoch 63] loss: 0.09301393828414323 acc: 0.7063
[Epoch 67] loss: 0.08537450132494354 acc: 0.7008
[Epoch 71] loss: 0.08381684841987108 acc: 0.7001
--> [test] acc: 0.7112
--> [accuracy] finished 0.7112
new state: tensor([800.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7112
--> [reward] 0.7112
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2527]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5054]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7109]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.8059]], device='cuda:0')
------ ------
delta_t: tensor([[0.7109]], device='cuda:0')
rewards[i]: 0.7112
values[i+1]: tensor([[0.0957]], device='cuda:0')
values[i]: tensor([[0.0950]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7109]], device='cuda:0')
delta_t: tensor([[0.7109]], device='cuda:0')
------ ------
policy_loss: 1.679514765739441
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7109]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.2412]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.9770]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4061]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.5006]], device='cuda:0')
------ ------
delta_t: tensor([[0.7023]], device='cuda:0')
rewards[i]: 0.7027
values[i+1]: tensor([[0.0950]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0945]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4061]], device='cuda:0')
delta_t: tensor([[0.7023]], device='cuda:0')
------ ------
policy_loss: 5.024825572967529
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4061]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.4486]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.4148]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1011]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1959]], device='cuda:0')
------ ------
delta_t: tensor([[0.7091]], device='cuda:0')
rewards[i]: 0.7103
values[i+1]: tensor([[0.0945]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0947]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1011]], device='cuda:0')
delta_t: tensor([[0.7091]], device='cuda:0')
------ ------
policy_loss: 10.038110733032227
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1011]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.3476]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.7981]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.7925]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8871]], device='cuda:0')
------ ------
delta_t: tensor([[0.7124]], device='cuda:0')
rewards[i]: 0.7132
values[i+1]: tensor([[0.0947]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0946]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.7925]], device='cuda:0')
delta_t: tensor([[0.7124]], device='cuda:0')
------ ------
policy_loss: 16.712610244750977
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.7925]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[13.3508]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.0064]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.4650]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.5594]], device='cuda:0')
------ ------
delta_t: tensor([[0.7005]], device='cuda:0')
rewards[i]: 0.7012
values[i+1]: tensor([[0.0946]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0944]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.4650]], device='cuda:0')
delta_t: tensor([[0.7005]], device='cuda:0')
------ ------
policy_loss: 24.991802215576172
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.4650]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[21.8756]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.0496]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1291]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2235]], device='cuda:0')
------ ------
delta_t: tensor([[0.6987]], device='cuda:0')
rewards[i]: 0.6997
values[i+1]: tensor([[0.0944]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0944]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1291]], device='cuda:0')
delta_t: tensor([[0.6987]], device='cuda:0')
------ ------
policy_loss: 34.862327575683594
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1291]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[33.3403]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.9294]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7885]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8836]], device='cuda:0')
------ ------
delta_t: tensor([[0.7006]], device='cuda:0')
rewards[i]: 0.7023
values[i+1]: tensor([[0.0944]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0951]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7885]], device='cuda:0')
delta_t: tensor([[0.7006]], device='cuda:0')
------ ------
policy_loss: 46.318214416503906
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7885]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[48.1614]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.6423]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4445]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.5398]], device='cuda:0')
------ ------
delta_t: tensor([[0.7039]], device='cuda:0')
rewards[i]: 0.705
values[i+1]: tensor([[0.0951]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0953]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4445]], device='cuda:0')
delta_t: tensor([[0.7039]], device='cuda:0')
------ ------
policy_loss: 59.34798049926758
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4445]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[66.7293]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.1357]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.0939]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1896]], device='cuda:0')
------ ------
delta_t: tensor([[0.7039]], device='cuda:0')
rewards[i]: 0.7052
values[i+1]: tensor([[0.0953]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0957]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.0939]], device='cuda:0')
delta_t: tensor([[0.7039]], device='cuda:0')
------ ------
policy_loss: 73.93350982666016
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.0939]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[89.3390]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.2195]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7245]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8200]], device='cuda:0')
------ ------
delta_t: tensor([[0.6916]], device='cuda:0')
rewards[i]: 0.6923
values[i+1]: tensor([[0.0957]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0954]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7245]], device='cuda:0')
delta_t: tensor([[0.6916]], device='cuda:0')
------ ------
policy_loss: 90.03923797607422
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7245]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[116.2280]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[53.7780]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.3333]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.4284]], device='cuda:0')
------ ------
delta_t: tensor([[0.6760]], device='cuda:0')
rewards[i]: 0.6766
values[i+1]: tensor([[0.0954]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0950]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.3333]], device='cuda:0')
delta_t: tensor([[0.6760]], device='cuda:0')
------ ------
policy_loss: 107.6060791015625
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.3333]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[148.0509]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[63.6458]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9778]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.0727]], device='cuda:0')
------ ------
delta_t: tensor([[0.7178]], device='cuda:0')
rewards[i]: 0.7186
values[i+1]: tensor([[0.0950]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0948]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9778]], device='cuda:0')
delta_t: tensor([[0.7178]], device='cuda:0')
------ ------
policy_loss: 126.70984649658203
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9778]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[184.9283]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[73.7548]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.5881]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6822]], device='cuda:0')
------ ------
delta_t: tensor([[0.6900]], device='cuda:0')
rewards[i]: 0.6902
values[i+1]: tensor([[0.0948]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0941]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.5881]], device='cuda:0')
delta_t: tensor([[0.6900]], device='cuda:0')
------ ------
policy_loss: 147.28012084960938
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.5881]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[227.2975]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[84.7385]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2054]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2982]], device='cuda:0')
------ ------
delta_t: tensor([[0.7032]], device='cuda:0')
rewards[i]: 0.7029
values[i+1]: tensor([[0.0941]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0929]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2054]], device='cuda:0')
delta_t: tensor([[0.7032]], device='cuda:0')
------ ------
policy_loss: 169.33743286132812
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2054]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[275.4363]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[96.2774]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8121]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.9036]], device='cuda:0')
------ ------
delta_t: tensor([[0.6988]], device='cuda:0')
rewards[i]: 0.6983
values[i+1]: tensor([[0.0929]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0914]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8121]], device='cuda:0')
delta_t: tensor([[0.6988]], device='cuda:0')
------ ------
policy_loss: 192.85020446777344
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8121]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 192.85020446777344
value_loss: 275.436279296875
loss: 330.568359375



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-7.1346e-03, -4.8400e-05, -1.6015e-05, -1.5765e-05, -3.9645e-05],
        [ 2.0363e-01,  1.3726e-03,  4.5657e-04,  4.4782e-04,  1.1274e-03],
        [-1.6275e-03, -1.1176e-05, -3.7514e-06, -3.5778e-06, -8.9546e-06],
        [ 9.6935e-01,  6.5357e-03,  2.1890e-03,  2.1216e-03,  5.3448e-03],
        [ 4.5988e+00,  3.1095e-02,  1.0443e-02,  1.0047e-02,  2.5284e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 8.5172e-05,  5.8326e-05, -3.5276e-05, -6.1287e-05,  6.5050e-05],
        [-2.4255e-03, -1.6624e-03,  1.0029e-03,  1.7467e-03, -1.8529e-03],
        [ 1.9454e-05,  1.3298e-05, -8.0592e-06, -1.3978e-05,  1.4845e-05],
        [-1.1538e-02, -7.9073e-03,  4.7667e-03,  8.3086e-03, -8.8130e-03],
        [-5.4737e-02, -3.7499e-02,  2.2611e-02,  3.9404e-02, -4.1802e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.4076, -1.3839,  1.4064, -1.2440,  0.9078],
        [-0.4710,  0.4631, -0.4706,  0.4162, -0.3036],
        [-0.6376,  0.6269, -0.6371,  0.5637, -0.4114],
        [-0.0359,  0.0353, -0.0359,  0.0318, -0.0232],
        [-0.2630,  0.2586, -0.2628,  0.2324, -0.1696]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0482,  0.0317, -0.0209, -0.0336,  0.0360],
        [ 0.0183,  0.0112, -0.0098, -0.0119,  0.0142],
        [ 0.0064,  0.0043, -0.0022, -0.0045,  0.0044],
        [ 0.1790,  0.1225, -0.0724, -0.1289,  0.1368],
        [-0.0615, -0.0414,  0.0257,  0.0436, -0.0466],
        [-0.0611, -0.0412,  0.0255,  0.0434, -0.0463],
        [ 0.0110,  0.0078, -0.0039, -0.0080,  0.0083],
        [-0.0156, -0.0108,  0.0060,  0.0113, -0.0119],
        [-0.0602, -0.0405,  0.0251,  0.0427, -0.0456],
        [-0.0607, -0.0409,  0.0253,  0.0430, -0.0460],
        [-0.0039, -0.0028,  0.0018,  0.0028, -0.0033]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.6968,  2.4897, -1.5418, -2.6222,  2.8012]], device='cuda:0')
--> [loss] 330.568359375

---------------------------------- [[#23 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7112 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([800.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.72305857799852 acc: 0.6703
[Epoch 7] loss: 2.304495708945462 acc: 0.7128
[Epoch 11] loss: 0.7479769197409339 acc: 0.7125
[Epoch 15] loss: 0.3376644805116612 acc: 0.7134
[Epoch 19] loss: 0.24090778860537446 acc: 0.7042
[Epoch 23] loss: 0.199899659053806 acc: 0.7046
[Epoch 27] loss: 0.17179577420715747 acc: 0.7014
[Epoch 31] loss: 0.14951856165905208 acc: 0.7168
[Epoch 35] loss: 0.1464852347798512 acc: 0.697
[Epoch 39] loss: 0.12619801570811426 acc: 0.7062
[Epoch 43] loss: 0.12285769687693976 acc: 0.7122
[Epoch 47] loss: 0.10926650818306334 acc: 0.6882
[Epoch 51] loss: 0.106666471355397 acc: 0.7113
[Epoch 55] loss: 0.09099296988595439 acc: 0.7063
[Epoch 59] loss: 0.09292144704293913 acc: 0.7026
[Epoch 63] loss: 0.08352747646427197 acc: 0.6963
[Epoch 67] loss: 0.0932780606952041 acc: 0.7081
[Epoch 71] loss: 0.07706237554727623 acc: 0.7076
--> [test] acc: 0.7083
--> [accuracy] finished 0.7083
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7083
--> [reward] 0.7083
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7083 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.659430398812989 acc: 0.6686
[Epoch 7] loss: 2.2528849108063658 acc: 0.7015
[Epoch 11] loss: 0.721552939690135 acc: 0.7073
[Epoch 15] loss: 0.313950060633347 acc: 0.7105
[Epoch 19] loss: 0.2391736045089143 acc: 0.7036
[Epoch 23] loss: 0.2023734333150832 acc: 0.7103
[Epoch 27] loss: 0.16142570102930337 acc: 0.7033
[Epoch 31] loss: 0.15337898995955962 acc: 0.6977
[Epoch 35] loss: 0.1360842715198522 acc: 0.7049
[Epoch 39] loss: 0.12461341570472093 acc: 0.703
[Epoch 43] loss: 0.11180650826086246 acc: 0.7085
[Epoch 47] loss: 0.10798455265861558 acc: 0.7074
[Epoch 51] loss: 0.10249221329208073 acc: 0.7049
[Epoch 55] loss: 0.09833813667573664 acc: 0.7042
[Epoch 59] loss: 0.08303212609184939 acc: 0.7113
[Epoch 63] loss: 0.09274775687964809 acc: 0.7001
[Epoch 67] loss: 0.0814503550718841 acc: 0.7032
[Epoch 71] loss: 0.08190756312439271 acc: 0.7086
--> [test] acc: 0.706
--> [accuracy] finished 0.706
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.706
--> [reward] 0.706
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.706  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   6.,   2.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.691788736206796 acc: 0.6599
[Epoch 7] loss: 2.31721836183687 acc: 0.7108
[Epoch 11] loss: 0.7478220843521836 acc: 0.6991
[Epoch 15] loss: 0.32943515878056395 acc: 0.7154
[Epoch 19] loss: 0.24568821119187434 acc: 0.7166
[Epoch 23] loss: 0.20304296834065633 acc: 0.7017
[Epoch 27] loss: 0.1725571389855517 acc: 0.7067
[Epoch 31] loss: 0.1564108964801311 acc: 0.7018
[Epoch 35] loss: 0.13550762022438143 acc: 0.6921
[Epoch 39] loss: 0.13684121590784137 acc: 0.7004
[Epoch 43] loss: 0.11467342900440973 acc: 0.7043
[Epoch 47] loss: 0.11105379182845354 acc: 0.7005
[Epoch 51] loss: 0.1097708897453154 acc: 0.7014
[Epoch 55] loss: 0.09628809727085254 acc: 0.6969
[Epoch 59] loss: 0.09749846331491742 acc: 0.6951
[Epoch 63] loss: 0.09479263611067154 acc: 0.7081
[Epoch 67] loss: 0.0837171853724224 acc: 0.7046
[Epoch 71] loss: 0.08263730154047623 acc: 0.7009
--> [test] acc: 0.6946
--> [accuracy] finished 0.6946
new state: tensor([768.,   6.,   2.,   2.,   3.], device='cuda:0')
new reward: 0.6946
--> [reward] 0.6946
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     3.0     | 0.6946 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.257888533575151 acc: 0.6114
[Epoch 7] loss: 3.1951697198173883 acc: 0.671
[Epoch 11] loss: 1.5070902503779173 acc: 0.6581
[Epoch 15] loss: 0.6185181011610171 acc: 0.6525
[Epoch 19] loss: 0.3848580979596814 acc: 0.6551
[Epoch 23] loss: 0.3013119103858614 acc: 0.6574
[Epoch 27] loss: 0.26166042010716695 acc: 0.6522
[Epoch 31] loss: 0.22765912972581204 acc: 0.66
[Epoch 35] loss: 0.20713650491898475 acc: 0.6565
[Epoch 39] loss: 0.190595448757891 acc: 0.6554
[Epoch 43] loss: 0.17960312958482816 acc: 0.6486
[Epoch 47] loss: 0.15920131856485095 acc: 0.6548
[Epoch 51] loss: 0.15334545986791193 acc: 0.6508
[Epoch 55] loss: 0.14191949219130875 acc: 0.6507
[Epoch 59] loss: 0.13541000601990372 acc: 0.6441
[Epoch 63] loss: 0.1375115525865894 acc: 0.6499
[Epoch 67] loss: 0.12535401419653078 acc: 0.6402
[Epoch 71] loss: 0.12401764633019677 acc: 0.6509
--> [test] acc: 0.6528
--> [accuracy] finished 0.6528
new state: tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.6528
--> [reward] 0.6528
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     3.0     | 0.6528 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.121781985625587 acc: 0.6299
[Epoch 7] loss: 3.0878734993355352 acc: 0.6614
[Epoch 11] loss: 1.415670562849935 acc: 0.6679
[Epoch 15] loss: 0.5944008496554231 acc: 0.6592
[Epoch 19] loss: 0.3574301636756381 acc: 0.6532
[Epoch 23] loss: 0.2900020711438354 acc: 0.6658
[Epoch 27] loss: 0.23833631201530509 acc: 0.6679
[Epoch 31] loss: 0.2204009004537483 acc: 0.6505
[Epoch 35] loss: 0.19703624639755396 acc: 0.6557
[Epoch 39] loss: 0.18554854277245073 acc: 0.6584
[Epoch 43] loss: 0.15474903135491377 acc: 0.6614
[Epoch 47] loss: 0.16665729864136034 acc: 0.6612
[Epoch 51] loss: 0.14618158938072603 acc: 0.658
[Epoch 55] loss: 0.1371037626205503 acc: 0.6526
[Epoch 59] loss: 0.13473610795291183 acc: 0.6576
[Epoch 63] loss: 0.11571696104810518 acc: 0.6606
[Epoch 67] loss: 0.11842315280905275 acc: 0.6587
[Epoch 71] loss: 0.12093846206946293 acc: 0.6501
--> [test] acc: 0.663
--> [accuracy] finished 0.663
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.663
--> [reward] 0.663
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.663  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.158143786365724 acc: 0.6227
[Epoch 7] loss: 3.0958023806057318 acc: 0.6666
[Epoch 11] loss: 1.4476842914738923 acc: 0.6628
[Epoch 15] loss: 0.5810688376912604 acc: 0.6657
[Epoch 19] loss: 0.362742164908239 acc: 0.6709
[Epoch 23] loss: 0.29198068470868005 acc: 0.6594
[Epoch 27] loss: 0.25078789410336166 acc: 0.663
[Epoch 31] loss: 0.21016034983393864 acc: 0.6569
[Epoch 35] loss: 0.20851254423894464 acc: 0.6595
[Epoch 39] loss: 0.1769131656374087 acc: 0.6577
[Epoch 43] loss: 0.17500010869272833 acc: 0.6516
[Epoch 47] loss: 0.15171729974642806 acc: 0.6581
[Epoch 51] loss: 0.14814655327111903 acc: 0.6552
[Epoch 55] loss: 0.14084593651940108 acc: 0.6593
[Epoch 59] loss: 0.1352123819211798 acc: 0.6556
[Epoch 63] loss: 0.12873733421678052 acc: 0.6555
[Epoch 67] loss: 0.11689222749689465 acc: 0.6543
[Epoch 71] loss: 0.11634332104913576 acc: 0.6577
--> [test] acc: 0.6529
--> [accuracy] finished 0.6529
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6529
--> [reward] 0.6529
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.6529 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.152895166288556 acc: 0.6297
[Epoch 7] loss: 3.0895080887295707 acc: 0.6729
[Epoch 11] loss: 1.4255598522627446 acc: 0.6671
[Epoch 15] loss: 0.5691145213840105 acc: 0.66
[Epoch 19] loss: 0.35922502578162324 acc: 0.6601
[Epoch 23] loss: 0.2891058766609415 acc: 0.6614
[Epoch 27] loss: 0.24502628767038778 acc: 0.6638
[Epoch 31] loss: 0.21673311423653227 acc: 0.6614
[Epoch 35] loss: 0.19876710598445152 acc: 0.6546
[Epoch 39] loss: 0.19293668404426378 acc: 0.659
[Epoch 43] loss: 0.16079719021530522 acc: 0.6564
[Epoch 47] loss: 0.15351881651812807 acc: 0.6587
[Epoch 51] loss: 0.1511054148015035 acc: 0.6575
[Epoch 55] loss: 0.1443363450529516 acc: 0.6448
[Epoch 59] loss: 0.12428027445264637 acc: 0.6511
[Epoch 63] loss: 0.1332315878957198 acc: 0.6572
[Epoch 67] loss: 0.12003937326705136 acc: 0.6551
[Epoch 71] loss: 0.12579055386178595 acc: 0.6557
--> [test] acc: 0.6524
--> [accuracy] finished 0.6524
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6524
--> [reward] 0.6524
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.6524 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.174815068312008 acc: 0.6163
[Epoch 7] loss: 3.092627252611663 acc: 0.6704
[Epoch 11] loss: 1.3983418948167121 acc: 0.6668
[Epoch 15] loss: 0.5771904415629633 acc: 0.6713
[Epoch 19] loss: 0.36411143891284686 acc: 0.656
[Epoch 23] loss: 0.2753975474873505 acc: 0.665
[Epoch 27] loss: 0.24254448207147666 acc: 0.65
[Epoch 31] loss: 0.22621239522410094 acc: 0.6623
[Epoch 35] loss: 0.18809499871938506 acc: 0.6576
[Epoch 39] loss: 0.18354657336192973 acc: 0.6565
[Epoch 43] loss: 0.17263370200924938 acc: 0.6571
[Epoch 47] loss: 0.14652299547753753 acc: 0.6521
[Epoch 51] loss: 0.15116052557636872 acc: 0.6657
[Epoch 55] loss: 0.13198594164753527 acc: 0.6552
[Epoch 59] loss: 0.13247258871641304 acc: 0.6527
[Epoch 63] loss: 0.1282639784817024 acc: 0.651
[Epoch 67] loss: 0.12563947387411237 acc: 0.657
[Epoch 71] loss: 0.11035661974771763 acc: 0.6554
--> [test] acc: 0.6578
--> [accuracy] finished 0.6578
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6578
--> [reward] 0.6578
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.6578 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([768.,   6.,   1.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.181685794954714 acc: 0.6102
[Epoch 7] loss: 3.2160365939750086 acc: 0.6572
[Epoch 11] loss: 1.5188266022316634 acc: 0.6589
[Epoch 15] loss: 0.6185308249995989 acc: 0.6568
[Epoch 19] loss: 0.3800953829189396 acc: 0.6631
[Epoch 23] loss: 0.2969955999042143 acc: 0.6565
[Epoch 27] loss: 0.2518973580544905 acc: 0.6559
[Epoch 31] loss: 0.22777035043758276 acc: 0.6532
[Epoch 35] loss: 0.20340934803094857 acc: 0.6607
[Epoch 39] loss: 0.1865622331424023 acc: 0.6569
[Epoch 43] loss: 0.16593665359994333 acc: 0.6572
[Epoch 47] loss: 0.16955175680463272 acc: 0.652
[Epoch 51] loss: 0.1464553300048346 acc: 0.6504
[Epoch 55] loss: 0.15058020537045053 acc: 0.647
[Epoch 59] loss: 0.1394236326060446 acc: 0.6487
[Epoch 63] loss: 0.1298681963220253 acc: 0.6519
[Epoch 67] loss: 0.1204242145766497 acc: 0.6439
[Epoch 71] loss: 0.12209311216001344 acc: 0.6444
--> [test] acc: 0.6499
--> [accuracy] finished 0.6499
new state: tensor([768.,   6.,   1.,   1.,   4.], device='cuda:0')
new reward: 0.6499
--> [reward] 0.6499
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     1.0      |     4.0     | 0.6499 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([768.,   6.,   1.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.181631036884035 acc: 0.6114
[Epoch 7] loss: 3.2129187571728015 acc: 0.6676
[Epoch 11] loss: 1.503413472200751 acc: 0.6554
[Epoch 15] loss: 0.6107102769767613 acc: 0.6528
[Epoch 19] loss: 0.37512959714007116 acc: 0.6486
[Epoch 23] loss: 0.2996165142001589 acc: 0.6555
[Epoch 27] loss: 0.252142630038721 acc: 0.6559
[Epoch 31] loss: 0.22311425905989107 acc: 0.6491
[Epoch 35] loss: 0.20441197205687422 acc: 0.6478
[Epoch 39] loss: 0.18456788587591152 acc: 0.6455
[Epoch 43] loss: 0.17281797069989507 acc: 0.6499
[Epoch 47] loss: 0.15850537693571023 acc: 0.6444
[Epoch 51] loss: 0.15910432662438515 acc: 0.6539
[Epoch 55] loss: 0.13925444898421846 acc: 0.6404
[Epoch 59] loss: 0.1379912393453443 acc: 0.6486
[Epoch 63] loss: 0.13257042789096943 acc: 0.6503
[Epoch 67] loss: 0.13013981207978942 acc: 0.65
[Epoch 71] loss: 0.11492405388065997 acc: 0.6468
--> [test] acc: 0.643
--> [accuracy] finished 0.643
new state: tensor([768.,   6.,   1.,   1.,   4.], device='cuda:0')
new reward: 0.643
--> [reward] 0.643
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     1.0      |     4.0     | 0.643  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   6.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.295144909940412 acc: 0.5794
[Epoch 7] loss: 3.3319016491513116 acc: 0.6563
[Epoch 11] loss: 1.6038032124757462 acc: 0.6365
[Epoch 15] loss: 0.6576650111752627 acc: 0.6343
[Epoch 19] loss: 0.3985626870466163 acc: 0.6402
[Epoch 23] loss: 0.30708686674437713 acc: 0.6491
[Epoch 27] loss: 0.2695248821735992 acc: 0.644
[Epoch 31] loss: 0.23785141512008426 acc: 0.6476
[Epoch 35] loss: 0.21381220389443362 acc: 0.64
[Epoch 39] loss: 0.19391293555517178 acc: 0.6482
[Epoch 43] loss: 0.1789211934156087 acc: 0.6441
[Epoch 47] loss: 0.173033707351793 acc: 0.6316
[Epoch 51] loss: 0.15501203998813734 acc: 0.6416
[Epoch 55] loss: 0.15070969320100058 acc: 0.6463
[Epoch 59] loss: 0.1463653675859313 acc: 0.6338
[Epoch 63] loss: 0.1373367844342404 acc: 0.6283
[Epoch 67] loss: 0.1341318077767444 acc: 0.6314
[Epoch 71] loss: 0.11864546234921916 acc: 0.6412
--> [test] acc: 0.6397
--> [accuracy] finished 0.6397
new state: tensor([768.,   6.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6397
--> [reward] 0.6397
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     1.0      |     3.0     | 0.6397 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.260904211827251 acc: 0.6189
[Epoch 7] loss: 3.1919729311752807 acc: 0.6625
[Epoch 11] loss: 1.4734730799980176 acc: 0.657
[Epoch 15] loss: 0.6195757467480724 acc: 0.6428
[Epoch 19] loss: 0.3850811983546828 acc: 0.6558
[Epoch 23] loss: 0.2891069903440983 acc: 0.6532
[Epoch 27] loss: 0.2580311713940309 acc: 0.6556
[Epoch 31] loss: 0.23040750365325577 acc: 0.6465
[Epoch 35] loss: 0.205618642932733 acc: 0.648
[Epoch 39] loss: 0.18275370867089238 acc: 0.6452
[Epoch 43] loss: 0.17600361429764638 acc: 0.6514
[Epoch 47] loss: 0.16263054017110934 acc: 0.6437
[Epoch 51] loss: 0.1461049630401937 acc: 0.6493
[Epoch 55] loss: 0.15496233842380897 acc: 0.6488
[Epoch 59] loss: 0.1382897292737804 acc: 0.6413
[Epoch 63] loss: 0.12127256182872731 acc: 0.647
[Epoch 67] loss: 0.1267052917762676 acc: 0.6366
[Epoch 71] loss: 0.11903198211408599 acc: 0.6546
--> [test] acc: 0.6441
--> [accuracy] finished 0.6441
new state: tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.6441
--> [reward] 0.6441
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     3.0     | 0.6441 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.192434747658117 acc: 0.6168
[Epoch 7] loss: 3.1675475209265413 acc: 0.6726
[Epoch 11] loss: 1.4837294825926766 acc: 0.6614
[Epoch 15] loss: 0.6276867459849704 acc: 0.6625
[Epoch 19] loss: 0.38533173628208583 acc: 0.6559
[Epoch 23] loss: 0.29972297267850173 acc: 0.6542
[Epoch 27] loss: 0.2516582855464095 acc: 0.6546
[Epoch 31] loss: 0.22356275439052783 acc: 0.6476
[Epoch 35] loss: 0.2086266216362262 acc: 0.6367
[Epoch 39] loss: 0.19441064220884113 acc: 0.6553
[Epoch 43] loss: 0.1749509682383417 acc: 0.6487
[Epoch 47] loss: 0.1548801881199836 acc: 0.6471
[Epoch 51] loss: 0.15335027069148735 acc: 0.6517
[Epoch 55] loss: 0.15034976558130989 acc: 0.6476
[Epoch 59] loss: 0.13630576585145557 acc: 0.6554
[Epoch 63] loss: 0.12900707396723882 acc: 0.6474
[Epoch 67] loss: 0.1310128470191427 acc: 0.6546
[Epoch 71] loss: 0.12207394836010778 acc: 0.6588
--> [test] acc: 0.6554
--> [accuracy] finished 0.6554
new state: tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.6554
--> [reward] 0.6554
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     3.0     | 0.6554 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.137467322751994 acc: 0.62
[Epoch 7] loss: 3.122639892351292 acc: 0.6604
[Epoch 11] loss: 1.4610942348723521 acc: 0.6521
[Epoch 15] loss: 0.5897636038520376 acc: 0.647
[Epoch 19] loss: 0.3721852930348433 acc: 0.6634
[Epoch 23] loss: 0.29061030532183396 acc: 0.6642
[Epoch 27] loss: 0.24597294544300918 acc: 0.6615
[Epoch 31] loss: 0.20989322519439566 acc: 0.6543
[Epoch 35] loss: 0.20403303956503377 acc: 0.649
[Epoch 39] loss: 0.18343151143759184 acc: 0.65
[Epoch 43] loss: 0.16284058819217678 acc: 0.65
[Epoch 47] loss: 0.16649852725712921 acc: 0.654
[Epoch 51] loss: 0.149579735339412 acc: 0.6555
[Epoch 55] loss: 0.13088225790173236 acc: 0.6517
[Epoch 59] loss: 0.1346179988643731 acc: 0.6574
[Epoch 63] loss: 0.11958307804911376 acc: 0.6381
[Epoch 67] loss: 0.1264244032406803 acc: 0.6355
[Epoch 71] loss: 0.11621496925854584 acc: 0.651
--> [test] acc: 0.6501
--> [accuracy] finished 0.6501
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6501
--> [reward] 0.6501
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.6501 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0901, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3987, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([768.,   6.,   1.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.258432572455052 acc: 0.6071
[Epoch 7] loss: 3.2248085195298697 acc: 0.6599
[Epoch 11] loss: 1.5010864606789311 acc: 0.6396
[Epoch 15] loss: 0.6337756846109619 acc: 0.6509
[Epoch 19] loss: 0.37928641899524596 acc: 0.6447
[Epoch 23] loss: 0.3055623456445115 acc: 0.642
[Epoch 27] loss: 0.24929951817330803 acc: 0.6505
[Epoch 31] loss: 0.23826333630325086 acc: 0.6449
[Epoch 35] loss: 0.21492744649491272 acc: 0.6499
[Epoch 39] loss: 0.19201070771497838 acc: 0.6476
[Epoch 43] loss: 0.17605056058463003 acc: 0.6494
[Epoch 47] loss: 0.16195529851767107 acc: 0.6476
[Epoch 51] loss: 0.15089187994265876 acc: 0.6447
[Epoch 55] loss: 0.14997762083040211 acc: 0.6437
[Epoch 59] loss: 0.13497343758607042 acc: 0.6499
[Epoch 63] loss: 0.13704818275178332 acc: 0.6418
[Epoch 67] loss: 0.12385360024574087 acc: 0.6447
[Epoch 71] loss: 0.1178047580559931 acc: 0.644
--> [test] acc: 0.644
--> [accuracy] finished 0.644
new state: tensor([768.,   6.,   1.,   2.,   5.], device='cuda:0')
new reward: 0.644
--> [reward] 0.644
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2066]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4133]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6429]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7460]], device='cuda:0')
------ ------
delta_t: tensor([[0.6429]], device='cuda:0')
rewards[i]: 0.644
values[i+1]: tensor([[0.1031]], device='cuda:0')
values[i]: tensor([[0.1032]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6429]], device='cuda:0')
delta_t: tensor([[0.6429]], device='cuda:0')
------ ------
policy_loss: 1.5179238319396973
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6429]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0329]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6525]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2855]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3887]], device='cuda:0')
------ ------
delta_t: tensor([[0.6490]], device='cuda:0')
rewards[i]: 0.6501
values[i+1]: tensor([[0.1032]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1032]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2855]], device='cuda:0')
delta_t: tensor([[0.6490]], device='cuda:0')
------ ------
policy_loss: 4.577110767364502
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2855]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.8897]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.7135]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9271]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0302]], device='cuda:0')
------ ------
delta_t: tensor([[0.6544]], device='cuda:0')
rewards[i]: 0.6554
values[i+1]: tensor([[0.1032]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9271]], device='cuda:0')
delta_t: tensor([[0.6544]], device='cuda:0')
------ ------
policy_loss: 9.172857284545898
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9271]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.1432]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.5071]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5509]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.6540]], device='cuda:0')
------ ------
delta_t: tensor([[0.6431]], device='cuda:0')
rewards[i]: 0.6441
values[i+1]: tensor([[0.1031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5509]], device='cuda:0')
delta_t: tensor([[0.6431]], device='cuda:0')
------ ------
policy_loss: 15.264955520629883
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5509]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[11.1491]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.0118]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.1641]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2672]], device='cuda:0')
------ ------
delta_t: tensor([[0.6387]], device='cuda:0')
rewards[i]: 0.6397
values[i+1]: tensor([[0.1031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1030]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.1641]], device='cuda:0')
delta_t: tensor([[0.6387]], device='cuda:0')
------ ------
policy_loss: 22.832107543945312
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.1641]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[18.2725]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.2469]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.7745]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.8775]], device='cuda:0')
------ ------
delta_t: tensor([[0.6420]], device='cuda:0')
rewards[i]: 0.643
values[i+1]: tensor([[0.1030]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1030]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.7745]], device='cuda:0')
delta_t: tensor([[0.6420]], device='cuda:0')
------ ------
policy_loss: 31.859352111816406
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.7745]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[27.8897]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.2343]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3857]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.4886]], device='cuda:0')
------ ------
delta_t: tensor([[0.6489]], device='cuda:0')
rewards[i]: 0.6499
values[i+1]: tensor([[0.1030]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1029]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3857]], device='cuda:0')
delta_t: tensor([[0.6489]], device='cuda:0')
------ ------
policy_loss: 42.35221862792969
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3857]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[40.3830]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.9868]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9987]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.1015]], device='cuda:0')
------ ------
delta_t: tensor([[0.6568]], device='cuda:0')
rewards[i]: 0.6578
values[i+1]: tensor([[0.1029]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1028]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9987]], device='cuda:0')
delta_t: tensor([[0.6568]], device='cuda:0')
------ ------
policy_loss: 54.318519592285156
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9987]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[56.0641]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[31.3621]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.6002]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.7029]], device='cuda:0')
------ ------
delta_t: tensor([[0.6515]], device='cuda:0')
rewards[i]: 0.6524
values[i+1]: tensor([[0.1028]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1027]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.6002]], device='cuda:0')
delta_t: tensor([[0.6515]], device='cuda:0')
------ ------
policy_loss: 67.72161102294922
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.6002]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[75.2609]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[38.3936]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1963]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2988]], device='cuda:0')
------ ------
delta_t: tensor([[0.6521]], device='cuda:0')
rewards[i]: 0.6529
values[i+1]: tensor([[0.1027]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1025]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1963]], device='cuda:0')
delta_t: tensor([[0.6521]], device='cuda:0')
------ ------
policy_loss: 82.56053924560547
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1963]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[98.3578]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[46.1938]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7966]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8988]], device='cuda:0')
------ ------
delta_t: tensor([[0.6623]], device='cuda:0')
rewards[i]: 0.663
values[i+1]: tensor([[0.1025]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1022]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7966]], device='cuda:0')
delta_t: tensor([[0.6623]], device='cuda:0')
------ ------
policy_loss: 98.83776092529297
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7966]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[125.5976]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[54.4797]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.3810]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.4826]], device='cuda:0')
------ ------
delta_t: tensor([[0.6524]], device='cuda:0')
rewards[i]: 0.6528
values[i+1]: tensor([[0.1022]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1016]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.3810]], device='cuda:0')
delta_t: tensor([[0.6524]], device='cuda:0')
------ ------
policy_loss: 116.50833892822266
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.3810]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[157.6088]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[64.0223]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0014]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1024]], device='cuda:0')
------ ------
delta_t: tensor([[0.6942]], device='cuda:0')
rewards[i]: 0.6946
values[i+1]: tensor([[0.1016]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1010]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0014]], device='cuda:0')
delta_t: tensor([[0.6942]], device='cuda:0')
------ ------
policy_loss: 135.6805877685547
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0014]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[194.8241]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.4305]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6273]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7274]], device='cuda:0')
------ ------
delta_t: tensor([[0.7059]], device='cuda:0')
rewards[i]: 0.706
values[i+1]: tensor([[0.1010]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1000]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6273]], device='cuda:0')
delta_t: tensor([[0.7059]], device='cuda:0')
------ ------
policy_loss: 156.35072326660156
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6273]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[237.5990]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.5498]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2493]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.3484]], device='cuda:0')
------ ------
delta_t: tensor([[0.7083]], device='cuda:0')
rewards[i]: 0.7083
values[i+1]: tensor([[0.1000]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0991]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2493]], device='cuda:0')
delta_t: tensor([[0.7083]], device='cuda:0')
------ ------
policy_loss: 178.50091552734375
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2493]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 178.50091552734375
value_loss: 237.5989532470703
loss: 297.3003845214844



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-6.5805e-03, -5.1630e-05, -1.4128e-05, -1.6477e-05, -3.2900e-05],
        [ 2.0336e-01,  1.5930e-03,  4.3489e-04,  5.0857e-04,  1.0142e-03],
        [-1.4550e-03, -1.1399e-05, -3.0987e-06, -3.6384e-06, -7.2437e-06],
        [ 9.8776e-01,  7.7287e-03,  2.1109e-03,  2.4684e-03,  4.9199e-03],
        [ 4.6998e+00,  3.6753e-02,  1.0050e-02,  1.1745e-02,  2.3406e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 8.1651e-05,  5.3130e-05, -3.4860e-05, -5.6275e-05,  6.1088e-05],
        [-2.5202e-03, -1.6396e-03,  1.0758e-03,  1.7367e-03, -1.8853e-03],
        [ 1.8058e-05,  1.1740e-05, -7.7034e-06, -1.2438e-05,  1.3503e-05],
        [-1.2230e-02, -7.9566e-03,  5.2193e-03,  8.4278e-03, -9.1480e-03],
        [-5.8150e-02, -3.7836e-02,  2.4816e-02,  4.0076e-02, -4.3497e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.4542, -1.4309,  1.4532, -1.2902,  0.9550],
        [-0.4915,  0.4836, -0.4912,  0.4360, -0.3225],
        [-0.6575,  0.6470, -0.6571,  0.5834, -0.4320],
        [-0.0372,  0.0366, -0.0371,  0.0330, -0.0244],
        [-0.2680,  0.2637, -0.2678,  0.2378, -0.1760]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0208,  0.0148, -0.0079, -0.0153,  0.0156],
        [-0.0611, -0.0390,  0.0259,  0.0415, -0.0453],
        [-0.0601, -0.0384,  0.0255,  0.0408, -0.0445],
        [ 0.0155,  0.0109, -0.0063, -0.0113,  0.0120],
        [ 0.0227,  0.0142, -0.0098, -0.0152,  0.0169],
        [-0.0599, -0.0383,  0.0254,  0.0407, -0.0444],
        [ 0.0131,  0.0078, -0.0059, -0.0085,  0.0094],
        [ 0.0130,  0.0078, -0.0058, -0.0085,  0.0094],
        [ 0.0384,  0.0246, -0.0164, -0.0261,  0.0286],
        [ 0.0178,  0.0109, -0.0077, -0.0118,  0.0131],
        [ 0.0398,  0.0248, -0.0172, -0.0265,  0.0292]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.6332,  2.3227, -1.5419, -2.4706,  2.6949]], device='cuda:0')
--> [loss] 297.3003845214844

---------------------------------- [[#24 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     5.0     | 0.644  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.095163288171334 acc: 0.6255
[Epoch 7] loss: 3.0544375900722223 acc: 0.676
[Epoch 11] loss: 1.3980382075700004 acc: 0.6726
[Epoch 15] loss: 0.5688516821312096 acc: 0.6695
[Epoch 19] loss: 0.3580923346502473 acc: 0.6592
[Epoch 23] loss: 0.28897719717134374 acc: 0.6608
[Epoch 27] loss: 0.2424051953822641 acc: 0.6677
[Epoch 31] loss: 0.2141282375487487 acc: 0.649
[Epoch 35] loss: 0.19543868062727135 acc: 0.6622
[Epoch 39] loss: 0.17880087367394734 acc: 0.6613
[Epoch 43] loss: 0.16803835538427925 acc: 0.6637
[Epoch 47] loss: 0.15612865245574728 acc: 0.6646
[Epoch 51] loss: 0.142450628912492 acc: 0.6674
[Epoch 55] loss: 0.13150259004036427 acc: 0.6648
[Epoch 59] loss: 0.12964386872761427 acc: 0.6603
[Epoch 63] loss: 0.1302514238809438 acc: 0.6703
[Epoch 67] loss: 0.12274203479946222 acc: 0.6657
[Epoch 71] loss: 0.1046427381982672 acc: 0.6638
--> [test] acc: 0.6659
--> [accuracy] finished 0.6659
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6659
--> [reward] 0.6659
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.6659 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.142703213197801 acc: 0.6328
[Epoch 7] loss: 3.0588452254262424 acc: 0.6683
[Epoch 11] loss: 1.403443775327919 acc: 0.6674
[Epoch 15] loss: 0.57229200811566 acc: 0.6694
[Epoch 19] loss: 0.3626554367368293 acc: 0.6706
[Epoch 23] loss: 0.28285774717207457 acc: 0.6562
[Epoch 27] loss: 0.24853336811780244 acc: 0.6602
[Epoch 31] loss: 0.2140881745406734 acc: 0.6585
[Epoch 35] loss: 0.19724702453026383 acc: 0.666
[Epoch 39] loss: 0.18499306517908032 acc: 0.6663
[Epoch 43] loss: 0.16370641284376916 acc: 0.6598
[Epoch 47] loss: 0.15860349507621296 acc: 0.6633
[Epoch 51] loss: 0.15087158862463274 acc: 0.656
[Epoch 55] loss: 0.1391317932225634 acc: 0.6613
[Epoch 59] loss: 0.12983432903831058 acc: 0.658
[Epoch 63] loss: 0.13049357546233784 acc: 0.6543
[Epoch 67] loss: 0.12335319476275969 acc: 0.6604
[Epoch 71] loss: 0.10953386989600784 acc: 0.6584
--> [test] acc: 0.6568
--> [accuracy] finished 0.6568
new state: tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
new reward: 0.6568
--> [reward] 0.6568
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     1.0     |     2.0      |     4.0     | 0.6568 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   1.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.716415163806028 acc: 0.6647
[Epoch 7] loss: 2.299986692912438 acc: 0.7194
[Epoch 11] loss: 0.7388606082238353 acc: 0.7019
[Epoch 15] loss: 0.3322988218006194 acc: 0.7088
[Epoch 19] loss: 0.2437123791398028 acc: 0.695
[Epoch 23] loss: 0.199021496243127 acc: 0.7072
[Epoch 27] loss: 0.1736566905060883 acc: 0.6962
[Epoch 31] loss: 0.1535693377594623 acc: 0.7071
[Epoch 35] loss: 0.1344155145015882 acc: 0.7125
[Epoch 39] loss: 0.1300804852353185 acc: 0.7003
[Epoch 43] loss: 0.11480875225151743 acc: 0.6963
[Epoch 47] loss: 0.1100392030049449 acc: 0.7093
[Epoch 51] loss: 0.10477854298455093 acc: 0.7078
[Epoch 55] loss: 0.10318634084120984 acc: 0.7059
[Epoch 59] loss: 0.10075544631864895 acc: 0.706
[Epoch 63] loss: 0.0835483072735274 acc: 0.7054
[Epoch 67] loss: 0.08075240100442153 acc: 0.7004
[Epoch 71] loss: 0.08840083874002233 acc: 0.6929
--> [test] acc: 0.7016
--> [accuracy] finished 0.7016
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7016
--> [reward] 0.7016
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7016 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.64206837906557 acc: 0.6623
[Epoch 7] loss: 2.2369368784796553 acc: 0.7141
[Epoch 11] loss: 0.7106376529702216 acc: 0.7158
[Epoch 15] loss: 0.3328810558651986 acc: 0.717
[Epoch 19] loss: 0.24256434826337545 acc: 0.7097
[Epoch 23] loss: 0.1876497630308599 acc: 0.7031
[Epoch 27] loss: 0.1809324493102939 acc: 0.7067
[Epoch 31] loss: 0.1483897971289466 acc: 0.7054
[Epoch 35] loss: 0.13735156637066237 acc: 0.7042
[Epoch 39] loss: 0.12784658898277146 acc: 0.7074
[Epoch 43] loss: 0.11477377263280085 acc: 0.7081
[Epoch 47] loss: 0.10786902138789463 acc: 0.7115
[Epoch 51] loss: 0.11210070136586761 acc: 0.7004
[Epoch 55] loss: 0.09357344260012916 acc: 0.7085
[Epoch 59] loss: 0.08373876300442706 acc: 0.7041
[Epoch 63] loss: 0.0969216557823436 acc: 0.7045
[Epoch 67] loss: 0.09069803188724057 acc: 0.7056
[Epoch 71] loss: 0.0853697776065647 acc: 0.7135
--> [test] acc: 0.7062
--> [accuracy] finished 0.7062
new state: tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.7062
--> [reward] 0.7062
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     6.0      |     2.0     |     2.0      |     4.0     | 0.7062 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([768.,   6.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([768.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.873953181611912 acc: 0.6573
[Epoch 7] loss: 2.5088480691165875 acc: 0.7121
[Epoch 11] loss: 0.8986110448684839 acc: 0.703
[Epoch 15] loss: 0.38790147734658265 acc: 0.7028
[Epoch 19] loss: 0.2644699243114085 acc: 0.6893
[Epoch 23] loss: 0.21933018382462433 acc: 0.7025
[Epoch 27] loss: 0.1895155317347754 acc: 0.6998
[Epoch 31] loss: 0.17142904014505275 acc: 0.6999
[Epoch 35] loss: 0.15956634358572 acc: 0.6969
[Epoch 39] loss: 0.1362202643771725 acc: 0.6867
[Epoch 43] loss: 0.1335358608268735 acc: 0.7014
[Epoch 47] loss: 0.12000033416716224 acc: 0.6922
[Epoch 51] loss: 0.11621064554049593 acc: 0.698
[Epoch 55] loss: 0.10286462577560064 acc: 0.6954
[Epoch 59] loss: 0.09858559501116805 acc: 0.6947
[Epoch 63] loss: 0.10339425371893113 acc: 0.7004
[Epoch 67] loss: 0.09113379819509204 acc: 0.6967
[Epoch 71] loss: 0.08601833099086681 acc: 0.6961
--> [test] acc: 0.6766
--> [accuracy] finished 0.6766
new state: tensor([768.,   5.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6766
--> [reward] 0.6766
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     5.0      |     2.0     |     2.0      |     4.0     | 0.6766 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([768.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([800.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.8017664681310235 acc: 0.6554
[Epoch 7] loss: 2.48154877332013 acc: 0.7097
[Epoch 11] loss: 0.8781542709035337 acc: 0.7039
[Epoch 15] loss: 0.38399185812162695 acc: 0.6889
[Epoch 19] loss: 0.27563338439501917 acc: 0.706
[Epoch 23] loss: 0.21904303695377716 acc: 0.7075
[Epoch 27] loss: 0.19142971579414195 acc: 0.7001
[Epoch 31] loss: 0.165681885119499 acc: 0.7039
[Epoch 35] loss: 0.16039515594782694 acc: 0.7026
[Epoch 39] loss: 0.14035126937391318 acc: 0.7017
[Epoch 43] loss: 0.13107126256417664 acc: 0.6918
[Epoch 47] loss: 0.12337163258748858 acc: 0.6864
[Epoch 51] loss: 0.11560106726453813 acc: 0.6966
[Epoch 55] loss: 0.10910062224168302 acc: 0.6969
[Epoch 59] loss: 0.0976165838734916 acc: 0.7016
[Epoch 63] loss: 0.10580351734843553 acc: 0.6986
[Epoch 67] loss: 0.09400945094496231 acc: 0.6992
[Epoch 71] loss: 0.09295317369615635 acc: 0.6932
--> [test] acc: 0.6881
--> [accuracy] finished 0.6881
new state: tensor([800.,   5.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6881
--> [reward] 0.6881
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     5.0      |     2.0     |     2.0      |     4.0     | 0.6881 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([800.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([800.,   5.,   2.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.774342858120608 acc: 0.6689
[Epoch 7] loss: 2.4656928890882552 acc: 0.7068
[Epoch 11] loss: 0.8714977190412981 acc: 0.7057
[Epoch 15] loss: 0.3758643405068942 acc: 0.6953
[Epoch 19] loss: 0.2604386395300784 acc: 0.6994
[Epoch 23] loss: 0.2186711042252419 acc: 0.7025
[Epoch 27] loss: 0.18387731878787203 acc: 0.7038
[Epoch 31] loss: 0.1698754517995107 acc: 0.7043
[Epoch 35] loss: 0.14375025082660645 acc: 0.7098
[Epoch 39] loss: 0.1426080434852282 acc: 0.6995
[Epoch 43] loss: 0.12623813142642723 acc: 0.7027
[Epoch 47] loss: 0.11835924604350267 acc: 0.708
[Epoch 51] loss: 0.10319567796157296 acc: 0.709
[Epoch 55] loss: 0.10950976155598259 acc: 0.7055
[Epoch 59] loss: 0.10149090638464017 acc: 0.7017
[Epoch 63] loss: 0.09652548017459052 acc: 0.7006
[Epoch 67] loss: 0.08926436313918656 acc: 0.6971
[Epoch 71] loss: 0.0863549492239733 acc: 0.7085
--> [test] acc: 0.7004
--> [accuracy] finished 0.7004
new state: tensor([800.,   5.,   2.,   3.,   4.], device='cuda:0')
new reward: 0.7004
--> [reward] 0.7004
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  800.0   |     5.0      |     2.0     |     3.0      |     4.0     | 0.7004 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3992, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([800.,   5.,   2.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([768.,   5.,   2.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.839489799783663 acc: 0.6594
[Epoch 7] loss: 2.5261953673932864 acc: 0.7106
[Epoch 11] loss: 0.8983606023671072 acc: 0.6988
[Epoch 15] loss: 0.3890915703066551 acc: 0.7006
[Epoch 19] loss: 0.262200901282432 acc: 0.687
[Epoch 23] loss: 0.2246112993077549 acc: 0.7044
[Epoch 27] loss: 0.18605071602060513 acc: 0.7052
[Epoch 31] loss: 0.16596959607647088 acc: 0.702
[Epoch 35] loss: 0.15456035295668083 acc: 0.6943
[Epoch 39] loss: 0.13980925420437323 acc: 0.6942
[Epoch 43] loss: 0.1218767245728856 acc: 0.69
[Epoch 47] loss: 0.11573481745779743 acc: 0.6911
[Epoch 51] loss: 0.11651366785206758 acc: 0.6904
[Epoch 55] loss: 0.10172641461627567 acc: 0.6979
[Epoch 59] loss: 0.09980713894389465 acc: 0.6998
[Epoch 63] loss: 0.10037622026458402 acc: 0.6926
[Epoch 67] loss: 0.08784967955827708 acc: 0.7004
[Epoch 71] loss: 0.09681738740847924 acc: 0.6962
--> [test] acc: 0.6958
--> [accuracy] finished 0.6958
new state: tensor([768.,   5.,   2.,   3.,   4.], device='cuda:0')
new reward: 0.6958
--> [reward] 0.6958
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     5.0      |     2.0     |     3.0      |     4.0     | 0.6958 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   5.,   2.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   5.,   2.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.7839745694719005 acc: 0.6529
[Epoch 7] loss: 2.4562935140126805 acc: 0.703
[Epoch 11] loss: 0.859754965604876 acc: 0.7066
[Epoch 15] loss: 0.3793328297241112 acc: 0.6901
[Epoch 19] loss: 0.273666016574082 acc: 0.6988
[Epoch 23] loss: 0.21284218809431624 acc: 0.6935
[Epoch 27] loss: 0.18790972837106423 acc: 0.6988
[Epoch 31] loss: 0.16928464891460468 acc: 0.6959
[Epoch 35] loss: 0.1581181307368533 acc: 0.7014
[Epoch 39] loss: 0.1342698369077538 acc: 0.6999
[Epoch 43] loss: 0.12500456097307128 acc: 0.6915
[Epoch 47] loss: 0.12105420863588133 acc: 0.6898
[Epoch 51] loss: 0.11560090009928169 acc: 0.7022
[Epoch 55] loss: 0.10124629904131068 acc: 0.7035
[Epoch 59] loss: 0.09388663449927288 acc: 0.6936
[Epoch 63] loss: 0.09874442530452938 acc: 0.696
[Epoch 67] loss: 0.08569970279765289 acc: 0.6966
[Epoch 71] loss: 0.09056403487742119 acc: 0.7006
--> [test] acc: 0.6996
--> [accuracy] finished 0.6996
new state: tensor([736.,   5.,   2.,   3.,   4.], device='cuda:0')
new reward: 0.6996
--> [reward] 0.6996
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     2.0     |     3.0      |     4.0     | 0.6996 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   2.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.852983887085829 acc: 0.6521
[Epoch 7] loss: 2.545766457991527 acc: 0.7003
[Epoch 11] loss: 0.8897149572506204 acc: 0.6927
[Epoch 15] loss: 0.3979241761767193 acc: 0.7
[Epoch 19] loss: 0.26678561183202376 acc: 0.7015
[Epoch 23] loss: 0.22297849697168068 acc: 0.6727
[Epoch 27] loss: 0.18193071303636674 acc: 0.6776
[Epoch 31] loss: 0.16985434981163047 acc: 0.6856
[Epoch 35] loss: 0.1519509471388405 acc: 0.6955
[Epoch 39] loss: 0.13771358794768523 acc: 0.6916
[Epoch 43] loss: 0.13625882994419303 acc: 0.6978
[Epoch 47] loss: 0.12094915676933458 acc: 0.6838
[Epoch 51] loss: 0.11944821064629595 acc: 0.6923
[Epoch 55] loss: 0.1003218926727543 acc: 0.6763
[Epoch 59] loss: 0.10384260898853755 acc: 0.6988
[Epoch 63] loss: 0.09584201946544944 acc: 0.6978
[Epoch 67] loss: 0.09086700391275641 acc: 0.6964
[Epoch 71] loss: 0.08760641159953775 acc: 0.6933
--> [test] acc: 0.6912
--> [accuracy] finished 0.6912
new state: tensor([736.,   5.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6912
--> [reward] 0.6912
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     2.0     |     2.0      |     4.0     | 0.6912 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   5.,   2.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.98612734439123 acc: 0.6476
[Epoch 7] loss: 2.700313058426923 acc: 0.6924
[Epoch 11] loss: 1.0163560371722102 acc: 0.6789
[Epoch 15] loss: 0.43568802436770837 acc: 0.683
[Epoch 19] loss: 0.2939174873873477 acc: 0.682
[Epoch 23] loss: 0.22653068794066186 acc: 0.6809
[Epoch 27] loss: 0.20760399011699746 acc: 0.6802
[Epoch 31] loss: 0.18786597730653823 acc: 0.6862
[Epoch 35] loss: 0.15661616735589093 acc: 0.6813
[Epoch 39] loss: 0.1467446903448047 acc: 0.6701
[Epoch 43] loss: 0.13756514627657965 acc: 0.6774
[Epoch 47] loss: 0.13363383798335995 acc: 0.6762
[Epoch 51] loss: 0.12941027932402577 acc: 0.6727
[Epoch 55] loss: 0.10653948815672866 acc: 0.6769
[Epoch 59] loss: 0.11007140743334198 acc: 0.6763
[Epoch 63] loss: 0.11010601335083661 acc: 0.6702
[Epoch 67] loss: 0.09892289988729921 acc: 0.6709
[Epoch 71] loss: 0.09486962320066303 acc: 0.6844
--> [test] acc: 0.6767
--> [accuracy] finished 0.6767
new state: tensor([736.,   5.,   2.,   2.,   5.], device='cuda:0')
new reward: 0.6767
--> [reward] 0.6767
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     2.0     |     2.0      |     5.0     | 0.6767 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   2.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   5.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.78063620981353 acc: 0.6514
[Epoch 7] loss: 2.505009218554972 acc: 0.7008
[Epoch 11] loss: 0.8882128551907247 acc: 0.7137
[Epoch 15] loss: 0.37862470921348124 acc: 0.7052
[Epoch 19] loss: 0.26266514739293195 acc: 0.7036
[Epoch 23] loss: 0.21445408666415897 acc: 0.7012
[Epoch 27] loss: 0.19398590636527752 acc: 0.7055
[Epoch 31] loss: 0.1604124249502197 acc: 0.7019
[Epoch 35] loss: 0.1582869667085864 acc: 0.7012
[Epoch 39] loss: 0.13884321911751157 acc: 0.7058
[Epoch 43] loss: 0.12939303331529659 acc: 0.6928
[Epoch 47] loss: 0.12554309663631955 acc: 0.6996
[Epoch 51] loss: 0.10949326559658283 acc: 0.6931
[Epoch 55] loss: 0.1062684681167102 acc: 0.6992
[Epoch 59] loss: 0.1100120089907685 acc: 0.6953
[Epoch 63] loss: 0.10094197652497106 acc: 0.7025
[Epoch 67] loss: 0.09266207475200905 acc: 0.6982
[Epoch 71] loss: 0.08627107888495887 acc: 0.6997
--> [test] acc: 0.7037
--> [accuracy] finished 0.7037
new state: tensor([736.,   5.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.7037
--> [reward] 0.7037
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     5.0      |     2.0     |     1.0      |     5.0     | 0.7037 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   5.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.008055576278121 acc: 0.6498
[Epoch 7] loss: 2.7308475092396405 acc: 0.6954
[Epoch 11] loss: 1.0621148746012878 acc: 0.6887
[Epoch 15] loss: 0.45363454107681045 acc: 0.6968
[Epoch 19] loss: 0.31261445952774697 acc: 0.6912
[Epoch 23] loss: 0.24713418105631457 acc: 0.6948
[Epoch 27] loss: 0.21104375862509317 acc: 0.6886
[Epoch 31] loss: 0.1805286579682013 acc: 0.6855
[Epoch 35] loss: 0.17325875794430456 acc: 0.685
[Epoch 39] loss: 0.15330939041569713 acc: 0.6892
[Epoch 43] loss: 0.14751524311225966 acc: 0.6884
[Epoch 47] loss: 0.13065309723115068 acc: 0.6804
[Epoch 51] loss: 0.13146390901197252 acc: 0.6882
[Epoch 55] loss: 0.12640923204119592 acc: 0.6881
[Epoch 59] loss: 0.11394546850396277 acc: 0.6874
[Epoch 63] loss: 0.10847682194174517 acc: 0.6833
[Epoch 67] loss: 0.10329696062066214 acc: 0.6821
[Epoch 71] loss: 0.10637884694205173 acc: 0.6907
--> [test] acc: 0.6841
--> [accuracy] finished 0.6841
new state: tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.6841
--> [reward] 0.6841
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     5.0     | 0.6841 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([768.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.968469283617366 acc: 0.6436
[Epoch 7] loss: 2.7071705488750086 acc: 0.6971
[Epoch 11] loss: 1.0536651517004918 acc: 0.6869
[Epoch 15] loss: 0.44801610840670286 acc: 0.6851
[Epoch 19] loss: 0.2928153381556692 acc: 0.6977
[Epoch 23] loss: 0.2409766995922074 acc: 0.6995
[Epoch 27] loss: 0.21064872540833662 acc: 0.6942
[Epoch 31] loss: 0.19172339624418017 acc: 0.6946
[Epoch 35] loss: 0.16615944898322874 acc: 0.6854
[Epoch 39] loss: 0.15831295152783126 acc: 0.681
[Epoch 43] loss: 0.13923496179773337 acc: 0.6828
[Epoch 47] loss: 0.13163741886177482 acc: 0.6966
[Epoch 51] loss: 0.13389095256898237 acc: 0.6834
[Epoch 55] loss: 0.11340536446789341 acc: 0.6899
[Epoch 59] loss: 0.10762868617611278 acc: 0.6843
[Epoch 63] loss: 0.12371682837162443 acc: 0.6793
[Epoch 67] loss: 0.10001945454160423 acc: 0.6929
[Epoch 71] loss: 0.09991456062980758 acc: 0.6821
--> [test] acc: 0.6864
--> [accuracy] finished 0.6864
new state: tensor([768.,   4.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.6864
--> [reward] 0.6864
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     1.0      |     5.0     | 0.6864 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0912, 0.0902, 0.0915, 0.0910, 0.0908, 0.0912, 0.0897,
         0.0904, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3976, -2.3986, -2.3973, -2.3978, -2.3980, -2.3976,
         -2.3991, -2.3984, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([768.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.946495644271831 acc: 0.6275
[Epoch 7] loss: 2.718339133140681 acc: 0.7053
[Epoch 11] loss: 1.0339523435706068 acc: 0.6927
[Epoch 15] loss: 0.44418220884402465 acc: 0.6875
[Epoch 19] loss: 0.302908277012827 acc: 0.6897
[Epoch 23] loss: 0.24265164201436065 acc: 0.6946
[Epoch 27] loss: 0.20636270968172024 acc: 0.6924
[Epoch 31] loss: 0.18184361359595186 acc: 0.6881
[Epoch 35] loss: 0.17233265356679955 acc: 0.6813
[Epoch 39] loss: 0.1586389752352596 acc: 0.6953
[Epoch 43] loss: 0.14632823750557725 acc: 0.686
[Epoch 47] loss: 0.12710830070735776 acc: 0.6818
[Epoch 51] loss: 0.13239243713950696 acc: 0.69
[Epoch 55] loss: 0.12055635040440142 acc: 0.6861
[Epoch 59] loss: 0.11375886919643835 acc: 0.6822
[Epoch 63] loss: 0.1123561963553319 acc: 0.6825
[Epoch 67] loss: 0.10319377051736645 acc: 0.6879
[Epoch 71] loss: 0.0979191922651761 acc: 0.6788
--> [test] acc: 0.6877
--> [accuracy] finished 0.6877
new state: tensor([768.,   4.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.6877
--> [reward] 0.6877
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2358]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4716]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6867]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7929]], device='cuda:0')
------ ------
delta_t: tensor([[0.6867]], device='cuda:0')
rewards[i]: 0.6877
values[i+1]: tensor([[0.1063]], device='cuda:0')
values[i]: tensor([[0.1062]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6867]], device='cuda:0')
delta_t: tensor([[0.6867]], device='cuda:0')
------ ------
policy_loss: 1.6232259273529053
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6867]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.1679]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.8643]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.3654]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4714]], device='cuda:0')
------ ------
delta_t: tensor([[0.6855]], device='cuda:0')
rewards[i]: 0.6864
values[i+1]: tensor([[0.1062]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1060]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.3654]], device='cuda:0')
delta_t: tensor([[0.6855]], device='cuda:0')
------ ------
policy_loss: 4.871034622192383
log_probs[i]: tensor([[-2.3962]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.3654]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.2368]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.1378]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.0341]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1408]], device='cuda:0')
------ ------
delta_t: tensor([[0.6824]], device='cuda:0')
rewards[i]: 0.6841
values[i+1]: tensor([[0.1060]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1066]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.0341]], device='cuda:0')
delta_t: tensor([[0.6824]], device='cuda:0')
------ ------
policy_loss: 9.724234580993652
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.0341]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.9251]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.3766]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.7160]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8231]], device='cuda:0')
------ ------
delta_t: tensor([[0.7022]], device='cuda:0')
rewards[i]: 0.7037
values[i+1]: tensor([[0.1066]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1071]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.7160]], device='cuda:0')
delta_t: tensor([[0.7022]], device='cuda:0')
------ ------
policy_loss: 16.21312713623047
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.7160]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[12.5830]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[11.3159]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3639]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.4715]], device='cuda:0')
------ ------
delta_t: tensor([[0.6751]], device='cuda:0')
rewards[i]: 0.6767
values[i+1]: tensor([[0.1071]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1076]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3639]], device='cuda:0')
delta_t: tensor([[0.6751]], device='cuda:0')
------ ------
policy_loss: 24.257287979125977
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3639]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[20.6632]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.1602]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.0200]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.1280]], device='cuda:0')
------ ------
delta_t: tensor([[0.6897]], device='cuda:0')
rewards[i]: 0.6912
values[i+1]: tensor([[0.1076]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.0200]], device='cuda:0')
delta_t: tensor([[0.6897]], device='cuda:0')
------ ------
policy_loss: 33.873111724853516
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.0200]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[31.6034]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[21.8804]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.6777]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7863]], device='cuda:0')
------ ------
delta_t: tensor([[0.6979]], device='cuda:0')
rewards[i]: 0.6996
values[i+1]: tensor([[0.1080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1087]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.6777]], device='cuda:0')
delta_t: tensor([[0.6979]], device='cuda:0')
------ ------
policy_loss: 45.063194274902344
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.6777]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[45.7824]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[28.3580]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.3252]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4343]], device='cuda:0')
------ ------
delta_t: tensor([[0.6944]], device='cuda:0')
rewards[i]: 0.6958
values[i+1]: tensor([[0.1087]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1090]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.3252]], device='cuda:0')
delta_t: tensor([[0.6944]], device='cuda:0')
------ ------
policy_loss: 57.805747985839844
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.3252]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[63.6117]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[35.6585]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.9715]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.0803]], device='cuda:0')
------ ------
delta_t: tensor([[0.6995]], device='cuda:0')
rewards[i]: 0.7004
values[i+1]: tensor([[0.1090]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1088]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.9715]], device='cuda:0')
delta_t: tensor([[0.6995]], device='cuda:0')
------ ------
policy_loss: 72.09901428222656
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.9715]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[85.3868]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[43.5503]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.5993]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7076]], device='cuda:0')
------ ------
delta_t: tensor([[0.6875]], device='cuda:0')
rewards[i]: 0.6881
values[i+1]: tensor([[0.1088]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1084]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.5993]], device='cuda:0')
delta_t: tensor([[0.6875]], device='cuda:0')
------ ------
policy_loss: 87.88835906982422
log_probs[i]: tensor([[-2.3962]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.5993]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[111.3683]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[51.9630]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2085]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.3171]], device='cuda:0')
------ ------
delta_t: tensor([[0.6753]], device='cuda:0')
rewards[i]: 0.6766
values[i+1]: tensor([[0.1084]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1086]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2085]], device='cuda:0')
delta_t: tensor([[0.6753]], device='cuda:0')
------ ------
policy_loss: 105.14789581298828
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2085]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[142.1141]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[61.4916]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.8417]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.9502]], device='cuda:0')
------ ------
delta_t: tensor([[0.7052]], device='cuda:0')
rewards[i]: 0.7062
values[i+1]: tensor([[0.1086]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1085]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.8417]], device='cuda:0')
delta_t: tensor([[0.7052]], device='cuda:0')
------ ------
policy_loss: 123.93290710449219
log_probs[i]: tensor([[-2.3986]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.8417]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[177.9331]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[71.6381]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.4639]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.5723]], device='cuda:0')
------ ------
delta_t: tensor([[0.7007]], device='cuda:0')
rewards[i]: 0.7016
values[i+1]: tensor([[0.1085]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1083]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.4639]], device='cuda:0')
delta_t: tensor([[0.7007]], device='cuda:0')
------ ------
policy_loss: 144.2041015625
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.4639]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[218.7553]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[81.6443]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.0357]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.1433]], device='cuda:0')
------ ------
delta_t: tensor([[0.6564]], device='cuda:0')
rewards[i]: 0.6568
values[i+1]: tensor([[0.1083]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1076]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.0357]], device='cuda:0')
delta_t: tensor([[0.6564]], device='cuda:0')
------ ------
policy_loss: 165.8543243408203
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.0357]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[264.9438]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[92.3770]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.6113]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.7178]], device='cuda:0')
------ ------
delta_t: tensor([[0.6659]], device='cuda:0')
rewards[i]: 0.6659
values[i+1]: tensor([[0.1076]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1065]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.6113]], device='cuda:0')
delta_t: tensor([[0.6659]], device='cuda:0')
------ ------
policy_loss: 188.8892364501953
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.6113]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 188.8892364501953
value_loss: 264.94378662109375
loss: 321.36114501953125



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-7.4838e-03, -5.5555e-05, -1.4172e-05, -1.9741e-05, -3.9173e-05],
        [ 2.3731e-01,  1.7626e-03,  4.4520e-04,  6.2525e-04,  1.2343e-03],
        [-1.5673e-03, -1.1591e-05, -3.0258e-06, -4.1753e-06, -8.2400e-06],
        [ 1.1568e+00,  8.5774e-03,  2.1795e-03,  3.0547e-03,  6.0139e-03],
        [ 5.3139e+00,  3.9394e-02,  1.0014e-02,  1.4049e-02,  2.7628e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 9.7718e-05,  6.0144e-05, -4.2456e-05, -6.4387e-05,  7.2076e-05],
        [-3.0946e-03, -1.9057e-03,  1.3433e-03,  2.0401e-03, -2.2819e-03],
        [ 2.0480e-05,  1.2591e-05, -8.9053e-06, -1.3480e-05,  1.5106e-05],
        [-1.5078e-02, -9.2835e-03,  6.5448e-03,  9.9387e-03, -1.1118e-02],
        [-6.9249e-02, -4.2635e-02,  3.0056e-02,  4.5645e-02, -5.1057e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.6672, -1.6401,  1.6660, -1.4748,  1.1075],
        [-0.5778,  0.5684, -0.5774,  0.5112, -0.3839],
        [-0.7427,  0.7306, -0.7422,  0.6569, -0.4933],
        [-0.0423,  0.0416, -0.0423,  0.0374, -0.0281],
        [-0.3045,  0.2995, -0.3042,  0.2693, -0.2022]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0276,  0.0170, -0.0117, -0.0184,  0.0203],
        [ 0.0068,  0.0039, -0.0035, -0.0042,  0.0054],
        [ 0.0186,  0.0110, -0.0088, -0.0117,  0.0139],
        [ 0.0066,  0.0038, -0.0028, -0.0041,  0.0046],
        [-0.0662, -0.0402,  0.0291,  0.0430, -0.0489],
        [ 0.0125,  0.0075, -0.0050, -0.0080,  0.0088],
        [-0.0052, -0.0035,  0.0012,  0.0038, -0.0031],
        [-0.0102, -0.0062,  0.0048,  0.0065, -0.0077],
        [ 0.0217,  0.0140, -0.0086, -0.0148,  0.0156],
        [-0.0353, -0.0217,  0.0150,  0.0232, -0.0258],
        [ 0.0233,  0.0144, -0.0096, -0.0153,  0.0169]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.9789,  2.4150, -1.7494, -2.5859,  2.9394]], device='cuda:0')
--> [loss] 321.36114501953125

---------------------------------- [[#25 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     1.0      |     5.0     | 0.6877 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([768.,   4.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.874665488977262 acc: 0.6553
[Epoch 7] loss: 2.556319388358489 acc: 0.7303
[Epoch 11] loss: 0.9211247317645403 acc: 0.7146
[Epoch 15] loss: 0.3863596643090172 acc: 0.7145
[Epoch 19] loss: 0.27595749343066567 acc: 0.7085
[Epoch 23] loss: 0.22023941996171498 acc: 0.7044
[Epoch 27] loss: 0.1937907337481418 acc: 0.7024
[Epoch 31] loss: 0.1857376460562391 acc: 0.7152
[Epoch 35] loss: 0.15456884313443833 acc: 0.7003
[Epoch 39] loss: 0.1465119134427508 acc: 0.7103
[Epoch 43] loss: 0.13507823571157845 acc: 0.7087
[Epoch 47] loss: 0.13326156488083818 acc: 0.6991
[Epoch 51] loss: 0.1211001803363671 acc: 0.7146
[Epoch 55] loss: 0.10998305439165391 acc: 0.7099
[Epoch 59] loss: 0.10653023224543123 acc: 0.7007
[Epoch 63] loss: 0.10864432797769603 acc: 0.7091
[Epoch 67] loss: 0.0979124212078095 acc: 0.7099
[Epoch 71] loss: 0.09744944481495911 acc: 0.7096
--> [test] acc: 0.704
--> [accuracy] finished 0.704
new state: tensor([768.,   4.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.704
--> [reward] 0.704
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  768.0   |     4.0      |     2.0     |     1.0      |     4.0     | 0.704  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([768.,   4.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([736.,   4.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.832788450333774 acc: 0.6641
[Epoch 7] loss: 2.565961493936646 acc: 0.718
[Epoch 11] loss: 0.9500683313116546 acc: 0.7152
[Epoch 15] loss: 0.400510997451899 acc: 0.7067
[Epoch 19] loss: 0.268103215964916 acc: 0.7056
[Epoch 23] loss: 0.23067306581160527 acc: 0.7038
[Epoch 27] loss: 0.19263864226658325 acc: 0.7166
[Epoch 31] loss: 0.1770696726811054 acc: 0.7119
[Epoch 35] loss: 0.15744672002478638 acc: 0.7127
[Epoch 39] loss: 0.14309312589228382 acc: 0.7017
[Epoch 43] loss: 0.1355967843557334 acc: 0.7099
[Epoch 47] loss: 0.13068528951543484 acc: 0.7112
[Epoch 51] loss: 0.11322419957288772 acc: 0.7071
[Epoch 55] loss: 0.11419267897777584 acc: 0.7043
[Epoch 59] loss: 0.11507688323452668 acc: 0.7023
[Epoch 63] loss: 0.10159107062267735 acc: 0.6981
[Epoch 67] loss: 0.09995228043370916 acc: 0.6928
[Epoch 71] loss: 0.09999476195386875 acc: 0.6947
--> [test] acc: 0.6959
--> [accuracy] finished 0.6959
new state: tensor([736.,   4.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.6959
--> [reward] 0.6959
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     4.0     | 0.6959 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.84766688523695 acc: 0.671
[Epoch 7] loss: 2.5704201085259544 acc: 0.7159
[Epoch 11] loss: 0.9211016648575244 acc: 0.7085
[Epoch 15] loss: 0.4042393838953408 acc: 0.7061
[Epoch 19] loss: 0.2749164832846435 acc: 0.7056
[Epoch 23] loss: 0.22484287350197962 acc: 0.7086
[Epoch 27] loss: 0.2018103664455092 acc: 0.7112
[Epoch 31] loss: 0.18180941661719777 acc: 0.7073
[Epoch 35] loss: 0.15802417336570104 acc: 0.7116
[Epoch 39] loss: 0.15182389787015985 acc: 0.7071
[Epoch 43] loss: 0.1352322791807849 acc: 0.7135
[Epoch 47] loss: 0.1315998332872701 acc: 0.7001
[Epoch 51] loss: 0.11809200535827885 acc: 0.7123
[Epoch 55] loss: 0.11541227176737831 acc: 0.7057
[Epoch 59] loss: 0.12421295190196666 acc: 0.708
[Epoch 63] loss: 0.10409360897579632 acc: 0.7035
[Epoch 67] loss: 0.09698266391713849 acc: 0.7049
[Epoch 71] loss: 0.09717624745024439 acc: 0.6902
--> [test] acc: 0.6942
--> [accuracy] finished 0.6942
new state: tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.6942
--> [reward] 0.6942
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     3.0     | 0.6942 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   4.,   2.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.1881713508949865 acc: 0.6371
[Epoch 7] loss: 2.950144175297159 acc: 0.6903
[Epoch 11] loss: 1.2110233873586216 acc: 0.673
[Epoch 15] loss: 0.5133418658810198 acc: 0.6735
[Epoch 19] loss: 0.33632431138554575 acc: 0.6751
[Epoch 23] loss: 0.26762565694358725 acc: 0.6789
[Epoch 27] loss: 0.22531796560224976 acc: 0.6703
[Epoch 31] loss: 0.20164054407573798 acc: 0.6652
[Epoch 35] loss: 0.18591493281685864 acc: 0.6643
[Epoch 39] loss: 0.16097945252866924 acc: 0.6697
[Epoch 43] loss: 0.14942886178498455 acc: 0.6731
[Epoch 47] loss: 0.14852561706872394 acc: 0.6744
[Epoch 51] loss: 0.1332898532704967 acc: 0.6718
[Epoch 55] loss: 0.13212196976470444 acc: 0.6681
[Epoch 59] loss: 0.12063816246336036 acc: 0.6544
[Epoch 63] loss: 0.12466863931402031 acc: 0.6668
[Epoch 67] loss: 0.11091767349409933 acc: 0.6632
[Epoch 71] loss: 0.10975624378318029 acc: 0.6618
--> [test] acc: 0.6674
--> [accuracy] finished 0.6674
new state: tensor([736.,   4.,   2.,   2.,   3.], device='cuda:0')
new reward: 0.6674
--> [reward] 0.6674
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     2.0      |     3.0     | 0.6674 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.836690326023589 acc: 0.6544
[Epoch 7] loss: 2.5581874515089535 acc: 0.7125
[Epoch 11] loss: 0.9185394472764123 acc: 0.6985
[Epoch 15] loss: 0.3875867852302807 acc: 0.7187
[Epoch 19] loss: 0.2799766940324355 acc: 0.7021
[Epoch 23] loss: 0.22700923832509753 acc: 0.7085
[Epoch 27] loss: 0.1908110784690665 acc: 0.71
[Epoch 31] loss: 0.18042787377629668 acc: 0.7144
[Epoch 35] loss: 0.15223758433090376 acc: 0.7056
[Epoch 39] loss: 0.1512627191829693 acc: 0.7067
[Epoch 43] loss: 0.1365937195966006 acc: 0.7097
[Epoch 47] loss: 0.12735529266097737 acc: 0.7088
[Epoch 51] loss: 0.11948862222506834 acc: 0.7089
[Epoch 55] loss: 0.12084479186722004 acc: 0.709
[Epoch 59] loss: 0.10765399099857596 acc: 0.7075
[Epoch 63] loss: 0.10785305798839769 acc: 0.7041
[Epoch 67] loss: 0.08513338425282456 acc: 0.7038
[Epoch 71] loss: 0.10256764636812088 acc: 0.7068
--> [test] acc: 0.7173
--> [accuracy] finished 0.7173
new state: tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7173
--> [reward] 0.7173
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     3.0     | 0.7173 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   4.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.8393485619284 acc: 0.6186
[Epoch 7] loss: 2.5556707858963086 acc: 0.7134
[Epoch 11] loss: 0.9239231999915885 acc: 0.7029
[Epoch 15] loss: 0.39799284951194475 acc: 0.7087
[Epoch 19] loss: 0.2713944753739135 acc: 0.7118
[Epoch 23] loss: 0.23070601634967053 acc: 0.7108
[Epoch 27] loss: 0.19213483241312873 acc: 0.7079
[Epoch 31] loss: 0.1695527623257483 acc: 0.7117
[Epoch 35] loss: 0.15858899823049336 acc: 0.7084
[Epoch 39] loss: 0.13820581148852548 acc: 0.7136
[Epoch 43] loss: 0.14388753257482254 acc: 0.7072
[Epoch 47] loss: 0.12719844942650452 acc: 0.7014
[Epoch 51] loss: 0.11455582448667215 acc: 0.7079
[Epoch 55] loss: 0.10331246391048327 acc: 0.7036
[Epoch 59] loss: 0.11867222673011958 acc: 0.7044
[Epoch 63] loss: 0.09689699782396081 acc: 0.7041
[Epoch 67] loss: 0.09645106181468996 acc: 0.7095
[Epoch 71] loss: 0.09695482662762217 acc: 0.7155
--> [test] acc: 0.7084
--> [accuracy] finished 0.7084
new state: tensor([736.,   4.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7084
--> [reward] 0.7084
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     4.0     | 0.7084 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.851712812852981 acc: 0.6716
[Epoch 7] loss: 2.557729977628459 acc: 0.7079
[Epoch 11] loss: 0.9212569410214796 acc: 0.7128
[Epoch 15] loss: 0.4035932752672973 acc: 0.7102
[Epoch 19] loss: 0.26278402771362486 acc: 0.7074
[Epoch 23] loss: 0.23989466235251225 acc: 0.7121
[Epoch 27] loss: 0.19535314147134342 acc: 0.7166
[Epoch 31] loss: 0.16911970183034153 acc: 0.712
[Epoch 35] loss: 0.1647943376737368 acc: 0.7051
[Epoch 39] loss: 0.14121031877167922 acc: 0.6984
[Epoch 43] loss: 0.138061514555398 acc: 0.705
[Epoch 47] loss: 0.13686607812609894 acc: 0.7052
[Epoch 51] loss: 0.12510824204265328 acc: 0.7069
[Epoch 55] loss: 0.11299812359035091 acc: 0.6973
[Epoch 59] loss: 0.11002564587263519 acc: 0.7068
[Epoch 63] loss: 0.10426037152157262 acc: 0.6935
[Epoch 67] loss: 0.10345088563832071 acc: 0.7041
[Epoch 71] loss: 0.09097192630462368 acc: 0.71
--> [test] acc: 0.7019
--> [accuracy] finished 0.7019
new state: tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7019
--> [reward] 0.7019
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     3.0     | 0.7019 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.824267132660312 acc: 0.6526
[Epoch 7] loss: 2.565698734482231 acc: 0.7244
[Epoch 11] loss: 0.9206181753026631 acc: 0.7101
[Epoch 15] loss: 0.40246736040086395 acc: 0.7106
[Epoch 19] loss: 0.2742714182237911 acc: 0.7166
[Epoch 23] loss: 0.22213543466794902 acc: 0.7012
[Epoch 27] loss: 0.19965543712029601 acc: 0.7039
[Epoch 31] loss: 0.17562847660230402 acc: 0.7089
[Epoch 35] loss: 0.16366117478936643 acc: 0.7092
[Epoch 39] loss: 0.14530842873217809 acc: 0.7
[Epoch 43] loss: 0.13496957202334806 acc: 0.7002
[Epoch 47] loss: 0.13928484104459396 acc: 0.7159
[Epoch 51] loss: 0.11757614611245482 acc: 0.7024
[Epoch 55] loss: 0.11970643449784316 acc: 0.699
[Epoch 59] loss: 0.11130285327990606 acc: 0.7094
[Epoch 63] loss: 0.10781387510516531 acc: 0.7029
[Epoch 67] loss: 0.10630450169782123 acc: 0.7009
[Epoch 71] loss: 0.09320733423554398 acc: 0.6963
--> [test] acc: 0.7076
--> [accuracy] finished 0.7076
new state: tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7076
--> [reward] 0.7076
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     4.0      |     2.0     |     1.0      |     3.0     | 0.7076 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0896,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.864668112581648 acc: 0.6295
[Epoch 7] loss: 2.5763414427447504 acc: 0.7086
[Epoch 11] loss: 0.9513154930204076 acc: 0.7137
[Epoch 15] loss: 0.39624342249463435 acc: 0.7136
[Epoch 19] loss: 0.2760368953875797 acc: 0.7051
[Epoch 23] loss: 0.23885008882459663 acc: 0.7052
[Epoch 27] loss: 0.19986117756008492 acc: 0.7065
[Epoch 31] loss: 0.1793399118006115 acc: 0.7025
[Epoch 35] loss: 0.16100245034750885 acc: 0.7047
[Epoch 39] loss: 0.1528860088009054 acc: 0.7044
[Epoch 43] loss: 0.13719977647461512 acc: 0.7027
[Epoch 47] loss: 0.13138550991797462 acc: 0.703
[Epoch 51] loss: 0.12223092806132516 acc: 0.6909
[Epoch 55] loss: 0.1083819613642538 acc: 0.7063
[Epoch 59] loss: 0.11871753261684229 acc: 0.7044
[Epoch 63] loss: 0.11403736431399346 acc: 0.7013
[Epoch 67] loss: 0.09786604945917073 acc: 0.7004
[Epoch 71] loss: 0.10143108682806247 acc: 0.6932
--> [test] acc: 0.6821
--> [accuracy] finished 0.6821
new state: tensor([704.,   4.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.6821
--> [reward] 0.6821
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     4.0      |     2.0     |     1.0      |     3.0     | 0.6821 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([704.,   4.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.783978769236513 acc: 0.672
[Epoch 7] loss: 2.4964438479422304 acc: 0.7288
[Epoch 11] loss: 0.9409265876092645 acc: 0.7258
[Epoch 15] loss: 0.40931746917193196 acc: 0.7337
[Epoch 19] loss: 0.28569523973719163 acc: 0.723
[Epoch 23] loss: 0.231668988535719 acc: 0.7296
[Epoch 27] loss: 0.19658223960234228 acc: 0.725
[Epoch 31] loss: 0.1815546175245853 acc: 0.7111
[Epoch 35] loss: 0.16982163748968288 acc: 0.7194
[Epoch 39] loss: 0.14513399667056548 acc: 0.7287
[Epoch 43] loss: 0.15228609650221933 acc: 0.7115
[Epoch 47] loss: 0.12936402177628692 acc: 0.7137
[Epoch 51] loss: 0.12864404479565714 acc: 0.7198
[Epoch 55] loss: 0.12088287935789932 acc: 0.7166
[Epoch 59] loss: 0.10828298653352081 acc: 0.725
[Epoch 63] loss: 0.10665775115168213 acc: 0.7153
[Epoch 67] loss: 0.1142285903487021 acc: 0.7188
[Epoch 71] loss: 0.0934354442015738 acc: 0.7262
--> [test] acc: 0.7227
--> [accuracy] finished 0.7227
new state: tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7227
--> [reward] 0.7227
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7227 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.743229239035751 acc: 0.6778
[Epoch 7] loss: 2.463047212499487 acc: 0.7116
[Epoch 11] loss: 0.9259702655512964 acc: 0.7264
[Epoch 15] loss: 0.397858486279769 acc: 0.7259
[Epoch 19] loss: 0.2790554421584663 acc: 0.7304
[Epoch 23] loss: 0.2326335589260892 acc: 0.7328
[Epoch 27] loss: 0.20462379388301574 acc: 0.726
[Epoch 31] loss: 0.1827590154183795 acc: 0.7239
[Epoch 35] loss: 0.16151569533945465 acc: 0.7137
[Epoch 39] loss: 0.14389535529619973 acc: 0.721
[Epoch 43] loss: 0.15086630187348685 acc: 0.7223
[Epoch 47] loss: 0.12344161041534704 acc: 0.7189
[Epoch 51] loss: 0.13527469858741553 acc: 0.7271
[Epoch 55] loss: 0.11491301471603882 acc: 0.7203
[Epoch 59] loss: 0.11561835527567722 acc: 0.719
[Epoch 63] loss: 0.11076074184806985 acc: 0.7187
[Epoch 67] loss: 0.0973076559342694 acc: 0.7199
[Epoch 71] loss: 0.10249469224173013 acc: 0.7203
--> [test] acc: 0.7231
--> [accuracy] finished 0.7231
new state: tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7231
--> [reward] 0.7231
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7231 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.769622728647783 acc: 0.6615
[Epoch 7] loss: 2.501417529933593 acc: 0.7266
[Epoch 11] loss: 0.9486373500979465 acc: 0.731
[Epoch 15] loss: 0.4045426931608554 acc: 0.7239
[Epoch 19] loss: 0.2825509587188473 acc: 0.7161
[Epoch 23] loss: 0.22991162537099302 acc: 0.7219
[Epoch 27] loss: 0.2033993253732086 acc: 0.7189
[Epoch 31] loss: 0.1767356895420062 acc: 0.7142
[Epoch 35] loss: 0.17212960585866058 acc: 0.722
[Epoch 39] loss: 0.14906237200867203 acc: 0.7126
[Epoch 43] loss: 0.13612380633761872 acc: 0.7249
[Epoch 47] loss: 0.13751159613366093 acc: 0.7208
[Epoch 51] loss: 0.13036438090074093 acc: 0.7205
[Epoch 55] loss: 0.12163352007475084 acc: 0.726
[Epoch 59] loss: 0.11136150531782804 acc: 0.7098
[Epoch 63] loss: 0.11563573761359619 acc: 0.7205
[Epoch 67] loss: 0.10472843434442491 acc: 0.7157
[Epoch 71] loss: 0.09541529307946982 acc: 0.7137
--> [test] acc: 0.7207
--> [accuracy] finished 0.7207
new state: tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7207
--> [reward] 0.7207
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7207 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.7503848219161755 acc: 0.6537
[Epoch 7] loss: 2.475375924192731 acc: 0.7426
[Epoch 11] loss: 0.9253024880957725 acc: 0.7266
[Epoch 15] loss: 0.4098679957258732 acc: 0.7342
[Epoch 19] loss: 0.26968149326341534 acc: 0.7232
[Epoch 23] loss: 0.22098801265735074 acc: 0.7127
[Epoch 27] loss: 0.20358252157921644 acc: 0.7168
[Epoch 31] loss: 0.1805364388336554 acc: 0.725
[Epoch 35] loss: 0.1616464172039762 acc: 0.7162
[Epoch 39] loss: 0.1509302341762711 acc: 0.7211
[Epoch 43] loss: 0.14334370087251028 acc: 0.7146
[Epoch 47] loss: 0.13471548855209442 acc: 0.7218
[Epoch 51] loss: 0.12534442675940674 acc: 0.7154
[Epoch 55] loss: 0.11307706044810584 acc: 0.7158
[Epoch 59] loss: 0.12292815120164258 acc: 0.7184
[Epoch 63] loss: 0.11477075462632091 acc: 0.7273
[Epoch 67] loss: 0.1014756256955035 acc: 0.7197
[Epoch 71] loss: 0.09927868661280159 acc: 0.725
--> [test] acc: 0.7143
--> [accuracy] finished 0.7143
new state: tensor([736.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7143
--> [reward] 0.7143
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7143 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([736.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.7859276778557724 acc: 0.6788
[Epoch 7] loss: 2.4803918820741537 acc: 0.7393
[Epoch 11] loss: 0.929370154471844 acc: 0.7153
[Epoch 15] loss: 0.3960451654365758 acc: 0.7298
[Epoch 19] loss: 0.28900798734353705 acc: 0.7313
[Epoch 23] loss: 0.23038530087246156 acc: 0.7221
[Epoch 27] loss: 0.2034805892702297 acc: 0.7246
[Epoch 31] loss: 0.1738729963622168 acc: 0.7141
[Epoch 35] loss: 0.1707429601822782 acc: 0.7154
[Epoch 39] loss: 0.14257229144787392 acc: 0.7162
[Epoch 43] loss: 0.1440116482756465 acc: 0.715
[Epoch 47] loss: 0.13078508051076565 acc: 0.723
[Epoch 51] loss: 0.12279029095740727 acc: 0.7256
[Epoch 55] loss: 0.12310069637766699 acc: 0.7256
[Epoch 59] loss: 0.10631458590740857 acc: 0.7251
[Epoch 63] loss: 0.11814111229204965 acc: 0.7159
[Epoch 67] loss: 0.10263114664084313 acc: 0.7238
[Epoch 71] loss: 0.10432447275728025 acc: 0.7171
--> [test] acc: 0.7257
--> [accuracy] finished 0.7257
new state: tensor([736.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7257
--> [reward] 0.7257
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7257 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0914, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0908, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3974, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3980, -2.3976,
         -2.3992, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([736.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([736.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.950075276977266 acc: 0.6527
[Epoch 7] loss: 2.75729947413325 acc: 0.7157
[Epoch 11] loss: 1.1642908878205225 acc: 0.7085
[Epoch 15] loss: 0.4958052335025938 acc: 0.7186
[Epoch 19] loss: 0.3227696879278592 acc: 0.7058
[Epoch 23] loss: 0.26630595602009377 acc: 0.7109
[Epoch 27] loss: 0.23561120135685826 acc: 0.7089
[Epoch 31] loss: 0.19521450393302056 acc: 0.7181
[Epoch 35] loss: 0.2022196540258863 acc: 0.7047
[Epoch 39] loss: 0.1640446233989485 acc: 0.7107
[Epoch 43] loss: 0.16003178612238553 acc: 0.715
[Epoch 47] loss: 0.1544568385119619 acc: 0.7071
[Epoch 51] loss: 0.1376053494236925 acc: 0.7026
[Epoch 55] loss: 0.13843639964080603 acc: 0.7138
[Epoch 59] loss: 0.13027897844699335 acc: 0.7033
[Epoch 63] loss: 0.13277931468766135 acc: 0.7116
[Epoch 67] loss: 0.1120347465657036 acc: 0.713
[Epoch 71] loss: 0.12396144728435923 acc: 0.707
--> [test] acc: 0.7022
--> [accuracy] finished 0.7022
new state: tensor([736.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7022
--> [reward] 0.7022
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2457]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4915]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7011]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.8114]], device='cuda:0')
------ ------
delta_t: tensor([[0.7011]], device='cuda:0')
rewards[i]: 0.7022
values[i+1]: tensor([[0.1103]], device='cuda:0')
values[i]: tensor([[0.1104]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7011]], device='cuda:0')
delta_t: tensor([[0.7011]], device='cuda:0')
------ ------
policy_loss: 1.6569218635559082
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7011]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.2522]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.0129]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4188]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.5290]], device='cuda:0')
------ ------
delta_t: tensor([[0.7247]], device='cuda:0')
rewards[i]: 0.7257
values[i+1]: tensor([[0.1104]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1102]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4188]], device='cuda:0')
delta_t: tensor([[0.7247]], device='cuda:0')
------ ------
policy_loss: 5.0350728034973145
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4188]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.4953]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.4863]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1181]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.2280]], device='cuda:0')
------ ------
delta_t: tensor([[0.7135]], device='cuda:0')
rewards[i]: 0.7143
values[i+1]: tensor([[0.1102]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1099]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1181]], device='cuda:0')
delta_t: tensor([[0.7135]], device='cuda:0')
------ ------
policy_loss: 10.086552619934082
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1181]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.4606]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.9305]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.8161]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.9264]], device='cuda:0')
------ ------
delta_t: tensor([[0.7192]], device='cuda:0')
rewards[i]: 0.7207
values[i+1]: tensor([[0.1099]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1103]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.8161]], device='cuda:0')
delta_t: tensor([[0.7192]], device='cuda:0')
------ ------
policy_loss: 16.815505981445312
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.8161]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[13.6187]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.3163]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.5095]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6203]], device='cuda:0')
------ ------
delta_t: tensor([[0.7215]], device='cuda:0')
rewards[i]: 0.7231
values[i+1]: tensor([[0.1103]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1108]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.5095]], device='cuda:0')
delta_t: tensor([[0.7215]], device='cuda:0')
------ ------
policy_loss: 25.20704460144043
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.5095]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[22.4193]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.6011]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1954]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3068]], device='cuda:0')
------ ------
delta_t: tensor([[0.7210]], device='cuda:0')
rewards[i]: 0.7227
values[i+1]: tensor([[0.1108]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1114]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1954]], device='cuda:0')
delta_t: tensor([[0.7210]], device='cuda:0')
------ ------
policy_loss: 35.24217987060547
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1954]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[34.1020]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[23.3655]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.8338]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.9458]], device='cuda:0')
------ ------
delta_t: tensor([[0.6804]], device='cuda:0')
rewards[i]: 0.6821
values[i+1]: tensor([[0.1114]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1120]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.8338]], device='cuda:0')
delta_t: tensor([[0.6804]], device='cuda:0')
------ ------
policy_loss: 46.80651092529297
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.8338]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[49.1826]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[30.1611]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4919]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.6039]], device='cuda:0')
------ ------
delta_t: tensor([[0.7065]], device='cuda:0')
rewards[i]: 0.7076
values[i+1]: tensor([[0.1120]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1120]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4919]], device='cuda:0')
delta_t: tensor([[0.7065]], device='cuda:0')
------ ------
policy_loss: 59.95615005493164
log_probs[i]: tensor([[-2.3987]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4919]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[68.0185]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.6718]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1377]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2498]], device='cuda:0')
------ ------
delta_t: tensor([[0.7007]], device='cuda:0')
rewards[i]: 0.7019
values[i+1]: tensor([[0.1120]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1121]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1377]], device='cuda:0')
delta_t: tensor([[0.7007]], device='cuda:0')
------ ------
policy_loss: 74.65754699707031
log_probs[i]: tensor([[-2.3992]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1377]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[91.0267]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[46.0165]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7835]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8957]], device='cuda:0')
------ ------
delta_t: tensor([[0.7072]], device='cuda:0')
rewards[i]: 0.7084
values[i+1]: tensor([[0.1121]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1122]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7835]], device='cuda:0')
delta_t: tensor([[0.7072]], device='cuda:0')
------ ------
policy_loss: 90.90363311767578
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7835]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[118.6434]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[55.2334]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4319]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5441]], device='cuda:0')
------ ------
delta_t: tensor([[0.7162]], device='cuda:0')
rewards[i]: 0.7173
values[i+1]: tensor([[0.1122]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1121]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4319]], device='cuda:0')
delta_t: tensor([[0.7162]], device='cuda:0')
------ ------
policy_loss: 108.70105743408203
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4319]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[150.8357]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[64.3846]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0240]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1360]], device='cuda:0')
------ ------
delta_t: tensor([[0.6664]], device='cuda:0')
rewards[i]: 0.6674
values[i+1]: tensor([[0.1121]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1120]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0240]], device='cuda:0')
delta_t: tensor([[0.6664]], device='cuda:0')
------ ------
policy_loss: 127.91545104980469
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0240]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[188.1370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.6026]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6373]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7489]], device='cuda:0')
------ ------
delta_t: tensor([[0.6935]], device='cuda:0')
rewards[i]: 0.6942
values[i+1]: tensor([[0.1120]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1116]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6373]], device='cuda:0')
delta_t: tensor([[0.6935]], device='cuda:0')
------ ------
policy_loss: 148.61370849609375
log_probs[i]: tensor([[-2.3992]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6373]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[230.8819]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.4898]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2461]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.3573]], device='cuda:0')
------ ------
delta_t: tensor([[0.6952]], device='cuda:0')
rewards[i]: 0.6959
values[i+1]: tensor([[0.1116]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1112]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2461]], device='cuda:0')
delta_t: tensor([[0.6952]], device='cuda:0')
------ ------
policy_loss: 170.75582885742188
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2461]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[279.4713]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[97.1788]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8579]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.9677]], device='cuda:0')
------ ------
delta_t: tensor([[0.7043]], device='cuda:0')
rewards[i]: 0.704
values[i+1]: tensor([[0.1112]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8579]], device='cuda:0')
delta_t: tensor([[0.7043]], device='cuda:0')
------ ------
policy_loss: 194.38262939453125
log_probs[i]: tensor([[-2.3992]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8579]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 194.38262939453125
value_loss: 279.47125244140625
loss: 334.1182556152344



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.0155e-02, -5.5657e-05, -2.7447e-05, -1.6208e-05, -5.2835e-05],
        [ 2.8208e-01,  1.5491e-03,  7.6067e-04,  4.5235e-04,  1.4783e-03],
        [-1.9009e-03, -1.0373e-05, -5.1371e-06, -3.0127e-06, -9.8221e-06],
        [ 1.2863e+00,  7.0627e-03,  3.4634e-03,  2.0587e-03,  6.7789e-03],
        [ 5.4149e+00,  2.9761e-02,  1.4568e-02,  8.6779e-03,  2.8681e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 1.3653e-04,  8.0395e-05, -6.2946e-05, -8.5992e-05,  1.0336e-04],
        [-3.7840e-03, -2.2316e-03,  1.7420e-03,  2.3865e-03, -2.8636e-03],
        [ 2.5560e-05,  1.5043e-05, -1.1785e-05, -1.6091e-05,  1.9355e-05],
        [-1.7226e-02, -1.0169e-02,  7.9236e-03,  1.0873e-02, -1.3033e-02],
        [-7.2439e-02, -4.2791e-02,  3.3308e-02,  4.5750e-02, -5.4802e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.7234, -1.6902,  1.7218, -1.5001,  1.1313],
        [-0.6133,  0.6015, -0.6128,  0.5340, -0.4026],
        [-0.7566,  0.7421, -0.7559,  0.6586, -0.4967],
        [-0.0449,  0.0440, -0.0448,  0.0391, -0.0295],
        [-0.3085,  0.3026, -0.3082,  0.2685, -0.2025]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0629,  0.0374, -0.0279, -0.0400,  0.0472],
        [-0.0494, -0.0289,  0.0224,  0.0310, -0.0372],
        [-0.0232, -0.0139,  0.0100,  0.0148, -0.0172],
        [-0.0664, -0.0385,  0.0308,  0.0413, -0.0504],
        [-0.0674, -0.0391,  0.0313,  0.0419, -0.0512],
        [-0.0669, -0.0388,  0.0310,  0.0416, -0.0508],
        [ 0.0693,  0.0390, -0.0337, -0.0420,  0.0535],
        [ 0.0067,  0.0037, -0.0029, -0.0040,  0.0048],
        [ 0.1559,  0.0924, -0.0706, -0.0986,  0.1178],
        [-0.0049, -0.0033,  0.0021,  0.0034, -0.0038],
        [-0.0167, -0.0100,  0.0074,  0.0106, -0.0127]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 4.0595,  2.3576, -1.8841, -2.5251,  3.0849]], device='cuda:0')
--> [loss] 334.1182556152344

---------------------------------- [[#26 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7022 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([736.,   2.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.98406736122068 acc: 0.6172
[Epoch 7] loss: 2.772724208776908 acc: 0.7216
[Epoch 11] loss: 1.1976684763684602 acc: 0.707
[Epoch 15] loss: 0.5339864971559218 acc: 0.7135
[Epoch 19] loss: 0.3346199513891774 acc: 0.713
[Epoch 23] loss: 0.27690179961616807 acc: 0.7103
[Epoch 27] loss: 0.2436055587461728 acc: 0.7054
[Epoch 31] loss: 0.20620558026920804 acc: 0.7027
[Epoch 35] loss: 0.18676347261213738 acc: 0.7125
[Epoch 39] loss: 0.18049681737013828 acc: 0.7092
[Epoch 43] loss: 0.15927692920343517 acc: 0.7084
[Epoch 47] loss: 0.15863290122088491 acc: 0.7007
[Epoch 51] loss: 0.14340774627054667 acc: 0.7041
[Epoch 55] loss: 0.13896881100242894 acc: 0.6984
[Epoch 59] loss: 0.13428455448526022 acc: 0.7003
[Epoch 63] loss: 0.11786165892603852 acc: 0.7095
[Epoch 67] loss: 0.12573598094208313 acc: 0.7057
[Epoch 71] loss: 0.10856854463117602 acc: 0.7158
--> [test] acc: 0.7084
--> [accuracy] finished 0.7084
new state: tensor([736.,   2.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7084
--> [reward] 0.7084
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     1.0      |     4.0     | 0.7084 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.647861098389491 acc: 0.693
[Epoch 7] loss: 2.306917000838253 acc: 0.752
[Epoch 11] loss: 0.8351966986037276 acc: 0.7379
[Epoch 15] loss: 0.3760939912961038 acc: 0.743
[Epoch 19] loss: 0.25494044294337864 acc: 0.7546
[Epoch 23] loss: 0.2192513661392395 acc: 0.7537
[Epoch 27] loss: 0.1886795506505367 acc: 0.7375
[Epoch 31] loss: 0.16586373871683008 acc: 0.7431
[Epoch 35] loss: 0.16227323335388202 acc: 0.7433
[Epoch 39] loss: 0.14555521391908569 acc: 0.7491
[Epoch 43] loss: 0.13771162783284016 acc: 0.7398
[Epoch 47] loss: 0.13075155799296181 acc: 0.7303
[Epoch 51] loss: 0.12268374727466061 acc: 0.7436
[Epoch 55] loss: 0.10743302410251707 acc: 0.7435
[Epoch 59] loss: 0.11393011848578025 acc: 0.7442
[Epoch 63] loss: 0.09876495600639916 acc: 0.7335
[Epoch 67] loss: 0.09917306815020388 acc: 0.7397
[Epoch 71] loss: 0.10218737365992363 acc: 0.7247
--> [test] acc: 0.7421
--> [accuracy] finished 0.7421
new state: tensor([736.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7421
--> [reward] 0.7421
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7421 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   2.,   3.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.71636065131868 acc: 0.6783
[Epoch 7] loss: 2.3487118494785046 acc: 0.7473
[Epoch 11] loss: 0.8853001698537175 acc: 0.7309
[Epoch 15] loss: 0.40649901244008096 acc: 0.7317
[Epoch 19] loss: 0.2812263134395337 acc: 0.742
[Epoch 23] loss: 0.2229536783255046 acc: 0.7337
[Epoch 27] loss: 0.20253346818725548 acc: 0.7345
[Epoch 31] loss: 0.1681436941794613 acc: 0.7384
[Epoch 35] loss: 0.1624888932222829 acc: 0.7322
[Epoch 39] loss: 0.14148899703822515 acc: 0.7351
[Epoch 43] loss: 0.14568659064688666 acc: 0.7387
[Epoch 47] loss: 0.12677139807444857 acc: 0.7324
[Epoch 51] loss: 0.12810802023114679 acc: 0.7255
[Epoch 55] loss: 0.11677953268772305 acc: 0.7297
[Epoch 59] loss: 0.10922016604932125 acc: 0.7339
[Epoch 63] loss: 0.10711005442541169 acc: 0.7195
[Epoch 67] loss: 0.09783792155354148 acc: 0.735
[Epoch 71] loss: 0.09610582810119175 acc: 0.7266
--> [test] acc: 0.7147
--> [accuracy] finished 0.7147
new state: tensor([736.,   2.,   3.,   2.,   4.], device='cuda:0')
new reward: 0.7147
--> [reward] 0.7147
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     3.0     |     2.0      |     4.0     | 0.7147 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0926, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3962, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   3.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   2.,   3.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.724758380209393 acc: 0.6946
[Epoch 7] loss: 2.38972541617463 acc: 0.7352
[Epoch 11] loss: 0.9117905970977243 acc: 0.7321
[Epoch 15] loss: 0.4019622190062271 acc: 0.7333
[Epoch 19] loss: 0.2902555878981567 acc: 0.73
[Epoch 23] loss: 0.22985765153346846 acc: 0.7345
[Epoch 27] loss: 0.20499703898559066 acc: 0.7268
[Epoch 31] loss: 0.17548134084790945 acc: 0.7312
[Epoch 35] loss: 0.15647812289855137 acc: 0.7299
[Epoch 39] loss: 0.14941800020568435 acc: 0.7335
[Epoch 43] loss: 0.13756760108568097 acc: 0.7327
[Epoch 47] loss: 0.1318951020019648 acc: 0.7411
[Epoch 51] loss: 0.11783361929508111 acc: 0.7269
[Epoch 55] loss: 0.12367187267707666 acc: 0.7288
[Epoch 59] loss: 0.1026183374297848 acc: 0.7208
[Epoch 63] loss: 0.10714646046379786 acc: 0.7252
[Epoch 67] loss: 0.10432102808328655 acc: 0.7308
[Epoch 71] loss: 0.09905895711045744 acc: 0.734
--> [test] acc: 0.7249
--> [accuracy] finished 0.7249
new state: tensor([704.,   2.,   3.,   2.,   4.], device='cuda:0')
new reward: 0.7249
--> [reward] 0.7249
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     3.0     |     2.0      |     4.0     | 0.7249 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   3.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([704.,   1.,   3.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.24414771719052 acc: 0.6324
[Epoch 7] loss: 3.0482565001453583 acc: 0.7015
[Epoch 11] loss: 1.5518373873113367 acc: 0.6962
[Epoch 15] loss: 0.6913769773310027 acc: 0.6968
[Epoch 19] loss: 0.4145276054616093 acc: 0.6984
[Epoch 23] loss: 0.3205608158560513 acc: 0.6852
[Epoch 27] loss: 0.27251328628919924 acc: 0.69
[Epoch 31] loss: 0.2477175010906537 acc: 0.6978
[Epoch 35] loss: 0.22782866315692282 acc: 0.6933
[Epoch 39] loss: 0.2136654423721268 acc: 0.6953
[Epoch 43] loss: 0.17801532085003602 acc: 0.6942
[Epoch 47] loss: 0.18180696730432874 acc: 0.6934
[Epoch 51] loss: 0.1765221719078653 acc: 0.6869
[Epoch 55] loss: 0.16000310891448422 acc: 0.6897
[Epoch 59] loss: 0.1547337716154735 acc: 0.6884
[Epoch 63] loss: 0.157818927230579 acc: 0.6951
[Epoch 67] loss: 0.14065597492598875 acc: 0.6858
[Epoch 71] loss: 0.13897594285097517 acc: 0.6875
--> [test] acc: 0.687
--> [accuracy] finished 0.687
new state: tensor([704.,   1.,   3.,   2.,   4.], device='cuda:0')
new reward: 0.687
--> [reward] 0.687
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     2.0      |     4.0     | 0.687  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.371600043285838 acc: 0.6201
[Epoch 7] loss: 3.274542662112609 acc: 0.6916
[Epoch 11] loss: 1.7412115762300808 acc: 0.6756
[Epoch 15] loss: 0.7880711249645104 acc: 0.6718
[Epoch 19] loss: 0.47169253528785066 acc: 0.6759
[Epoch 23] loss: 0.34998754771840773 acc: 0.6742
[Epoch 27] loss: 0.2998756308542074 acc: 0.6808
[Epoch 31] loss: 0.2734532001335412 acc: 0.6789
[Epoch 35] loss: 0.2431287438468174 acc: 0.6786
[Epoch 39] loss: 0.22765625390650518 acc: 0.6757
[Epoch 43] loss: 0.2054806544309687 acc: 0.6788
[Epoch 47] loss: 0.1962059019478824 acc: 0.6607
[Epoch 51] loss: 0.18784453410207463 acc: 0.673
[Epoch 55] loss: 0.1693320728998031 acc: 0.6702
[Epoch 59] loss: 0.16515974359601127 acc: 0.6758
[Epoch 63] loss: 0.15717927617547306 acc: 0.6664
[Epoch 67] loss: 0.15370119010667552 acc: 0.6699
[Epoch 71] loss: 0.15582873542433429 acc: 0.6672
--> [test] acc: 0.6665
--> [accuracy] finished 0.6665
new state: tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6665
--> [reward] 0.6665
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     2.0      |     5.0     | 0.6665 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.385806184900386 acc: 0.6179
[Epoch 7] loss: 3.2335471545186496 acc: 0.6897
[Epoch 11] loss: 1.7061108825990305 acc: 0.6713
[Epoch 15] loss: 0.7736296888031161 acc: 0.6691
[Epoch 19] loss: 0.44504897790434567 acc: 0.677
[Epoch 23] loss: 0.3603547826633238 acc: 0.6744
[Epoch 27] loss: 0.2917145367121071 acc: 0.6751
[Epoch 31] loss: 0.2602576114132505 acc: 0.6606
[Epoch 35] loss: 0.2425727034606935 acc: 0.6755
[Epoch 39] loss: 0.22380556031594726 acc: 0.6762
[Epoch 43] loss: 0.19263195652452766 acc: 0.6833
[Epoch 47] loss: 0.19502814742676972 acc: 0.688
[Epoch 51] loss: 0.1873422438478874 acc: 0.6799
[Epoch 55] loss: 0.1763423306410751 acc: 0.6772
[Epoch 59] loss: 0.16864393680425518 acc: 0.6746
[Epoch 63] loss: 0.14960321836183063 acc: 0.6821
[Epoch 67] loss: 0.15263179898955156 acc: 0.669
[Epoch 71] loss: 0.1410547940017622 acc: 0.6748
--> [test] acc: 0.6785
--> [accuracy] finished 0.6785
new state: tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6785
--> [reward] 0.6785
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     2.0      |     5.0     | 0.6785 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([704.,   2.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.889977835175936 acc: 0.662
[Epoch 7] loss: 2.591391605336953 acc: 0.7082
[Epoch 11] loss: 1.0392204718783384 acc: 0.7176
[Epoch 15] loss: 0.4598212694401479 acc: 0.7143
[Epoch 19] loss: 0.30295032536005956 acc: 0.7105
[Epoch 23] loss: 0.25180854314647594 acc: 0.7089
[Epoch 27] loss: 0.20496769653409339 acc: 0.7081
[Epoch 31] loss: 0.1893634450600466 acc: 0.7193
[Epoch 35] loss: 0.17036483456116275 acc: 0.7079
[Epoch 39] loss: 0.16066419176966942 acc: 0.7104
[Epoch 43] loss: 0.14764365646988153 acc: 0.7108
[Epoch 47] loss: 0.13693414938926715 acc: 0.7092
[Epoch 51] loss: 0.138624767648548 acc: 0.707
[Epoch 55] loss: 0.11619362901643757 acc: 0.7003
[Epoch 59] loss: 0.12047531972508259 acc: 0.7104
[Epoch 63] loss: 0.10817992999849607 acc: 0.7076
[Epoch 67] loss: 0.10550514804770994 acc: 0.7037
[Epoch 71] loss: 0.10246750872696528 acc: 0.7122
--> [test] acc: 0.7101
--> [accuracy] finished 0.7101
new state: tensor([704.,   2.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.7101
--> [reward] 0.7101
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     3.0     |     2.0      |     5.0     | 0.7101 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.351046490242414 acc: 0.6179
[Epoch 7] loss: 3.206304279968257 acc: 0.683
[Epoch 11] loss: 1.7250932792720892 acc: 0.6874
[Epoch 15] loss: 0.7775012666497694 acc: 0.6829
[Epoch 19] loss: 0.45953180873409255 acc: 0.6837
[Epoch 23] loss: 0.3478311280388852 acc: 0.6855
[Epoch 27] loss: 0.2972779294804615 acc: 0.6889
[Epoch 31] loss: 0.27309909425294765 acc: 0.6744
[Epoch 35] loss: 0.2389900279648202 acc: 0.6872
[Epoch 39] loss: 0.21321366830964755 acc: 0.6726
[Epoch 43] loss: 0.22198726722965842 acc: 0.674
[Epoch 47] loss: 0.1866191927274532 acc: 0.6708
[Epoch 51] loss: 0.1940987019197029 acc: 0.6718
[Epoch 55] loss: 0.17112594702616907 acc: 0.6666
[Epoch 59] loss: 0.16769238006171133 acc: 0.6788
[Epoch 63] loss: 0.1533576848218813 acc: 0.6711
[Epoch 67] loss: 0.15739796914236473 acc: 0.6815
[Epoch 71] loss: 0.14473240283410757 acc: 0.6812
--> [test] acc: 0.6763
--> [accuracy] finished 0.6763
new state: tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6763
--> [reward] 0.6763
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     2.0      |     5.0     | 0.6763 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([704.,   1.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.289746363449584 acc: 0.5115
[Epoch 7] loss: 4.560926059780218 acc: 0.5725
[Epoch 11] loss: 3.211837952932738 acc: 0.5792
[Epoch 15] loss: 1.8602074915567017 acc: 0.5835
[Epoch 19] loss: 1.017989007284498 acc: 0.5708
[Epoch 23] loss: 0.6774528655021087 acc: 0.5617
[Epoch 27] loss: 0.5348147763858747 acc: 0.5682
[Epoch 31] loss: 0.4421963641500991 acc: 0.5702
[Epoch 35] loss: 0.4083182232363907 acc: 0.5699
[Epoch 39] loss: 0.35470846681938983 acc: 0.5639
[Epoch 43] loss: 0.3295876645147229 acc: 0.5626
[Epoch 47] loss: 0.31754928345665756 acc: 0.5589
[Epoch 51] loss: 0.2874477869185531 acc: 0.5612
[Epoch 55] loss: 0.2577187689545248 acc: 0.5541
[Epoch 59] loss: 0.2648631610581294 acc: 0.564
[Epoch 63] loss: 0.24699064994426181 acc: 0.5641
[Epoch 67] loss: 0.24572566487407194 acc: 0.5625
[Epoch 71] loss: 0.22433494337026955 acc: 0.5623
--> [test] acc: 0.5623
--> [accuracy] finished 0.5623
new state: tensor([704.,   1.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.5623
--> [reward] 0.5623
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     3.0      |     5.0     | 0.5623 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([704.,   1.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.2828331167435705 acc: 0.5124
[Epoch 7] loss: 4.591454739765743 acc: 0.5763
[Epoch 11] loss: 3.2523827423220095 acc: 0.5704
[Epoch 15] loss: 1.8882990296920548 acc: 0.5637
[Epoch 19] loss: 1.0264334469995535 acc: 0.5571
[Epoch 23] loss: 0.6779289199777728 acc: 0.5562
[Epoch 27] loss: 0.5387269509170214 acc: 0.5595
[Epoch 31] loss: 0.4433855876169356 acc: 0.554
[Epoch 35] loss: 0.39267953711054515 acc: 0.5617
[Epoch 39] loss: 0.3722186768999147 acc: 0.5615
[Epoch 43] loss: 0.32511991992249817 acc: 0.5477
[Epoch 47] loss: 0.3097907071552046 acc: 0.5445
[Epoch 51] loss: 0.29015810260801667 acc: 0.5536
[Epoch 55] loss: 0.2762258508935799 acc: 0.5533
[Epoch 59] loss: 0.25794574875703746 acc: 0.5596
[Epoch 63] loss: 0.24939132658788538 acc: 0.5493
[Epoch 67] loss: 0.23176469983261488 acc: 0.5591
[Epoch 71] loss: 0.2332982895276545 acc: 0.5528
--> [test] acc: 0.5432
--> [accuracy] finished 0.5432
new state: tensor([704.,   1.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.5432
--> [reward] 0.5432
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     3.0      |     5.0     | 0.5432 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([704.,   1.,   2.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.510612206690756 acc: 0.4863
[Epoch 7] loss: 5.017160785472607 acc: 0.5431
[Epoch 11] loss: 3.920639730780326 acc: 0.5459
[Epoch 15] loss: 2.656472133020001 acc: 0.544
[Epoch 19] loss: 1.5499233640825656 acc: 0.5313
[Epoch 23] loss: 0.9678797274827957 acc: 0.5337
[Epoch 27] loss: 0.7093752939682787 acc: 0.5278
[Epoch 31] loss: 0.5854005592939494 acc: 0.5333
[Epoch 35] loss: 0.5154832404659456 acc: 0.5218
[Epoch 39] loss: 0.47748041746523373 acc: 0.5208
[Epoch 43] loss: 0.4298221500747649 acc: 0.5351
[Epoch 47] loss: 0.3843708527857995 acc: 0.5266
[Epoch 51] loss: 0.3845263655318895 acc: 0.5228
[Epoch 55] loss: 0.3484062158462146 acc: 0.5232
[Epoch 59] loss: 0.3268604906171065 acc: 0.5308
[Epoch 63] loss: 0.32496383669964796 acc: 0.5225
[Epoch 67] loss: 0.308150301153874 acc: 0.5223
[Epoch 71] loss: 0.29900644806301807 acc: 0.5255
--> [test] acc: 0.5215
--> [accuracy] finished 0.5215
new state: tensor([704.,   1.,   2.,   3.,   5.], device='cuda:0')
new reward: 0.5215
--> [reward] 0.5215
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     2.0     |     3.0      |     5.0     | 0.5215 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   2.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([704.,   1.,   2.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.3456745537955435 acc: 0.514
[Epoch 7] loss: 4.8241934828136275 acc: 0.5605
[Epoch 11] loss: 3.691074168438192 acc: 0.5744
[Epoch 15] loss: 2.4192542889538933 acc: 0.5582
[Epoch 19] loss: 1.3730734979443233 acc: 0.5557
[Epoch 23] loss: 0.8590059001427477 acc: 0.5584
[Epoch 27] loss: 0.6288252253456951 acc: 0.5387
[Epoch 31] loss: 0.5272915797817814 acc: 0.5575
[Epoch 35] loss: 0.4718105022169059 acc: 0.5485
[Epoch 39] loss: 0.42669403648285 acc: 0.5467
[Epoch 43] loss: 0.3833585328058056 acc: 0.5471
[Epoch 47] loss: 0.3639570085136482 acc: 0.5492
[Epoch 51] loss: 0.3447545638565174 acc: 0.5435
[Epoch 55] loss: 0.31060648502310373 acc: 0.5468
[Epoch 59] loss: 0.317691356198543 acc: 0.5495
[Epoch 63] loss: 0.2807293107728367 acc: 0.5482
[Epoch 67] loss: 0.28809751795552424 acc: 0.5493
[Epoch 71] loss: 0.26706650531838844 acc: 0.5503
--> [test] acc: 0.5381
--> [accuracy] finished 0.5381
new state: tensor([704.,   1.,   2.,   3.,   4.], device='cuda:0')
new reward: 0.5381
--> [reward] 0.5381
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     2.0     |     3.0      |     4.0     | 0.5381 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   2.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   1.,   2.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.397220907613749 acc: 0.5062
[Epoch 7] loss: 4.8462882101383356 acc: 0.5623
[Epoch 11] loss: 3.720475935417673 acc: 0.5701
[Epoch 15] loss: 2.4653935263986173 acc: 0.5604
[Epoch 19] loss: 1.3933496067438589 acc: 0.5506
[Epoch 23] loss: 0.873016571487917 acc: 0.5557
[Epoch 27] loss: 0.6438563387659962 acc: 0.5522
[Epoch 31] loss: 0.5453457568993654 acc: 0.5492
[Epoch 35] loss: 0.475132685754915 acc: 0.5456
[Epoch 39] loss: 0.4215103018161891 acc: 0.5402
[Epoch 43] loss: 0.421456532867249 acc: 0.5526
[Epoch 47] loss: 0.3614560917539098 acc: 0.5454
[Epoch 51] loss: 0.3640047963136983 acc: 0.5454
[Epoch 55] loss: 0.3260551470015055 acc: 0.5506
[Epoch 59] loss: 0.3193712306144597 acc: 0.5397
[Epoch 63] loss: 0.319279204858729 acc: 0.5374
[Epoch 67] loss: 0.27236830099435794 acc: 0.5434
[Epoch 71] loss: 0.2846879929809086 acc: 0.5432
--> [test] acc: 0.5373
--> [accuracy] finished 0.5373
new state: tensor([672.,   1.,   2.,   3.,   4.], device='cuda:0')
new reward: 0.5373
--> [reward] 0.5373
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     3.0      |     4.0     | 0.5373 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0903, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0901]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3985, -2.3973, -2.3979, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3987]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   2.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.369010590226449 acc: 0.4952
[Epoch 7] loss: 4.8733824961020815 acc: 0.5419
[Epoch 11] loss: 3.773522559029367 acc: 0.5514
[Epoch 15] loss: 2.513464961667805 acc: 0.5487
[Epoch 19] loss: 1.448648286197344 acc: 0.5406
[Epoch 23] loss: 0.8959313894686339 acc: 0.5342
[Epoch 27] loss: 0.65202108631506 acc: 0.5407
[Epoch 31] loss: 0.5412282963161883 acc: 0.5369
[Epoch 35] loss: 0.48786932359094665 acc: 0.543
[Epoch 39] loss: 0.4307662079639523 acc: 0.5374
[Epoch 43] loss: 0.41129544816072794 acc: 0.5356
[Epoch 47] loss: 0.37561984311627306 acc: 0.5368
[Epoch 51] loss: 0.35242388034691974 acc: 0.5384
[Epoch 55] loss: 0.32581835960888345 acc: 0.5365
[Epoch 59] loss: 0.3300690695409999 acc: 0.5446
[Epoch 63] loss: 0.29683828678296503 acc: 0.5279
[Epoch 67] loss: 0.2923874673636063 acc: 0.5384
[Epoch 71] loss: 0.27497973439552825 acc: 0.5443
--> [test] acc: 0.5346
--> [accuracy] finished 0.5346
new state: tensor([672.,   1.,   2.,   3.,   3.], device='cuda:0')
new reward: 0.5346
--> [reward] 0.5346
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1422]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.2844]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.5333]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6465]], device='cuda:0')
------ ------
delta_t: tensor([[0.5333]], device='cuda:0')
rewards[i]: 0.5346
values[i+1]: tensor([[0.1130]], device='cuda:0')
values[i]: tensor([[0.1132]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.5333]], device='cuda:0')
delta_t: tensor([[0.5333]], device='cuda:0')
------ ------
policy_loss: 1.2555499076843262
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.5333]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.7080]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.1316]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.0637]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.1774]], device='cuda:0')
------ ------
delta_t: tensor([[0.5357]], device='cuda:0')
rewards[i]: 0.5373
values[i+1]: tensor([[0.1132]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1136]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.0637]], device='cuda:0')
delta_t: tensor([[0.5357]], device='cuda:0')
------ ------
policy_loss: 3.7817299365997314
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.0637]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[1.9725]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.5289]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.5903]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.7037]], device='cuda:0')
------ ------
delta_t: tensor([[0.5371]], device='cuda:0')
rewards[i]: 0.5381
values[i+1]: tensor([[0.1136]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1134]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.5903]], device='cuda:0')
delta_t: tensor([[0.5371]], device='cuda:0')
------ ------
policy_loss: 7.572972774505615
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.5903]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[4.1668]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.3886]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.0949]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.2081]], device='cuda:0')
------ ------
delta_t: tensor([[0.5206]], device='cuda:0')
rewards[i]: 0.5215
values[i+1]: tensor([[0.1134]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1132]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.0949]], device='cuda:0')
delta_t: tensor([[0.5206]], device='cuda:0')
------ ------
policy_loss: 12.571128845214844
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.0949]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[7.5882]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.8429]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.6159]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7293]], device='cuda:0')
------ ------
delta_t: tensor([[0.5419]], device='cuda:0')
rewards[i]: 0.5432
values[i+1]: tensor([[0.1132]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1134]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.6159]], device='cuda:0')
delta_t: tensor([[0.5419]], device='cuda:0')
------ ------
policy_loss: 18.821304321289062
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.6159]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[12.5516]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.9267]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.1507]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.2643]], device='cuda:0')
------ ------
delta_t: tensor([[0.5609]], device='cuda:0')
rewards[i]: 0.5623
values[i+1]: tensor([[0.1134]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1136]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.1507]], device='cuda:0')
delta_t: tensor([[0.5609]], device='cuda:0')
------ ------
policy_loss: 26.351377487182617
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.1507]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[19.7487]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.3943]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.7940]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.9079]], device='cuda:0')
------ ------
delta_t: tensor([[0.6748]], device='cuda:0')
rewards[i]: 0.6763
values[i+1]: tensor([[0.1136]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1139]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.7940]], device='cuda:0')
delta_t: tensor([[0.6748]], device='cuda:0')
------ ------
policy_loss: 35.42420959472656
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.7940]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[29.7167]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.9360]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.4650]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.5789]], device='cuda:0')
------ ------
delta_t: tensor([[0.7089]], device='cuda:0')
rewards[i]: 0.7101
values[i+1]: tensor([[0.1139]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1140]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.4650]], device='cuda:0')
delta_t: tensor([[0.7089]], device='cuda:0')
------ ------
policy_loss: 46.10932540893555
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.4650]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[42.7077]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[25.9820]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.0973]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.2117]], device='cuda:0')
------ ------
delta_t: tensor([[0.6769]], device='cuda:0')
rewards[i]: 0.6785
values[i+1]: tensor([[0.1140]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1144]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.0973]], device='cuda:0')
delta_t: tensor([[0.6769]], device='cuda:0')
------ ------
policy_loss: 58.307003021240234
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.0973]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[59.0161]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.6167]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.7111]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.8260]], device='cuda:0')
------ ------
delta_t: tensor([[0.6648]], device='cuda:0')
rewards[i]: 0.6665
values[i+1]: tensor([[0.1144]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1149]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.7111]], device='cuda:0')
delta_t: tensor([[0.6648]], device='cuda:0')
------ ------
policy_loss: 71.9809799194336
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.7111]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[79.1097]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[40.1872]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3393]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4548]], device='cuda:0')
------ ------
delta_t: tensor([[0.6853]], device='cuda:0')
rewards[i]: 0.687
values[i+1]: tensor([[0.1149]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1154]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3393]], device='cuda:0')
delta_t: tensor([[0.6853]], device='cuda:0')
------ ------
policy_loss: 87.15678405761719
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3393]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[103.6043]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[48.9892]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9992]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1151]], device='cuda:0')
------ ------
delta_t: tensor([[0.7233]], device='cuda:0')
rewards[i]: 0.7249
values[i+1]: tensor([[0.1154]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1159]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9992]], device='cuda:0')
delta_t: tensor([[0.7233]], device='cuda:0')
------ ------
policy_loss: 103.91224670410156
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9992]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[132.8127]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[58.4170]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.6431]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7587]], device='cuda:0')
------ ------
delta_t: tensor([[0.7139]], device='cuda:0')
rewards[i]: 0.7147
values[i+1]: tensor([[0.1159]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1156]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.6431]], device='cuda:0')
delta_t: tensor([[0.7139]], device='cuda:0')
------ ------
policy_loss: 122.21336364746094
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.6431]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[167.3254]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[69.0254]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.3082]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.4232]], device='cuda:0')
------ ------
delta_t: tensor([[0.7415]], device='cuda:0')
rewards[i]: 0.7421
values[i+1]: tensor([[0.1156]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1150]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.3082]], device='cuda:0')
delta_t: tensor([[0.7415]], device='cuda:0')
------ ------
policy_loss: 142.1116485595703
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.3082]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[207.2294]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[79.8080]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.9335]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.0474]], device='cuda:0')
------ ------
delta_t: tensor([[0.7085]], device='cuda:0')
rewards[i]: 0.7084
values[i+1]: tensor([[0.1150]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1138]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.9335]], device='cuda:0')
delta_t: tensor([[0.7085]], device='cuda:0')
------ ------
policy_loss: 163.51454162597656
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.9335]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 163.51454162597656
value_loss: 207.2294158935547
loss: 267.1292419433594



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.0753e-02, -3.0558e-05, -3.7717e-05, -2.2801e-05, -5.7832e-05],
        [ 2.7195e-01,  7.8571e-04,  9.4253e-04,  5.6116e-04,  1.4434e-03],
        [-1.8260e-03, -5.0948e-06, -6.4607e-06, -3.9272e-06, -9.8947e-06],
        [ 1.1784e+00,  3.3984e-03,  4.0814e-03,  2.4180e-03,  6.2463e-03],
        [ 4.5892e+00,  1.3312e-02,  1.5855e-02,  9.3658e-03,  2.4259e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 1.4761e-04,  8.3111e-05, -7.1428e-05, -8.8891e-05,  1.1486e-04],
        [-3.7271e-03, -2.1010e-03,  1.8002e-03,  2.2474e-03, -2.8984e-03],
        [ 2.5072e-05,  1.4110e-05, -1.2133e-05, -1.5089e-05,  1.9515e-05],
        [-1.6141e-02, -9.1004e-03,  7.7899e-03,  9.7347e-03, -1.2550e-02],
        [-6.2832e-02, -3.5434e-02,  3.0316e-02,  3.7904e-02, -4.8848e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.4746, -1.4420,  1.4728, -1.2637,  0.9543],
        [-0.5455,  0.5335, -0.5449,  0.4675, -0.3531],
        [-0.6345,  0.6205, -0.6338,  0.5437, -0.4106],
        [-0.0369,  0.0361, -0.0369,  0.0317, -0.0239],
        [-0.2576,  0.2519, -0.2573,  0.2207, -0.1667]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0180,  0.0100, -0.0082, -0.0108,  0.0136],
        [-0.0571, -0.0318,  0.0278,  0.0340, -0.0446],
        [ 0.0802,  0.0440, -0.0407, -0.0469,  0.0634],
        [-0.0165, -0.0094,  0.0073,  0.0101, -0.0124],
        [-0.0380, -0.0212,  0.0181,  0.0227, -0.0294],
        [ 0.0205,  0.0118, -0.0085, -0.0128,  0.0152],
        [-0.0559, -0.0312,  0.0273,  0.0333, -0.0438],
        [ 0.0422,  0.0236, -0.0201, -0.0252,  0.0327],
        [-0.0368, -0.0206,  0.0176,  0.0220, -0.0285],
        [ 0.0987,  0.0554, -0.0475, -0.0593,  0.0771],
        [-0.0554, -0.0309,  0.0270,  0.0330, -0.0433]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.3874,  1.8874, -1.6510, -2.0154,  2.6493]], device='cuda:0')
--> [loss] 267.1292419433594

---------------------------------- [[#27 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     3.0      |     3.0     | 0.5346 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.76448447960417 acc: 0.4615
[Epoch 7] loss: 5.502505780790773 acc: 0.494
[Epoch 11] loss: 4.7010637331191845 acc: 0.5195
[Epoch 15] loss: 3.800091022115839 acc: 0.5053
[Epoch 19] loss: 2.73169217428283 acc: 0.4992
[Epoch 23] loss: 1.780836045665814 acc: 0.4859
[Epoch 27] loss: 1.2059818743668553 acc: 0.4804
[Epoch 31] loss: 0.9060666148391221 acc: 0.4864
[Epoch 35] loss: 0.7617140256173318 acc: 0.4815
[Epoch 39] loss: 0.6850660184345891 acc: 0.4801
[Epoch 43] loss: 0.6301071629585588 acc: 0.4858
[Epoch 47] loss: 0.5804734736909647 acc: 0.4802
[Epoch 51] loss: 0.5348841523861184 acc: 0.4824
[Epoch 55] loss: 0.5202418220422381 acc: 0.4802
[Epoch 59] loss: 0.49394270808190643 acc: 0.4793
[Epoch 63] loss: 0.44330132697635066 acc: 0.4738
[Epoch 67] loss: 0.45253272151665 acc: 0.4826
[Epoch 71] loss: 0.39264202852974006 acc: 0.4735
--> [test] acc: 0.4735
--> [accuracy] finished 0.4735
new state: tensor([672.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.4735
--> [reward] 0.4735
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.4735 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   1.,   1.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.700587738200527 acc: 0.4566
[Epoch 7] loss: 5.3867741667706035 acc: 0.5031
[Epoch 11] loss: 4.569299269515231 acc: 0.5316
[Epoch 15] loss: 3.6757153719282516 acc: 0.5224
[Epoch 19] loss: 2.595555888036328 acc: 0.5208
[Epoch 23] loss: 1.6456643911578772 acc: 0.5067
[Epoch 27] loss: 1.1315024258459316 acc: 0.5055
[Epoch 31] loss: 0.8508226695801596 acc: 0.4951
[Epoch 35] loss: 0.7296828960004212 acc: 0.5027
[Epoch 39] loss: 0.6463959094356088 acc: 0.5004
[Epoch 43] loss: 0.6014391128974192 acc: 0.4935
[Epoch 47] loss: 0.557131723293563 acc: 0.501
[Epoch 51] loss: 0.49239804464228015 acc: 0.4931
[Epoch 55] loss: 0.49322263683523515 acc: 0.4998
[Epoch 59] loss: 0.45720624921085967 acc: 0.4932
[Epoch 63] loss: 0.4736234370304648 acc: 0.4926
[Epoch 67] loss: 0.42042566817777843 acc: 0.4951
[Epoch 71] loss: 0.3951170399947011 acc: 0.4904
--> [test] acc: 0.4944
--> [accuracy] finished 0.4944
new state: tensor([672.,   1.,   1.,   3.,   4.], device='cuda:0')
new reward: 0.4944
--> [reward] 0.4944
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     1.0     |     3.0      |     4.0     | 0.4944 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   1.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([672.,   1.,   2.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.377566566857536 acc: 0.5101
[Epoch 7] loss: 4.837569480966729 acc: 0.5476
[Epoch 11] loss: 3.718917329293078 acc: 0.5742
[Epoch 15] loss: 2.4864719767704644 acc: 0.5566
[Epoch 19] loss: 1.4249080156959841 acc: 0.553
[Epoch 23] loss: 0.8975520878553848 acc: 0.5545
[Epoch 27] loss: 0.648243330952609 acc: 0.5455
[Epoch 31] loss: 0.5449318352638913 acc: 0.5386
[Epoch 35] loss: 0.468295324131694 acc: 0.5411
[Epoch 39] loss: 0.43779518503857695 acc: 0.5378
[Epoch 43] loss: 0.4011607757505134 acc: 0.5383
[Epoch 47] loss: 0.37200179223990654 acc: 0.5408
[Epoch 51] loss: 0.33311422113233896 acc: 0.5461
[Epoch 55] loss: 0.3289526158877079 acc: 0.5392
[Epoch 59] loss: 0.30974475416066627 acc: 0.5297
[Epoch 63] loss: 0.30016602278041565 acc: 0.552
[Epoch 67] loss: 0.2813417907051571 acc: 0.5422
[Epoch 71] loss: 0.2722644273494192 acc: 0.5395
--> [test] acc: 0.5459
--> [accuracy] finished 0.5459
new state: tensor([672.,   1.,   2.,   3.,   4.], device='cuda:0')
new reward: 0.5459
--> [reward] 0.5459
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     3.0      |     4.0     | 0.5459 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.483074771321339 acc: 0.6058
[Epoch 7] loss: 3.481968853784644 acc: 0.6762
[Epoch 11] loss: 2.0997598889233817 acc: 0.6755
[Epoch 15] loss: 1.0482734089426677 acc: 0.6759
[Epoch 19] loss: 0.5680270701827829 acc: 0.669
[Epoch 23] loss: 0.42390966151967224 acc: 0.6687
[Epoch 27] loss: 0.36295444072674377 acc: 0.6687
[Epoch 31] loss: 0.3052961181591996 acc: 0.6631
[Epoch 35] loss: 0.28664707320520794 acc: 0.6654
[Epoch 39] loss: 0.2549355185716925 acc: 0.6632
[Epoch 43] loss: 0.2438136067460565 acc: 0.6737
[Epoch 47] loss: 0.235112246728557 acc: 0.6586
[Epoch 51] loss: 0.21447921042447277 acc: 0.6625
[Epoch 55] loss: 0.20062338321915138 acc: 0.6601
[Epoch 59] loss: 0.2073254734777924 acc: 0.6482
[Epoch 63] loss: 0.17825767994829148 acc: 0.6631
[Epoch 67] loss: 0.18308174467879013 acc: 0.6646
[Epoch 71] loss: 0.16311678093681803 acc: 0.6498
--> [test] acc: 0.6592
--> [accuracy] finished 0.6592
new state: tensor([672.,   1.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6592
--> [reward] 0.6592
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     2.0      |     4.0     | 0.6592 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([672.,   2.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.0559324837096815 acc: 0.6398
[Epoch 7] loss: 2.8323269013858514 acc: 0.6985
[Epoch 11] loss: 1.3149157164956602 acc: 0.7123
[Epoch 15] loss: 0.5776720431483234 acc: 0.7073
[Epoch 19] loss: 0.35723773929316677 acc: 0.6998
[Epoch 23] loss: 0.30008507342032537 acc: 0.7053
[Epoch 27] loss: 0.2502122053404903 acc: 0.7036
[Epoch 31] loss: 0.22766215211528418 acc: 0.7086
[Epoch 35] loss: 0.20313751693014676 acc: 0.7029
[Epoch 39] loss: 0.1838939409212345 acc: 0.6934
[Epoch 43] loss: 0.17273361277778435 acc: 0.6982
[Epoch 47] loss: 0.16259533137111637 acc: 0.7052
[Epoch 51] loss: 0.15019948992644772 acc: 0.7027
[Epoch 55] loss: 0.15476042604374002 acc: 0.7032
[Epoch 59] loss: 0.1407274766408307 acc: 0.7031
[Epoch 63] loss: 0.13924896782097382 acc: 0.7008
[Epoch 67] loss: 0.11966965076349237 acc: 0.6995
[Epoch 71] loss: 0.12191354237494471 acc: 0.6986
--> [test] acc: 0.6987
--> [accuracy] finished 0.6987
new state: tensor([672.,   2.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6987
--> [reward] 0.6987
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     4.0     | 0.6987 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   2.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.972812219349015 acc: 0.6437
[Epoch 7] loss: 2.7696803485798407 acc: 0.701
[Epoch 11] loss: 1.1986691978524255 acc: 0.7142
[Epoch 15] loss: 0.5178015307211281 acc: 0.7123
[Epoch 19] loss: 0.32660904362835846 acc: 0.7093
[Epoch 23] loss: 0.27510530349400725 acc: 0.7076
[Epoch 27] loss: 0.23531639912163319 acc: 0.7152
[Epoch 31] loss: 0.2005686930230702 acc: 0.7125
[Epoch 35] loss: 0.19282397820764338 acc: 0.7102
[Epoch 39] loss: 0.17122642192370294 acc: 0.7091
[Epoch 43] loss: 0.16755637583915917 acc: 0.7073
[Epoch 47] loss: 0.1545077125372751 acc: 0.7131
[Epoch 51] loss: 0.14715770716586 acc: 0.7032
[Epoch 55] loss: 0.13302101733405952 acc: 0.7091
[Epoch 59] loss: 0.14207145164284826 acc: 0.7034
[Epoch 63] loss: 0.12089201932310906 acc: 0.7018
[Epoch 67] loss: 0.12613761881747476 acc: 0.7022
[Epoch 71] loss: 0.11536910553442557 acc: 0.7046
--> [test] acc: 0.6917
--> [accuracy] finished 0.6917
new state: tensor([672.,   2.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.6917
--> [reward] 0.6917
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     1.0      |     4.0     | 0.6917 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([672.,   2.,   2.,   2.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.020399024267026 acc: 0.6207
[Epoch 7] loss: 2.8299963590891464 acc: 0.7104
[Epoch 11] loss: 1.3256494232532008 acc: 0.7103
[Epoch 15] loss: 0.5709358622198519 acc: 0.7084
[Epoch 19] loss: 0.35800832791058607 acc: 0.6997
[Epoch 23] loss: 0.2902597033554483 acc: 0.7053
[Epoch 27] loss: 0.24542349819606052 acc: 0.7047
[Epoch 31] loss: 0.22010350021321679 acc: 0.7067
[Epoch 35] loss: 0.20212190373159963 acc: 0.6971
[Epoch 39] loss: 0.18515161489662918 acc: 0.7029
[Epoch 43] loss: 0.17013319644391003 acc: 0.7082
[Epoch 47] loss: 0.16294128292709437 acc: 0.7061
[Epoch 51] loss: 0.15671087650265403 acc: 0.6955
[Epoch 55] loss: 0.149992763151741 acc: 0.6921
[Epoch 59] loss: 0.13826862205644055 acc: 0.6916
[Epoch 63] loss: 0.13046648898674057 acc: 0.6803
[Epoch 67] loss: 0.12886843360636546 acc: 0.7031
[Epoch 71] loss: 0.1267936855025799 acc: 0.6973
--> [test] acc: 0.6939
--> [accuracy] finished 0.6939
new state: tensor([672.,   2.,   2.,   2.,   4.], device='cuda:0')
new reward: 0.6939
--> [reward] 0.6939
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     4.0     | 0.6939 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([672.,   2.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.020868973811265 acc: 0.6472
[Epoch 7] loss: 2.7926739989339238 acc: 0.7052
[Epoch 11] loss: 1.2376323634248865 acc: 0.7082
[Epoch 15] loss: 0.5130166020577825 acc: 0.7144
[Epoch 19] loss: 0.3455541796834611 acc: 0.7055
[Epoch 23] loss: 0.27065193621189243 acc: 0.6996
[Epoch 27] loss: 0.24580489267664188 acc: 0.7057
[Epoch 31] loss: 0.21276422972312134 acc: 0.7055
[Epoch 35] loss: 0.19019475083826753 acc: 0.703
[Epoch 39] loss: 0.17838341739062039 acc: 0.7044
[Epoch 43] loss: 0.1675392072862181 acc: 0.7063
[Epoch 47] loss: 0.1529381681195534 acc: 0.7094
[Epoch 51] loss: 0.1562599419494686 acc: 0.7028
[Epoch 55] loss: 0.14131002665957068 acc: 0.7061
[Epoch 59] loss: 0.13109856293551014 acc: 0.7015
[Epoch 63] loss: 0.11942024832642625 acc: 0.7057
[Epoch 67] loss: 0.13657827578697002 acc: 0.7104
[Epoch 71] loss: 0.11286307738044674 acc: 0.6957
--> [test] acc: 0.7057
--> [accuracy] finished 0.7057
new state: tensor([672.,   2.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7057
--> [reward] 0.7057
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     1.0      |     4.0     | 0.7057 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0909, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   2.,   2.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.176406024514562 acc: 0.6289
[Epoch 7] loss: 3.016060204076035 acc: 0.6933
[Epoch 11] loss: 1.427854806318155 acc: 0.6914
[Epoch 15] loss: 0.6180090609499637 acc: 0.6948
[Epoch 19] loss: 0.37752553288970153 acc: 0.6865
[Epoch 23] loss: 0.29924487524553944 acc: 0.6878
[Epoch 27] loss: 0.2590215668520507 acc: 0.6839
[Epoch 31] loss: 0.23341439658647303 acc: 0.6868
[Epoch 35] loss: 0.2072335312385922 acc: 0.6881
[Epoch 39] loss: 0.18490701381717345 acc: 0.6935
[Epoch 43] loss: 0.18346231634659535 acc: 0.6803
[Epoch 47] loss: 0.16899993960076318 acc: 0.686
[Epoch 51] loss: 0.14796210957876862 acc: 0.6845
[Epoch 55] loss: 0.15210401737000173 acc: 0.6838
[Epoch 59] loss: 0.13366463767124048 acc: 0.6868
[Epoch 63] loss: 0.14424033880786366 acc: 0.6929
[Epoch 67] loss: 0.13565986566579022 acc: 0.6897
[Epoch 71] loss: 0.12078729420534128 acc: 0.6836
--> [test] acc: 0.6919
--> [accuracy] finished 0.6919
new state: tensor([672.,   2.,   2.,   1.,   5.], device='cuda:0')
new reward: 0.6919
--> [reward] 0.6919
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     1.0      |     5.0     | 0.6919 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   2.,   2.,   1.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.491912795149761 acc: 0.5878
[Epoch 7] loss: 3.444608189337089 acc: 0.6593
[Epoch 11] loss: 1.8532619142090268 acc: 0.64
[Epoch 15] loss: 0.8491828849401011 acc: 0.6554
[Epoch 19] loss: 0.47282526783092554 acc: 0.6557
[Epoch 23] loss: 0.3642368924511058 acc: 0.6594
[Epoch 27] loss: 0.30070900051947447 acc: 0.6615
[Epoch 31] loss: 0.26403454352227396 acc: 0.6515
[Epoch 35] loss: 0.242867277942293 acc: 0.6623
[Epoch 39] loss: 0.2101944842187645 acc: 0.6539
[Epoch 43] loss: 0.21693544397059153 acc: 0.6534
[Epoch 47] loss: 0.18205726586872964 acc: 0.6357
[Epoch 51] loss: 0.18754557628408455 acc: 0.6531
[Epoch 55] loss: 0.17374731888494377 acc: 0.659
[Epoch 59] loss: 0.16240725253263247 acc: 0.6472
[Epoch 63] loss: 0.14479132846374151 acc: 0.6492
[Epoch 67] loss: 0.15219081874312762 acc: 0.6454
[Epoch 71] loss: 0.15433588724040315 acc: 0.6538
--> [test] acc: 0.6538
--> [accuracy] finished 0.6538
new state: tensor([672.,   2.,   2.,   1.,   6.], device='cuda:0')
new reward: 0.6538
--> [reward] 0.6538
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     1.0      |     6.0     | 0.6538 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   1.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.453611531525927 acc: 0.5851
[Epoch 7] loss: 3.4558786678192255 acc: 0.65
[Epoch 11] loss: 1.91322283763105 acc: 0.6606
[Epoch 15] loss: 0.8620914431846203 acc: 0.6501
[Epoch 19] loss: 0.5110926478291336 acc: 0.6526
[Epoch 23] loss: 0.357693467463088 acc: 0.6428
[Epoch 27] loss: 0.3155843149441892 acc: 0.6477
[Epoch 31] loss: 0.28655436784124283 acc: 0.6524
[Epoch 35] loss: 0.24872990229579112 acc: 0.6531
[Epoch 39] loss: 0.21515091006165307 acc: 0.644
[Epoch 43] loss: 0.21753524910048833 acc: 0.6445
[Epoch 47] loss: 0.19566210759017627 acc: 0.6351
[Epoch 51] loss: 0.1875082340015246 acc: 0.6468
[Epoch 55] loss: 0.18191315980909198 acc: 0.6329
[Epoch 59] loss: 0.1742362544128714 acc: 0.6473
[Epoch 63] loss: 0.15007584549896325 acc: 0.6459
[Epoch 67] loss: 0.15759405713585561 acc: 0.6501
[Epoch 71] loss: 0.1515881484541137 acc: 0.6531
--> [test] acc: 0.6462
--> [accuracy] finished 0.6462
new state: tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
new reward: 0.6462
--> [reward] 0.6462
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     6.0     | 0.6462 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.424210001288167 acc: 0.5857
[Epoch 7] loss: 3.4092035215071705 acc: 0.6562
[Epoch 11] loss: 1.8446241203323959 acc: 0.657
[Epoch 15] loss: 0.8385765258403842 acc: 0.6505
[Epoch 19] loss: 0.4894155343408551 acc: 0.6625
[Epoch 23] loss: 0.36572324660847255 acc: 0.6496
[Epoch 27] loss: 0.31850170569918346 acc: 0.6422
[Epoch 31] loss: 0.27398896319053284 acc: 0.6431
[Epoch 35] loss: 0.2515244756008277 acc: 0.649
[Epoch 39] loss: 0.21996214217923182 acc: 0.6538
[Epoch 43] loss: 0.20632750999844632 acc: 0.6541
[Epoch 47] loss: 0.2081884418321235 acc: 0.6445
[Epoch 51] loss: 0.18258384704980476 acc: 0.6405
[Epoch 55] loss: 0.17399973887831088 acc: 0.6535
[Epoch 59] loss: 0.16595649548456112 acc: 0.6475
[Epoch 63] loss: 0.1624142675674842 acc: 0.6522
[Epoch 67] loss: 0.163362067757303 acc: 0.6423
[Epoch 71] loss: 0.1470443898373667 acc: 0.6344
--> [test] acc: 0.6481
--> [accuracy] finished 0.6481
new state: tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
new reward: 0.6481
--> [reward] 0.6481
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     6.0     | 0.6481 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   2.,   2.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.189528403989494 acc: 0.6254
[Epoch 7] loss: 3.067761892247993 acc: 0.6933
[Epoch 11] loss: 1.5103261925070488 acc: 0.6929
[Epoch 15] loss: 0.646317418307409 acc: 0.6819
[Epoch 19] loss: 0.40326649290950173 acc: 0.6824
[Epoch 23] loss: 0.3184215226627486 acc: 0.6851
[Epoch 27] loss: 0.2690305164598329 acc: 0.6812
[Epoch 31] loss: 0.24610550807850898 acc: 0.6865
[Epoch 35] loss: 0.21109665899663743 acc: 0.683
[Epoch 39] loss: 0.2008017150404127 acc: 0.6728
[Epoch 43] loss: 0.1865182223448725 acc: 0.6765
[Epoch 47] loss: 0.16997222024756853 acc: 0.6738
[Epoch 51] loss: 0.17588926431701501 acc: 0.6827
[Epoch 55] loss: 0.15652899746604435 acc: 0.6712
[Epoch 59] loss: 0.15274261280684673 acc: 0.6787
[Epoch 63] loss: 0.14410627662332828 acc: 0.6811
[Epoch 67] loss: 0.1363661526595159 acc: 0.6729
[Epoch 71] loss: 0.13105526664877867 acc: 0.6692
--> [test] acc: 0.689
--> [accuracy] finished 0.689
new state: tensor([672.,   2.,   2.,   2.,   5.], device='cuda:0')
new reward: 0.689
--> [reward] 0.689
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     5.0     | 0.689  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.389724639827943 acc: 0.6018
[Epoch 7] loss: 3.4145345283896114 acc: 0.6409
[Epoch 11] loss: 1.8605341306885186 acc: 0.6617
[Epoch 15] loss: 0.8329130746900578 acc: 0.6544
[Epoch 19] loss: 0.4829436113290927 acc: 0.6575
[Epoch 23] loss: 0.3669144495549943 acc: 0.6399
[Epoch 27] loss: 0.3111440758208942 acc: 0.6429
[Epoch 31] loss: 0.2627171314681125 acc: 0.639
[Epoch 35] loss: 0.2563964812266057 acc: 0.6411
[Epoch 39] loss: 0.22499370887694534 acc: 0.6383
[Epoch 43] loss: 0.20941290574724716 acc: 0.6532
[Epoch 47] loss: 0.19896265572470512 acc: 0.6393
[Epoch 51] loss: 0.18495089237642526 acc: 0.6508
[Epoch 55] loss: 0.17436595373522595 acc: 0.642
[Epoch 59] loss: 0.17264826667121114 acc: 0.6502
[Epoch 63] loss: 0.1507637447622769 acc: 0.6527
[Epoch 67] loss: 0.16251488643057663 acc: 0.6501
[Epoch 71] loss: 0.14976073252370634 acc: 0.6445
--> [test] acc: 0.6489
--> [accuracy] finished 0.6489
new state: tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
new reward: 0.6489
--> [reward] 0.6489
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     6.0     | 0.6489 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0903, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3973, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3985, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.425493588380497 acc: 0.6038
[Epoch 7] loss: 3.436223697250761 acc: 0.6543
[Epoch 11] loss: 1.8699556558638277 acc: 0.6497
[Epoch 15] loss: 0.8494776749931028 acc: 0.6379
[Epoch 19] loss: 0.4950746271730689 acc: 0.6564
[Epoch 23] loss: 0.3628611156831274 acc: 0.6395
[Epoch 27] loss: 0.31342806892417124 acc: 0.6518
[Epoch 31] loss: 0.2732084352056236 acc: 0.6451
[Epoch 35] loss: 0.24043227544726084 acc: 0.6371
[Epoch 39] loss: 0.22600081113650633 acc: 0.6494
[Epoch 43] loss: 0.21515655911663342 acc: 0.6486
[Epoch 47] loss: 0.1928249687394675 acc: 0.6406
[Epoch 51] loss: 0.18790963724674775 acc: 0.6459
[Epoch 55] loss: 0.1781824269281972 acc: 0.6398
[Epoch 59] loss: 0.17404959702392672 acc: 0.6491
[Epoch 63] loss: 0.16091541390356315 acc: 0.6481
[Epoch 67] loss: 0.1448302300935111 acc: 0.6507
[Epoch 71] loss: 0.15350739177926193 acc: 0.6385
--> [test] acc: 0.6394
--> [accuracy] finished 0.6394
new state: tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
new reward: 0.6394
--> [reward] 0.6394
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2036]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4073]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6382]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7568]], device='cuda:0')
------ ------
delta_t: tensor([[0.6382]], device='cuda:0')
rewards[i]: 0.6394
values[i+1]: tensor([[0.1186]], device='cuda:0')
values[i]: tensor([[0.1186]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6382]], device='cuda:0')
delta_t: tensor([[0.6382]], device='cuda:0')
------ ------
policy_loss: 1.5066523551940918
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6382]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0221]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6370]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2795]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3981]], device='cuda:0')
------ ------
delta_t: tensor([[0.6477]], device='cuda:0')
rewards[i]: 0.6489
values[i+1]: tensor([[0.1186]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1187]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2795]], device='cuda:0')
delta_t: tensor([[0.6477]], device='cuda:0')
------ ------
policy_loss: 4.5514140129089355
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2795]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.9321]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.8200]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9545]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0732]], device='cuda:0')
------ ------
delta_t: tensor([[0.6878]], device='cuda:0')
rewards[i]: 0.689
values[i+1]: tensor([[0.1187]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1187]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9545]], device='cuda:0')
delta_t: tensor([[0.6878]], device='cuda:0')
------ ------
policy_loss: 9.216411590576172
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9545]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.2649]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[6.6655]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.5818]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7005]], device='cuda:0')
------ ------
delta_t: tensor([[0.6468]], device='cuda:0')
rewards[i]: 0.6481
values[i+1]: tensor([[0.1187]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1188]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.5818]], device='cuda:0')
delta_t: tensor([[0.6468]], device='cuda:0')
------ ------
policy_loss: 15.38471794128418
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.5818]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[11.3877]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[10.2455]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.2009]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3197]], device='cuda:0')
------ ------
delta_t: tensor([[0.6449]], device='cuda:0')
rewards[i]: 0.6462
values[i+1]: tensor([[0.1188]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.2009]], device='cuda:0')
delta_t: tensor([[0.6449]], device='cuda:0')
------ ------
policy_loss: 23.035091400146484
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.2009]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[18.6891]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[14.6029]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.8214]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.9403]], device='cuda:0')
------ ------
delta_t: tensor([[0.6525]], device='cuda:0')
rewards[i]: 0.6538
values[i+1]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.8214]], device='cuda:0')
delta_t: tensor([[0.6525]], device='cuda:0')
------ ------
policy_loss: 32.17654800415039
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.8214]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[28.6968]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[20.0152]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.4738]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.5928]], device='cuda:0')
------ ------
delta_t: tensor([[0.6907]], device='cuda:0')
rewards[i]: 0.6919
values[i+1]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1190]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.4738]], device='cuda:0')
delta_t: tensor([[0.6907]], device='cuda:0')
------ ------
policy_loss: 42.882911682128906
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.4738]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[41.8740]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[26.3545]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.1337]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.2526]], device='cuda:0')
------ ------
delta_t: tensor([[0.7046]], device='cuda:0')
rewards[i]: 0.7057
values[i+1]: tensor([[0.1190]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.1337]], device='cuda:0')
delta_t: tensor([[0.7046]], device='cuda:0')
------ ------
policy_loss: 55.168968200683594
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.1337]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[58.5498]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[33.3516]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.7751]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.8940]], device='cuda:0')
------ ------
delta_t: tensor([[0.6928]], device='cuda:0')
rewards[i]: 0.6939
values[i+1]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.7751]], device='cuda:0')
delta_t: tensor([[0.6928]], device='cuda:0')
------ ------
policy_loss: 68.99125671386719
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.7751]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[79.0810]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[41.0624]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.4080]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.5267]], device='cuda:0')
------ ------
delta_t: tensor([[0.6907]], device='cuda:0')
rewards[i]: 0.6917
values[i+1]: tensor([[0.1189]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1187]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.4080]], device='cuda:0')
delta_t: tensor([[0.6907]], device='cuda:0')
------ ------
policy_loss: 84.33303833007812
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.4080]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[103.8738]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[49.5856]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.0417]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1602]], device='cuda:0')
------ ------
delta_t: tensor([[0.6978]], device='cuda:0')
rewards[i]: 0.6987
values[i+1]: tensor([[0.1187]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1185]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.0417]], device='cuda:0')
delta_t: tensor([[0.6978]], device='cuda:0')
------ ------
policy_loss: 101.19804382324219
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.0417]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[132.9773]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[58.2071]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.6294]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7478]], device='cuda:0')
------ ------
delta_t: tensor([[0.6581]], device='cuda:0')
rewards[i]: 0.6592
values[i+1]: tensor([[0.1185]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1184]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.6294]], device='cuda:0')
delta_t: tensor([[0.6581]], device='cuda:0')
------ ------
policy_loss: 119.4685287475586
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.6294]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[165.7653]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[65.5759]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0979]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.2162]], device='cuda:0')
------ ------
delta_t: tensor([[0.5448]], device='cuda:0')
rewards[i]: 0.5459
values[i+1]: tensor([[0.1184]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1183]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0979]], device='cuda:0')
delta_t: tensor([[0.5448]], device='cuda:0')
------ ------
policy_loss: 138.86293029785156
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0979]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[201.9820]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[72.4336]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.5108]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6284]], device='cuda:0')
------ ------
delta_t: tensor([[0.4939]], device='cuda:0')
rewards[i]: 0.4944
values[i+1]: tensor([[0.1183]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1176]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.5108]], device='cuda:0')
delta_t: tensor([[0.4939]], device='cuda:0')
------ ------
policy_loss: 159.2517852783203
log_probs[i]: tensor([[-2.3985]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.5108]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[241.5798]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[79.1955]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.8992]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.0156]], device='cuda:0')
------ ------
delta_t: tensor([[0.4735]], device='cuda:0')
rewards[i]: 0.4735
values[i+1]: tensor([[0.1176]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1165]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.8992]], device='cuda:0')
delta_t: tensor([[0.4735]], device='cuda:0')
------ ------
policy_loss: 180.56211853027344
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.8992]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 180.56211853027344
value_loss: 241.57981872558594
loss: 301.3520202636719



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.7268e-02, -3.3781e-05, -4.7852e-05, -6.1735e-05, -1.0333e-04],
        [ 3.5751e-01,  6.8951e-04,  9.9368e-04,  1.2913e-03,  2.1359e-03],
        [-2.4987e-03, -4.9101e-06, -6.9509e-06, -8.9168e-06, -1.4961e-05],
        [ 1.3397e+00,  2.5735e-03,  3.7500e-03,  4.8467e-03,  8.0290e-03],
        [ 4.8801e+00,  9.3737e-03,  1.3725e-02,  1.7649e-02,  2.9340e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.4756e-04,  1.3162e-04, -1.2831e-04, -1.3988e-04,  1.9750e-04],
        [-5.1188e-03, -2.7255e-03,  2.6504e-03,  2.8964e-03, -4.0837e-03],
        [ 3.5826e-05,  1.9046e-05, -1.8574e-05, -2.0240e-05,  2.8582e-05],
        [-1.9165e-02, -1.0214e-02,  9.9188e-03,  1.0853e-02, -1.5289e-02],
        [-6.9773e-02, -3.7203e-02,  3.6104e-02,  3.9531e-02, -5.5657e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.5648, -1.5191,  1.5620, -1.3002,  0.9740],
        [-0.5861,  0.5690, -0.5850,  0.4871, -0.3649],
        [-0.6536,  0.6345, -0.6525,  0.5430, -0.4068],
        [-0.0396,  0.0384, -0.0395,  0.0329, -0.0246],
        [-0.2855,  0.2772, -0.2850,  0.2372, -0.1777]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0611, -0.0318,  0.0321,  0.0338, -0.0487],
        [-0.0618, -0.0322,  0.0325,  0.0342, -0.0493],
        [-0.0608, -0.0317,  0.0320,  0.0337, -0.0485],
        [ 0.0017,  0.0007, -0.0010, -0.0008,  0.0015],
        [ 0.0175,  0.0099, -0.0083, -0.0106,  0.0142],
        [ 0.0114,  0.0059, -0.0054, -0.0063,  0.0091],
        [ 0.1094,  0.0565, -0.0579, -0.0601,  0.0873],
        [ 0.0190,  0.0098, -0.0106, -0.0103,  0.0150],
        [-0.0428, -0.0222,  0.0222,  0.0237, -0.0341],
        [ 0.1274,  0.0664, -0.0672, -0.0705,  0.1014],
        [-0.0600, -0.0313,  0.0315,  0.0332, -0.0479]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.6762,  1.9148, -1.9320, -2.0350,  2.9312]], device='cuda:0')
--> [loss] 301.3520202636719

---------------------------------- [[#28 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     2.0      |     6.0     | 0.6394 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   2.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.8684378508716595 acc: 0.5531
[Epoch 7] loss: 4.057861771120135 acc: 0.6029
[Epoch 11] loss: 2.7038012468601433 acc: 0.6225
[Epoch 15] loss: 1.46647984185792 acc: 0.6219
[Epoch 19] loss: 0.8105264486330549 acc: 0.6064
[Epoch 23] loss: 0.5456956871940047 acc: 0.6128
[Epoch 27] loss: 0.4354327506935962 acc: 0.6086
[Epoch 31] loss: 0.3856365157843894 acc: 0.6144
[Epoch 35] loss: 0.33902682513093857 acc: 0.612
[Epoch 39] loss: 0.311606042349087 acc: 0.6113
[Epoch 43] loss: 0.2965393719284812 acc: 0.6051
[Epoch 47] loss: 0.2767873278692784 acc: 0.6119
[Epoch 51] loss: 0.26231995168025307 acc: 0.6112
[Epoch 55] loss: 0.23971130190622014 acc: 0.6019
[Epoch 59] loss: 0.2381075988237834 acc: 0.6033
[Epoch 63] loss: 0.22217176457547852 acc: 0.6059
[Epoch 67] loss: 0.22032961677378782 acc: 0.6064
[Epoch 71] loss: 0.19974671654722384 acc: 0.6185
--> [test] acc: 0.5996
--> [accuracy] finished 0.5996
new state: tensor([672.,   1.,   2.,   2.,   6.], device='cuda:0')
new reward: 0.5996
--> [reward] 0.5996
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     2.0      |     6.0     | 0.5996 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   2.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([672.,   1.,   2.,   3.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.763777801447817 acc: 0.4546
[Epoch 7] loss: 5.383844760372815 acc: 0.5188
[Epoch 11] loss: 4.352901338158971 acc: 0.5192
[Epoch 15] loss: 3.1621622636799924 acc: 0.5197
[Epoch 19] loss: 2.0084797667191765 acc: 0.5081
[Epoch 23] loss: 1.303216857678445 acc: 0.4991
[Epoch 27] loss: 0.9500671958984317 acc: 0.4965
[Epoch 31] loss: 0.7611943260502175 acc: 0.4907
[Epoch 35] loss: 0.6549652735023852 acc: 0.4877
[Epoch 39] loss: 0.5970962902440515 acc: 0.4846
[Epoch 43] loss: 0.5436900160454042 acc: 0.4956
[Epoch 47] loss: 0.5033538872542818 acc: 0.4855
[Epoch 51] loss: 0.45561874007134484 acc: 0.4905
[Epoch 55] loss: 0.4513703662892589 acc: 0.4839
[Epoch 59] loss: 0.4176546220603349 acc: 0.488
[Epoch 63] loss: 0.38956881131586213 acc: 0.4881
[Epoch 67] loss: 0.3793741064622541 acc: 0.4949
[Epoch 71] loss: 0.35845768972612024 acc: 0.483
--> [test] acc: 0.49
--> [accuracy] finished 0.49
new state: tensor([672.,   1.,   2.,   3.,   6.], device='cuda:0')
new reward: 0.49
--> [reward] 0.49
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     3.0      |     6.0     |  0.49  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   3.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([672.,   1.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.548896725525331 acc: 0.4633
[Epoch 7] loss: 5.123000151513483 acc: 0.5192
[Epoch 11] loss: 4.06578004756547 acc: 0.5391
[Epoch 15] loss: 2.8885562168362804 acc: 0.5358
[Epoch 19] loss: 1.780732136172102 acc: 0.5244
[Epoch 23] loss: 1.1124165204670422 acc: 0.5249
[Epoch 27] loss: 0.8127386183918589 acc: 0.5112
[Epoch 31] loss: 0.6446410455571874 acc: 0.5168
[Epoch 35] loss: 0.5482742782691707 acc: 0.5072
[Epoch 39] loss: 0.481494770525857 acc: 0.5119
[Epoch 43] loss: 0.4577550569029949 acc: 0.5121
[Epoch 47] loss: 0.4234091758947162 acc: 0.5122
[Epoch 51] loss: 0.39878326855466495 acc: 0.5073
[Epoch 55] loss: 0.3652880313160741 acc: 0.5066
[Epoch 59] loss: 0.3413831654083355 acc: 0.5128
[Epoch 63] loss: 0.3405499975268474 acc: 0.5094
[Epoch 67] loss: 0.32389021678315594 acc: 0.5129
[Epoch 71] loss: 0.3106385995907819 acc: 0.5138
--> [test] acc: 0.509
--> [accuracy] finished 0.509
new state: tensor([672.,   1.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.509
--> [reward] 0.509
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     4.0      |     6.0     | 0.509  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   1.,   2.,   4.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.497592538823862 acc: 0.4813
[Epoch 7] loss: 5.06453869486099 acc: 0.5268
[Epoch 11] loss: 3.99248119982917 acc: 0.5476
[Epoch 15] loss: 2.800265469971825 acc: 0.5431
[Epoch 19] loss: 1.7235990466211764 acc: 0.5256
[Epoch 23] loss: 1.0829634703028843 acc: 0.5227
[Epoch 27] loss: 0.7753671347199346 acc: 0.5125
[Epoch 31] loss: 0.6397851200302695 acc: 0.5129
[Epoch 35] loss: 0.5402240337770613 acc: 0.5106
[Epoch 39] loss: 0.4909936959338386 acc: 0.5185
[Epoch 43] loss: 0.43881496821846955 acc: 0.5197
[Epoch 47] loss: 0.41491080560933447 acc: 0.5188
[Epoch 51] loss: 0.3954722323976552 acc: 0.5147
[Epoch 55] loss: 0.36583303061583083 acc: 0.5154
[Epoch 59] loss: 0.33408274511089714 acc: 0.5138
[Epoch 63] loss: 0.3411294362362465 acc: 0.5038
[Epoch 67] loss: 0.3003823272276508 acc: 0.5118
[Epoch 71] loss: 0.31344395985021767 acc: 0.5171
--> [test] acc: 0.5092
--> [accuracy] finished 0.5092
new state: tensor([704.,   1.,   2.,   4.,   6.], device='cuda:0')
new reward: 0.5092
--> [reward] 0.5092
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     2.0     |     4.0      |     6.0     | 0.5092 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   2.,   4.,   6.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([704.,   1.,   2.,   5.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.334568849609941 acc: 0.4984
[Epoch 7] loss: 4.9486588302170835 acc: 0.5412
[Epoch 11] loss: 3.9057567523568486 acc: 0.5423
[Epoch 15] loss: 2.7352493912210245 acc: 0.5525
[Epoch 19] loss: 1.676872166335735 acc: 0.5418
[Epoch 23] loss: 1.0401342745746494 acc: 0.5262
[Epoch 27] loss: 0.7428337117690412 acc: 0.5327
[Epoch 31] loss: 0.5964691492602648 acc: 0.5302
[Epoch 35] loss: 0.5286994497184558 acc: 0.5227
[Epoch 39] loss: 0.46351384104270005 acc: 0.5269
[Epoch 43] loss: 0.44015925344022566 acc: 0.5267
[Epoch 47] loss: 0.4046996413822979 acc: 0.5243
[Epoch 51] loss: 0.3625369029348273 acc: 0.5158
[Epoch 55] loss: 0.3606566135197535 acc: 0.5266
[Epoch 59] loss: 0.33366558247643624 acc: 0.5285
[Epoch 63] loss: 0.3286515632001183 acc: 0.5239
[Epoch 67] loss: 0.28759755152265737 acc: 0.5316
[Epoch 71] loss: 0.301625559251765 acc: 0.5255
--> [test] acc: 0.5233
--> [accuracy] finished 0.5233
new state: tensor([704.,   1.,   2.,   5.,   6.], device='cuda:0')
new reward: 0.5233
--> [reward] 0.5233
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     2.0     |     5.0      |     6.0     | 0.5233 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0900]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3988]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   2.,   5.,   6.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   1.,   2.,   5.,   6.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.31752530967488 acc: 0.4923
[Epoch 7] loss: 4.915588533481978 acc: 0.5225
[Epoch 11] loss: 3.883033258835678 acc: 0.5545
[Epoch 15] loss: 2.687655291365236 acc: 0.5512
[Epoch 19] loss: 1.6374690812414565 acc: 0.5309
[Epoch 23] loss: 0.9956360646259145 acc: 0.5336
[Epoch 27] loss: 0.7144310058801985 acc: 0.5275
[Epoch 31] loss: 0.5901474246464651 acc: 0.5314
[Epoch 35] loss: 0.5067554676376492 acc: 0.5293
[Epoch 39] loss: 0.4520182577497743 acc: 0.5245
[Epoch 43] loss: 0.4253752970701212 acc: 0.5289
[Epoch 47] loss: 0.38162010440321836 acc: 0.532
[Epoch 51] loss: 0.3707945696185426 acc: 0.5376
[Epoch 55] loss: 0.35597088623820516 acc: 0.5304
[Epoch 59] loss: 0.318533930691707 acc: 0.5306
[Epoch 63] loss: 0.30870432941876635 acc: 0.5256
[Epoch 67] loss: 0.3026048112922655 acc: 0.5189
[Epoch 71] loss: 0.29779511901652417 acc: 0.5308
--> [test] acc: 0.5301
--> [accuracy] finished 0.5301
new state: tensor([736.,   1.,   2.,   5.,   6.], device='cuda:0')
new reward: 0.5301
--> [reward] 0.5301
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     2.0     |     5.0      |     6.0     | 0.5301 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   2.,   5.,   6.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([736.,   1.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.157080986920525 acc: 0.5035
[Epoch 7] loss: 4.689862663788563 acc: 0.5654
[Epoch 11] loss: 3.5882498702734633 acc: 0.5866
[Epoch 15] loss: 2.4031044049641057 acc: 0.5815
[Epoch 19] loss: 1.378780343762749 acc: 0.5673
[Epoch 23] loss: 0.8401824282696637 acc: 0.5647
[Epoch 27] loss: 0.6168637109772704 acc: 0.5516
[Epoch 31] loss: 0.5107723972986421 acc: 0.5575
[Epoch 35] loss: 0.45085293773437857 acc: 0.5632
[Epoch 39] loss: 0.39032766333473917 acc: 0.5525
[Epoch 43] loss: 0.36991243263053925 acc: 0.56
[Epoch 47] loss: 0.35595835006707693 acc: 0.5559
[Epoch 51] loss: 0.3215940163549407 acc: 0.5458
[Epoch 55] loss: 0.31623988670756675 acc: 0.5583
[Epoch 59] loss: 0.28738665318502415 acc: 0.5494
[Epoch 63] loss: 0.2798018195259068 acc: 0.5566
[Epoch 67] loss: 0.2567388059730973 acc: 0.5445
[Epoch 71] loss: 0.26272982667031153 acc: 0.5513
--> [test] acc: 0.5495
--> [accuracy] finished 0.5495
new state: tensor([736.,   1.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.5495
--> [reward] 0.5495
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     1.0      |     2.0     |     5.0      |     5.0     | 0.5495 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   1.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([736.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.7565659555937625 acc: 0.5588
[Epoch 7] loss: 3.927085451610253 acc: 0.6383
[Epoch 11] loss: 2.478902655336863 acc: 0.6261
[Epoch 15] loss: 1.2682673974758218 acc: 0.6263
[Epoch 19] loss: 0.6614302924603147 acc: 0.6224
[Epoch 23] loss: 0.4881964601633494 acc: 0.6237
[Epoch 27] loss: 0.38402204925332534 acc: 0.6163
[Epoch 31] loss: 0.31412713961852023 acc: 0.6089
[Epoch 35] loss: 0.28218030911939374 acc: 0.613
[Epoch 39] loss: 0.2752644606506276 acc: 0.6209
[Epoch 43] loss: 0.24493343132498013 acc: 0.6023
[Epoch 47] loss: 0.23201433098410515 acc: 0.6017
[Epoch 51] loss: 0.21810353612598707 acc: 0.6158
[Epoch 55] loss: 0.19098032172531118 acc: 0.6169
[Epoch 59] loss: 0.18856948788237313 acc: 0.6187
[Epoch 63] loss: 0.19464810743280078 acc: 0.6124
[Epoch 67] loss: 0.17343514715500005 acc: 0.6127
[Epoch 71] loss: 0.16111697913105827 acc: 0.6157
--> [test] acc: 0.6083
--> [accuracy] finished 0.6083
new state: tensor([736.,   2.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.6083
--> [reward] 0.6083
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     5.0      |     5.0     | 0.6083 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.511840400793363 acc: 0.5999
[Epoch 7] loss: 3.5040918365311438 acc: 0.6366
[Epoch 11] loss: 1.8664057270797623 acc: 0.6478
[Epoch 15] loss: 0.8009355985476156 acc: 0.6549
[Epoch 19] loss: 0.46682638665923226 acc: 0.657
[Epoch 23] loss: 0.3491735191934783 acc: 0.6593
[Epoch 27] loss: 0.3007651830682784 acc: 0.6544
[Epoch 31] loss: 0.24613223016462135 acc: 0.6482
[Epoch 35] loss: 0.23736674293084903 acc: 0.6406
[Epoch 39] loss: 0.20501617341757278 acc: 0.6588
[Epoch 43] loss: 0.1898983343673484 acc: 0.6394
[Epoch 47] loss: 0.1866453632437255 acc: 0.6474
[Epoch 51] loss: 0.1709239625364847 acc: 0.6357
[Epoch 55] loss: 0.14830616803939842 acc: 0.6419
[Epoch 59] loss: 0.1588448943151042 acc: 0.6465
[Epoch 63] loss: 0.139327943129727 acc: 0.6504
[Epoch 67] loss: 0.13633655840197526 acc: 0.6401
[Epoch 71] loss: 0.13224359060687674 acc: 0.6321
--> [test] acc: 0.6402
--> [accuracy] finished 0.6402
new state: tensor([736.,   2.,   3.,   5.,   5.], device='cuda:0')
new reward: 0.6402
--> [reward] 0.6402
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     3.0     |     5.0      |     5.0     | 0.6402 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.732077732720338 acc: 0.5559
[Epoch 7] loss: 3.9457648797413274 acc: 0.625
[Epoch 11] loss: 2.5220148240208933 acc: 0.6339
[Epoch 15] loss: 1.2933374917339486 acc: 0.6238
[Epoch 19] loss: 0.6833913146620593 acc: 0.6223
[Epoch 23] loss: 0.46430205296048577 acc: 0.6203
[Epoch 27] loss: 0.37887224716746515 acc: 0.6257
[Epoch 31] loss: 0.3244956444443949 acc: 0.6278
[Epoch 35] loss: 0.2915827822907235 acc: 0.6258
[Epoch 39] loss: 0.26743276458938636 acc: 0.6195
[Epoch 43] loss: 0.24928812408472037 acc: 0.6063
[Epoch 47] loss: 0.22253663441085297 acc: 0.6201
[Epoch 51] loss: 0.224321265180912 acc: 0.6223
[Epoch 55] loss: 0.2098123847871371 acc: 0.615
[Epoch 59] loss: 0.19516191814128128 acc: 0.6251
[Epoch 63] loss: 0.18426056425599263 acc: 0.6143
[Epoch 67] loss: 0.165037262189152 acc: 0.6196
[Epoch 71] loss: 0.17509337902173897 acc: 0.6148
--> [test] acc: 0.6152
--> [accuracy] finished 0.6152
new state: tensor([736.,   2.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.6152
--> [reward] 0.6152
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     5.0      |     5.0     | 0.6152 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.747390111084179 acc: 0.5672
[Epoch 7] loss: 3.909985539705857 acc: 0.6232
[Epoch 11] loss: 2.486716732268443 acc: 0.6289
[Epoch 15] loss: 1.263095461822989 acc: 0.623
[Epoch 19] loss: 0.6910224591269899 acc: 0.6315
[Epoch 23] loss: 0.4571709467069534 acc: 0.6245
[Epoch 27] loss: 0.38165360526241304 acc: 0.6183
[Epoch 31] loss: 0.3268756723684042 acc: 0.6186
[Epoch 35] loss: 0.2890601567066539 acc: 0.6315
[Epoch 39] loss: 0.28173121281415986 acc: 0.6218
[Epoch 43] loss: 0.23858948010365333 acc: 0.6236
[Epoch 47] loss: 0.23425087615456955 acc: 0.6275
[Epoch 51] loss: 0.22215823864426149 acc: 0.6185
[Epoch 55] loss: 0.20093048968450036 acc: 0.6242
[Epoch 59] loss: 0.19211724192223242 acc: 0.628
[Epoch 63] loss: 0.18676721699812146 acc: 0.6114
[Epoch 67] loss: 0.18173008758212675 acc: 0.6215
[Epoch 71] loss: 0.17127891354348576 acc: 0.6213
--> [test] acc: 0.6244
--> [accuracy] finished 0.6244
new state: tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.6244
--> [reward] 0.6244
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     5.0     | 0.6244 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.674051796993636 acc: 0.5757
[Epoch 7] loss: 3.7767450394837754 acc: 0.6313
[Epoch 11] loss: 2.335034175449625 acc: 0.6481
[Epoch 15] loss: 1.1544029449905886 acc: 0.6413
[Epoch 19] loss: 0.6186884590960524 acc: 0.6253
[Epoch 23] loss: 0.4214352582393171 acc: 0.6285
[Epoch 27] loss: 0.36417582196176357 acc: 0.6326
[Epoch 31] loss: 0.31224597139936655 acc: 0.6399
[Epoch 35] loss: 0.2729499923670307 acc: 0.6276
[Epoch 39] loss: 0.2556797385601627 acc: 0.6389
[Epoch 43] loss: 0.23045903698438802 acc: 0.6295
[Epoch 47] loss: 0.21406496534376498 acc: 0.6386
[Epoch 51] loss: 0.21592518730598795 acc: 0.6343
[Epoch 55] loss: 0.18388865017772787 acc: 0.6294
[Epoch 59] loss: 0.19043193341416242 acc: 0.6307
[Epoch 63] loss: 0.17402878047450615 acc: 0.6292
[Epoch 67] loss: 0.17679886314827387 acc: 0.6281
[Epoch 71] loss: 0.16041705583917726 acc: 0.6256
--> [test] acc: 0.6263
--> [accuracy] finished 0.6263
new state: tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
new reward: 0.6263
--> [reward] 0.6263
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     4.0     | 0.6263 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0915, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.673571939968392 acc: 0.5693
[Epoch 7] loss: 3.789319155466221 acc: 0.6248
[Epoch 11] loss: 2.3213669041843366 acc: 0.6354
[Epoch 15] loss: 1.1374943792019658 acc: 0.6297
[Epoch 19] loss: 0.6027659663449392 acc: 0.6313
[Epoch 23] loss: 0.42716644151264904 acc: 0.6382
[Epoch 27] loss: 0.36300161254623203 acc: 0.6326
[Epoch 31] loss: 0.3026446840461448 acc: 0.6336
[Epoch 35] loss: 0.27308920254487823 acc: 0.6272
[Epoch 39] loss: 0.2595125859903405 acc: 0.6274
[Epoch 43] loss: 0.2411484077044041 acc: 0.6334
[Epoch 47] loss: 0.2189131483335591 acc: 0.6307
[Epoch 51] loss: 0.20364945579339247 acc: 0.6269
[Epoch 55] loss: 0.19966709523168785 acc: 0.6245
[Epoch 59] loss: 0.19167393799442464 acc: 0.6335
[Epoch 63] loss: 0.16839877270695652 acc: 0.6224
[Epoch 67] loss: 0.17209159264869778 acc: 0.6284
[Epoch 71] loss: 0.16770825470271317 acc: 0.6274
--> [test] acc: 0.6254
--> [accuracy] finished 0.6254
new state: tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
new reward: 0.6254
--> [reward] 0.6254
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     4.0     | 0.6254 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([704.,   2.,   1.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.0893521924762775 acc: 0.5258
[Epoch 7] loss: 4.506702428583599 acc: 0.5923
[Epoch 11] loss: 3.420313703968092 acc: 0.5896
[Epoch 15] loss: 2.2711352263112814 acc: 0.5841
[Epoch 19] loss: 1.284557234314854 acc: 0.5938
[Epoch 23] loss: 0.7751541486214799 acc: 0.5851
[Epoch 27] loss: 0.5735125616573922 acc: 0.586
[Epoch 31] loss: 0.4778217913301857 acc: 0.5834
[Epoch 35] loss: 0.42207866062974686 acc: 0.5819
[Epoch 39] loss: 0.3832360991560247 acc: 0.5688
[Epoch 43] loss: 0.35507281993985024 acc: 0.5772
[Epoch 47] loss: 0.3319987191049301 acc: 0.576
[Epoch 51] loss: 0.31317740980573855 acc: 0.5738
[Epoch 55] loss: 0.28943925751063526 acc: 0.5717
[Epoch 59] loss: 0.2893321782760227 acc: 0.5673
[Epoch 63] loss: 0.2574966280909298 acc: 0.5743
[Epoch 67] loss: 0.2652247658456721 acc: 0.5731
[Epoch 71] loss: 0.2513014469891215 acc: 0.5742
--> [test] acc: 0.5686
--> [accuracy] finished 0.5686
new state: tensor([704.,   2.,   1.,   5.,   4.], device='cuda:0')
new reward: 0.5686
--> [reward] 0.5686
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     1.0     |     5.0      |     4.0     | 0.5686 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0909, 0.0912, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3979, -2.3976,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   1.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   2.,   1.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.156893895105328 acc: 0.5023
[Epoch 7] loss: 4.570170111515942 acc: 0.5773
[Epoch 11] loss: 3.443177352094894 acc: 0.592
[Epoch 15] loss: 2.2572091996212444 acc: 0.5806
[Epoch 19] loss: 1.2958436033038228 acc: 0.5652
[Epoch 23] loss: 0.8231139282417267 acc: 0.5812
[Epoch 27] loss: 0.5805951799301768 acc: 0.5811
[Epoch 31] loss: 0.4927020327657309 acc: 0.5666
[Epoch 35] loss: 0.44597210743419274 acc: 0.5679
[Epoch 39] loss: 0.4151491364892906 acc: 0.5681
[Epoch 43] loss: 0.35837793449544925 acc: 0.5637
[Epoch 47] loss: 0.34266819681762656 acc: 0.5627
[Epoch 51] loss: 0.3395166802946526 acc: 0.5673
[Epoch 55] loss: 0.2945548573978093 acc: 0.5625
[Epoch 59] loss: 0.2834134664376983 acc: 0.5641
[Epoch 63] loss: 0.28551526945751265 acc: 0.5568
[Epoch 67] loss: 0.2671409733117084 acc: 0.5644
[Epoch 71] loss: 0.24945800543269692 acc: 0.5708
--> [test] acc: 0.5656
--> [accuracy] finished 0.5656
new state: tensor([704.,   2.,   1.,   5.,   5.], device='cuda:0')
new reward: 0.5656
--> [reward] 0.5656
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1592]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3184]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.5643]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6930]], device='cuda:0')
------ ------
delta_t: tensor([[0.5643]], device='cuda:0')
rewards[i]: 0.5656
values[i+1]: tensor([[0.1287]], device='cuda:0')
values[i]: tensor([[0.1287]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.5643]], device='cuda:0')
delta_t: tensor([[0.5643]], device='cuda:0')
------ ------
policy_loss: 1.3294200897216797
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.5643]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.7932]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.2679]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.1260]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.2547]], device='cuda:0')
------ ------
delta_t: tensor([[0.5674]], device='cuda:0')
rewards[i]: 0.5686
values[i+1]: tensor([[0.1287]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1286]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.1260]], device='cuda:0')
delta_t: tensor([[0.5674]], device='cuda:0')
------ ------
policy_loss: 4.004930019378662
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.1260]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.3047]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.0230]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.7387]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.8675]], device='cuda:0')
------ ------
delta_t: tensor([[0.6239]], device='cuda:0')
rewards[i]: 0.6254
values[i+1]: tensor([[0.1286]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1289]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.7387]], device='cuda:0')
delta_t: tensor([[0.6239]], device='cuda:0')
------ ------
policy_loss: 8.151782989501953
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.7387]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.0570]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.5047]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.3462]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.4752]], device='cuda:0')
------ ------
delta_t: tensor([[0.6249]], device='cuda:0')
rewards[i]: 0.6263
values[i+1]: tensor([[0.1289]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1289]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.3462]], device='cuda:0')
delta_t: tensor([[0.6249]], device='cuda:0')
------ ------
policy_loss: 13.756643295288086
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.3462]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[9.3949]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.6757]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9455]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.0748]], device='cuda:0')
------ ------
delta_t: tensor([[0.6227]], device='cuda:0')
rewards[i]: 0.6244
values[i+1]: tensor([[0.1289]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1293]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9455]], device='cuda:0')
delta_t: tensor([[0.6227]], device='cuda:0')
------ ------
policy_loss: 20.79374885559082
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9455]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[15.6271]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.4644]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.5305]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6593]], device='cuda:0')
------ ------
delta_t: tensor([[0.6145]], device='cuda:0')
rewards[i]: 0.6152
values[i+1]: tensor([[0.1293]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1288]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.5305]], device='cuda:0')
delta_t: tensor([[0.6145]], device='cuda:0')
------ ------
policy_loss: 29.233627319335938
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.5305]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[24.1744]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.0946]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1346]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2629]], device='cuda:0')
------ ------
delta_t: tensor([[0.6394]], device='cuda:0')
rewards[i]: 0.6402
values[i+1]: tensor([[0.1288]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1283]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1346]], device='cuda:0')
delta_t: tensor([[0.6394]], device='cuda:0')
------ ------
policy_loss: 39.12422180175781
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1346]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[35.2253]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.1019]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7013]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8285]], device='cuda:0')
------ ------
delta_t: tensor([[0.6081]], device='cuda:0')
rewards[i]: 0.6083
values[i+1]: tensor([[0.1283]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1273]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7013]], device='cuda:0')
delta_t: tensor([[0.6081]], device='cuda:0')
------ ------
policy_loss: 50.375755310058594
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7013]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[48.7626]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[27.0744]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2033]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3297]], device='cuda:0')
------ ------
delta_t: tensor([[0.5491]], device='cuda:0')
rewards[i]: 0.5495
values[i+1]: tensor([[0.1273]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1264]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2033]], device='cuda:0')
delta_t: tensor([[0.5491]], device='cuda:0')
------ ------
policy_loss: 62.835121154785156
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2033]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[64.9012]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.2773]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.6813]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.8066]], device='cuda:0')
------ ------
delta_t: tensor([[0.5300]], device='cuda:0')
rewards[i]: 0.5301
values[i+1]: tensor([[0.1264]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1252]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.6813]], device='cuda:0')
delta_t: tensor([[0.5300]], device='cuda:0')
------ ------
policy_loss: 76.42543029785156
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.6813]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[83.7942]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.7860]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1470]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2718]], device='cuda:0')
------ ------
delta_t: tensor([[0.5225]], device='cuda:0')
rewards[i]: 0.5233
values[i+1]: tensor([[0.1252]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1248]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1470]], device='cuda:0')
delta_t: tensor([[0.5225]], device='cuda:0')
------ ------
policy_loss: 91.13936614990234
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1470]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[105.5374]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[43.4863]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.5944]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7183]], device='cuda:0')
------ ------
delta_t: tensor([[0.5089]], device='cuda:0')
rewards[i]: 0.5092
values[i+1]: tensor([[0.1248]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1239]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.5944]], device='cuda:0')
delta_t: tensor([[0.5089]], device='cuda:0')
------ ------
policy_loss: 106.9178466796875
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.5944]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[130.2943]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[49.5137]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.0366]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1601]], device='cuda:0')
------ ------
delta_t: tensor([[0.5081]], device='cuda:0')
rewards[i]: 0.509
values[i+1]: tensor([[0.1239]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1235]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.0366]], device='cuda:0')
delta_t: tensor([[0.5081]], device='cuda:0')
------ ------
policy_loss: 123.76457977294922
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.0366]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[158.0871]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[55.5856]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4556]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.5785]], device='cuda:0')
------ ------
delta_t: tensor([[0.4893]], device='cuda:0')
rewards[i]: 0.49
values[i+1]: tensor([[0.1235]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1229]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4556]], device='cuda:0')
delta_t: tensor([[0.4893]], device='cuda:0')
------ ------
policy_loss: 141.61582946777344
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4556]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[189.9292]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[63.6843]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9802]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1023]], device='cuda:0')
------ ------
delta_t: tensor([[0.5992]], device='cuda:0')
rewards[i]: 0.5996
values[i+1]: tensor([[0.1229]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1221]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9802]], device='cuda:0')
delta_t: tensor([[0.5992]], device='cuda:0')
------ ------
policy_loss: 160.72618103027344
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9802]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 160.72618103027344
value_loss: 189.92921447753906
loss: 255.6907958984375



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.5820e-02, -3.4883e-05, -4.6606e-05, -6.9249e-05, -1.3237e-04],
        [ 3.3665e-01,  7.3434e-04,  9.9016e-04,  1.5388e-03,  2.7895e-03],
        [-2.1455e-03, -4.6783e-06, -6.3192e-06, -9.7466e-06, -1.7857e-05],
        [ 1.3209e+00,  2.8566e-03,  3.8824e-03,  6.2097e-03,  1.0885e-02],
        [ 4.8926e+00,  1.0584e-02,  1.4379e-02,  2.3193e-02,  4.0237e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.3018e-04,  1.1804e-04, -1.2298e-04, -1.2495e-04,  1.8210e-04],
        [-4.8943e-03, -2.5078e-03,  2.6099e-03,  2.6564e-03, -3.8671e-03],
        [ 3.1215e-05,  1.5994e-05, -1.6656e-05, -1.6939e-05,  2.4671e-05],
        [-1.9192e-02, -9.8286e-03,  1.0222e-02,  1.0415e-02, -1.5152e-02],
        [-7.1068e-02, -3.6392e-02,  3.7835e-02,  3.8571e-02, -5.6093e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.5559, -1.5133,  1.5533, -1.2975,  0.9791],
        [-0.5825,  0.5666, -0.5816,  0.4858, -0.3666],
        [-0.6478,  0.6301, -0.6467,  0.5403, -0.4078],
        [-0.0375,  0.0364, -0.0374,  0.0312, -0.0235],
        [-0.2881,  0.2802, -0.2876,  0.2402, -0.1812]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0284, -0.0144,  0.0155,  0.0151, -0.0229],
        [ 0.0564,  0.0283, -0.0309, -0.0298,  0.0452],
        [ 0.0145,  0.0077, -0.0083, -0.0077,  0.0120],
        [-0.0117, -0.0056,  0.0070,  0.0058, -0.0096],
        [-0.0118, -0.0060,  0.0069,  0.0061, -0.0100],
        [-0.0168, -0.0083,  0.0096,  0.0086, -0.0138],
        [-0.0573, -0.0290,  0.0302,  0.0309, -0.0449],
        [ 0.1313,  0.0662, -0.0711, -0.0698,  0.1047],
        [ 0.0152,  0.0078, -0.0073, -0.0085,  0.0112],
        [-0.0515, -0.0261,  0.0272,  0.0278, -0.0404],
        [-0.0399, -0.0204,  0.0211,  0.0216, -0.0315]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.4550,  1.7505, -1.8226, -1.8635,  2.7065]], device='cuda:0')
--> [loss] 255.6907958984375

---------------------------------- [[#29 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     1.0     |     5.0      |     5.0     | 0.5656 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   1.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([736.,   2.,   1.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.165315400913853 acc: 0.4918
[Epoch 7] loss: 4.614851971873847 acc: 0.5812
[Epoch 11] loss: 3.464010948262861 acc: 0.5883
[Epoch 15] loss: 2.2462512567982342 acc: 0.5808
[Epoch 19] loss: 1.2492744233602149 acc: 0.5736
[Epoch 23] loss: 0.766987453336301 acc: 0.562
[Epoch 27] loss: 0.559509399139782 acc: 0.5669
[Epoch 31] loss: 0.49110903673331296 acc: 0.5679
[Epoch 35] loss: 0.4257090056215025 acc: 0.559
[Epoch 39] loss: 0.37642037185371074 acc: 0.5631
[Epoch 43] loss: 0.3645089207183751 acc: 0.5711
[Epoch 47] loss: 0.33118832824022876 acc: 0.5601
[Epoch 51] loss: 0.31944171298304785 acc: 0.566
[Epoch 55] loss: 0.2899528946251134 acc: 0.5632
[Epoch 59] loss: 0.28556730471256064 acc: 0.5521
[Epoch 63] loss: 0.2703765323409415 acc: 0.5627
[Epoch 67] loss: 0.2604026938550403 acc: 0.5615
[Epoch 71] loss: 0.2437090228557053 acc: 0.5647
--> [test] acc: 0.5557
--> [accuracy] finished 0.5557
new state: tensor([736.,   2.,   1.,   5.,   5.], device='cuda:0')
new reward: 0.5557
--> [reward] 0.5557
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     1.0     |     5.0      |     5.0     | 0.5557 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   1.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   2.,   1.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.143537767098078 acc: 0.5179
[Epoch 7] loss: 4.600893645945107 acc: 0.5828
[Epoch 11] loss: 3.4867100227824257 acc: 0.5876
[Epoch 15] loss: 2.301708968704009 acc: 0.5848
[Epoch 19] loss: 1.3149228795905552 acc: 0.5762
[Epoch 23] loss: 0.7967626872041341 acc: 0.5739
[Epoch 27] loss: 0.5944383756240921 acc: 0.5735
[Epoch 31] loss: 0.4856132015233378 acc: 0.5718
[Epoch 35] loss: 0.44653262963513735 acc: 0.5721
[Epoch 39] loss: 0.3888412037306963 acc: 0.5722
[Epoch 43] loss: 0.36914278652347493 acc: 0.5652
[Epoch 47] loss: 0.3241798383686358 acc: 0.5658
[Epoch 51] loss: 0.33381766587248085 acc: 0.5643
[Epoch 55] loss: 0.2935340284081676 acc: 0.5628
[Epoch 59] loss: 0.2929755866603778 acc: 0.5619
[Epoch 63] loss: 0.2724263444137962 acc: 0.5684
[Epoch 67] loss: 0.2575621007801131 acc: 0.5602
[Epoch 71] loss: 0.24573663448500435 acc: 0.5661
--> [test] acc: 0.5622
--> [accuracy] finished 0.5622
new state: tensor([736.,   2.,   1.,   5.,   5.], device='cuda:0')
new reward: 0.5622
--> [reward] 0.5622
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     1.0     |     5.0      |     5.0     | 0.5622 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   1.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([736.,   2.,   1.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.320787618227322 acc: 0.5057
[Epoch 7] loss: 4.824132249178484 acc: 0.5669
[Epoch 11] loss: 3.7539721305108134 acc: 0.5673
[Epoch 15] loss: 2.5521278482721286 acc: 0.5579
[Epoch 19] loss: 1.4861634312688237 acc: 0.552
[Epoch 23] loss: 0.9178234773599888 acc: 0.5529
[Epoch 27] loss: 0.6569389976808787 acc: 0.5452
[Epoch 31] loss: 0.5554202961428162 acc: 0.5447
[Epoch 35] loss: 0.4853947264239992 acc: 0.5327
[Epoch 39] loss: 0.4460990226367855 acc: 0.5406
[Epoch 43] loss: 0.3805795822130597 acc: 0.5485
[Epoch 47] loss: 0.38280157244447477 acc: 0.54
[Epoch 51] loss: 0.3560173154315528 acc: 0.541
[Epoch 55] loss: 0.3247392806641357 acc: 0.5424
[Epoch 59] loss: 0.3172502916501573 acc: 0.5392
[Epoch 63] loss: 0.2976901218428484 acc: 0.5452
[Epoch 67] loss: 0.29324688470881916 acc: 0.5383
[Epoch 71] loss: 0.2850997964267993 acc: 0.5463
--> [test] acc: 0.5322
--> [accuracy] finished 0.5322
new state: tensor([736.,   2.,   1.,   6.,   5.], device='cuda:0')
new reward: 0.5322
--> [reward] 0.5322
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     1.0     |     6.0      |     5.0     | 0.5322 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   1.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.956431027873398 acc: 0.5412
[Epoch 7] loss: 4.201962265517095 acc: 0.5991
[Epoch 11] loss: 2.81348500090182 acc: 0.6069
[Epoch 15] loss: 1.51650423188801 acc: 0.6077
[Epoch 19] loss: 0.8159220872823235 acc: 0.6095
[Epoch 23] loss: 0.5626616349343754 acc: 0.6004
[Epoch 27] loss: 0.4352425556829976 acc: 0.5944
[Epoch 31] loss: 0.39034834010597996 acc: 0.5999
[Epoch 35] loss: 0.33038158471579365 acc: 0.601
[Epoch 39] loss: 0.302273836144058 acc: 0.6036
[Epoch 43] loss: 0.2826083216344571 acc: 0.5939
[Epoch 47] loss: 0.25622549047098137 acc: 0.5977
[Epoch 51] loss: 0.24659243330378514 acc: 0.5811
[Epoch 55] loss: 0.23488603497300384 acc: 0.5974
[Epoch 59] loss: 0.22493538992893894 acc: 0.5924
[Epoch 63] loss: 0.2098731079693798 acc: 0.59
[Epoch 67] loss: 0.20434008973062306 acc: 0.5943
[Epoch 71] loss: 0.1867691642781029 acc: 0.5964
--> [test] acc: 0.5949
--> [accuracy] finished 0.5949
new state: tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
new reward: 0.5949
--> [reward] 0.5949
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     6.0      |     5.0     | 0.5949 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.9367599084859 acc: 0.5465
[Epoch 7] loss: 4.182967967846814 acc: 0.6037
[Epoch 11] loss: 2.789371621883129 acc: 0.6099
[Epoch 15] loss: 1.4998147212483388 acc: 0.5987
[Epoch 19] loss: 0.7885618156103222 acc: 0.6055
[Epoch 23] loss: 0.541569985096793 acc: 0.5936
[Epoch 27] loss: 0.437405743043098 acc: 0.5988
[Epoch 31] loss: 0.3685661940370946 acc: 0.6082
[Epoch 35] loss: 0.32913999017470935 acc: 0.5977
[Epoch 39] loss: 0.3069111669812437 acc: 0.5903
[Epoch 43] loss: 0.2842273435524434 acc: 0.5932
[Epoch 47] loss: 0.25859241651328246 acc: 0.5948
[Epoch 51] loss: 0.24013294520623543 acc: 0.5974
[Epoch 55] loss: 0.23391467826131285 acc: 0.5869
[Epoch 59] loss: 0.21514807542181 acc: 0.5874
[Epoch 63] loss: 0.20572082849119402 acc: 0.5951
[Epoch 67] loss: 0.19970968345184917 acc: 0.59
[Epoch 71] loss: 0.19637445353991007 acc: 0.599
--> [test] acc: 0.5837
--> [accuracy] finished 0.5837
new state: tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
new reward: 0.5837
--> [reward] 0.5837
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     6.0      |     5.0     | 0.5837 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([736.,   2.,   1.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.346441223798201 acc: 0.5033
[Epoch 7] loss: 4.838741268495769 acc: 0.5545
[Epoch 11] loss: 3.7865533595499783 acc: 0.5592
[Epoch 15] loss: 2.5898164888781965 acc: 0.559
[Epoch 19] loss: 1.5291815312850812 acc: 0.5513
[Epoch 23] loss: 0.9343876186424814 acc: 0.5506
[Epoch 27] loss: 0.6835235835665174 acc: 0.5465
[Epoch 31] loss: 0.5579804415526368 acc: 0.5504
[Epoch 35] loss: 0.4929931457952389 acc: 0.5418
[Epoch 39] loss: 0.43618652384127954 acc: 0.5452
[Epoch 43] loss: 0.4094808308784004 acc: 0.5404
[Epoch 47] loss: 0.3735182213042017 acc: 0.5458
[Epoch 51] loss: 0.3590187695677704 acc: 0.5332
[Epoch 55] loss: 0.3451812808208949 acc: 0.5457
[Epoch 59] loss: 0.31581464557505934 acc: 0.5445
[Epoch 63] loss: 0.29951418447963263 acc: 0.536
[Epoch 67] loss: 0.28814319863944027 acc: 0.5372
[Epoch 71] loss: 0.28648287462561256 acc: 0.5382
--> [test] acc: 0.5359
--> [accuracy] finished 0.5359
new state: tensor([736.,   2.,   1.,   6.,   5.], device='cuda:0')
new reward: 0.5359
--> [reward] 0.5359
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     1.0     |     6.0      |     5.0     | 0.5359 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   1.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.9210641518273315 acc: 0.5544
[Epoch 7] loss: 4.165834115289361 acc: 0.5949
[Epoch 11] loss: 2.7873251197283224 acc: 0.6173
[Epoch 15] loss: 1.4917947226739905 acc: 0.6082
[Epoch 19] loss: 0.8015238136300803 acc: 0.6033
[Epoch 23] loss: 0.5469827550794463 acc: 0.5952
[Epoch 27] loss: 0.4331233972192878 acc: 0.5994
[Epoch 31] loss: 0.3699695650974046 acc: 0.5957
[Epoch 35] loss: 0.3396349341663368 acc: 0.5969
[Epoch 39] loss: 0.3049618793803903 acc: 0.6
[Epoch 43] loss: 0.28113964062822444 acc: 0.6003
[Epoch 47] loss: 0.24089784207551376 acc: 0.5972
[Epoch 51] loss: 0.24467355728892567 acc: 0.5934
[Epoch 55] loss: 0.23867161929026207 acc: 0.5909
[Epoch 59] loss: 0.20656634092121326 acc: 0.594
[Epoch 63] loss: 0.2117840390988266 acc: 0.5931
[Epoch 67] loss: 0.2024077032680821 acc: 0.5981
[Epoch 71] loss: 0.1815077344805974 acc: 0.5948
--> [test] acc: 0.6001
--> [accuracy] finished 0.6001
new state: tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
new reward: 0.6001
--> [reward] 0.6001
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     6.0      |     5.0     | 0.6001 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.951488633594854 acc: 0.5475
[Epoch 7] loss: 4.208859346711727 acc: 0.6119
[Epoch 11] loss: 2.818073993829815 acc: 0.6168
[Epoch 15] loss: 1.536001114493898 acc: 0.5978
[Epoch 19] loss: 0.8026351092283226 acc: 0.6022
[Epoch 23] loss: 0.5568009892864453 acc: 0.5899
[Epoch 27] loss: 0.4460869943032332 acc: 0.5964
[Epoch 31] loss: 0.36595237132428626 acc: 0.5953
[Epoch 35] loss: 0.34344753823326446 acc: 0.583
[Epoch 39] loss: 0.2967234330080316 acc: 0.5974
[Epoch 43] loss: 0.2868197333267735 acc: 0.593
[Epoch 47] loss: 0.2682017389461017 acc: 0.6001
[Epoch 51] loss: 0.24776007732509842 acc: 0.5892
[Epoch 55] loss: 0.2307138629019489 acc: 0.5913
[Epoch 59] loss: 0.22286283437762877 acc: 0.5986
[Epoch 63] loss: 0.2096192758492268 acc: 0.5997
[Epoch 67] loss: 0.1930538710704564 acc: 0.5958
[Epoch 71] loss: 0.1995425808679341 acc: 0.5945
--> [test] acc: 0.5932
--> [accuracy] finished 0.5932
new state: tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
new reward: 0.5932
--> [reward] 0.5932
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  736.0   |     2.0      |     2.0     |     6.0      |     5.0     | 0.5932 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([736.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([704.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.9186353713959985 acc: 0.5239
[Epoch 7] loss: 4.180849965423574 acc: 0.6117
[Epoch 11] loss: 2.776463294425584 acc: 0.6228
[Epoch 15] loss: 1.494877621569597 acc: 0.6029
[Epoch 19] loss: 0.8090499924957905 acc: 0.6051
[Epoch 23] loss: 0.5528983350895593 acc: 0.5888
[Epoch 27] loss: 0.4375952458976175 acc: 0.5943
[Epoch 31] loss: 0.3802309867632968 acc: 0.5986
[Epoch 35] loss: 0.33151938480174986 acc: 0.5908
[Epoch 39] loss: 0.29936755595781156 acc: 0.601
[Epoch 43] loss: 0.2881573622669939 acc: 0.5924
[Epoch 47] loss: 0.25599543550444764 acc: 0.6004
[Epoch 51] loss: 0.24162306955269994 acc: 0.5966
[Epoch 55] loss: 0.23997607860771364 acc: 0.594
[Epoch 59] loss: 0.21280718871327997 acc: 0.5974
[Epoch 63] loss: 0.2216372083065093 acc: 0.5954
[Epoch 67] loss: 0.2036311615406133 acc: 0.5978
[Epoch 71] loss: 0.18952120434792946 acc: 0.5953
--> [test] acc: 0.5936
--> [accuracy] finished 0.5936
new state: tensor([704.,   2.,   2.,   6.,   5.], device='cuda:0')
new reward: 0.5936
--> [reward] 0.5936
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     6.0      |     5.0     | 0.5936 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   6.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([704.,   2.,   2.,   6.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.889903152995097 acc: 0.533
[Epoch 7] loss: 4.097371931271175 acc: 0.6134
[Epoch 11] loss: 2.707805807328285 acc: 0.6089
[Epoch 15] loss: 1.432000224166514 acc: 0.603
[Epoch 19] loss: 0.753017524262066 acc: 0.602
[Epoch 23] loss: 0.5287609478158642 acc: 0.5988
[Epoch 27] loss: 0.4242081553020212 acc: 0.6057
[Epoch 31] loss: 0.36225056677075373 acc: 0.5991
[Epoch 35] loss: 0.3102221594399313 acc: 0.6016
[Epoch 39] loss: 0.29167837062922053 acc: 0.5974
[Epoch 43] loss: 0.26780327126536224 acc: 0.5978
[Epoch 47] loss: 0.2474876756558333 acc: 0.598
[Epoch 51] loss: 0.2501910421878214 acc: 0.5942
[Epoch 55] loss: 0.21911607681990355 acc: 0.6053
[Epoch 59] loss: 0.19962392914611513 acc: 0.5911
[Epoch 63] loss: 0.20075950379985982 acc: 0.6017
[Epoch 67] loss: 0.1917441246240302 acc: 0.5932
[Epoch 71] loss: 0.1851998379780337 acc: 0.6022
--> [test] acc: 0.6014
--> [accuracy] finished 0.6014
new state: tensor([704.,   2.,   2.,   6.,   4.], device='cuda:0')
new reward: 0.6014
--> [reward] 0.6014
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     6.0      |     4.0     | 0.6014 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   6.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([704.,   2.,   2.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.878477683762455 acc: 0.5252
[Epoch 7] loss: 4.108724284202546 acc: 0.6236
[Epoch 11] loss: 2.7359531677287556 acc: 0.6202
[Epoch 15] loss: 1.4215494823996977 acc: 0.5993
[Epoch 19] loss: 0.769549239150551 acc: 0.5991
[Epoch 23] loss: 0.5189622258076735 acc: 0.5924
[Epoch 27] loss: 0.42070642567199207 acc: 0.6097
[Epoch 31] loss: 0.3569827406378963 acc: 0.6037
[Epoch 35] loss: 0.32635500745804946 acc: 0.606
[Epoch 39] loss: 0.2906929635850098 acc: 0.6062
[Epoch 43] loss: 0.26745023466575213 acc: 0.6042
[Epoch 47] loss: 0.2510704239068167 acc: 0.5936
[Epoch 51] loss: 0.22899659654444746 acc: 0.6017
[Epoch 55] loss: 0.23194998350051588 acc: 0.6031
[Epoch 59] loss: 0.197420316915054 acc: 0.6074
[Epoch 63] loss: 0.20827219110634893 acc: 0.6006
[Epoch 67] loss: 0.19399481068562974 acc: 0.6
[Epoch 71] loss: 0.1794158519934053 acc: 0.6006
--> [test] acc: 0.5898
--> [accuracy] finished 0.5898
new state: tensor([704.,   2.,   2.,   6.,   3.], device='cuda:0')
new reward: 0.5898
--> [reward] 0.5898
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     6.0      |     3.0     | 0.5898 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] new state is not available; not change
--> [step] to state tensor([704.,   2.,   2.,   6.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.891565389340491 acc: 0.5334
[Epoch 7] loss: 4.122418968268978 acc: 0.6043
[Epoch 11] loss: 2.7245277734973548 acc: 0.6182
[Epoch 15] loss: 1.4324790808322179 acc: 0.609
[Epoch 19] loss: 0.7718591201678872 acc: 0.6063
[Epoch 23] loss: 0.5340674994727763 acc: 0.6109
[Epoch 27] loss: 0.4187016925533943 acc: 0.5964
[Epoch 31] loss: 0.37422937110466575 acc: 0.5869
[Epoch 35] loss: 0.3188439260863358 acc: 0.606
[Epoch 39] loss: 0.29612268950990245 acc: 0.5978
[Epoch 43] loss: 0.2651144856029688 acc: 0.596
[Epoch 47] loss: 0.26032817397323793 acc: 0.6004
[Epoch 51] loss: 0.22842125007239603 acc: 0.6073
[Epoch 55] loss: 0.22333419654527895 acc: 0.5972
[Epoch 59] loss: 0.21285575290289147 acc: 0.6058
[Epoch 63] loss: 0.20405620793619042 acc: 0.6018
[Epoch 67] loss: 0.18378525385942757 acc: 0.6002
[Epoch 71] loss: 0.19158242547603518 acc: 0.6018
--> [test] acc: 0.6003
--> [accuracy] finished 0.6003
new state: tensor([704.,   2.,   2.,   6.,   3.], device='cuda:0')
new reward: 0.6003
--> [reward] 0.6003
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     6.0      |     3.0     | 0.6003 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   6.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   2.,   2.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.685157960333178 acc: 0.581
[Epoch 7] loss: 3.82380994582725 acc: 0.6318
[Epoch 11] loss: 2.335702520807076 acc: 0.646
[Epoch 15] loss: 1.149948348817618 acc: 0.6429
[Epoch 19] loss: 0.6226222807127039 acc: 0.6248
[Epoch 23] loss: 0.4296966964269386 acc: 0.6359
[Epoch 27] loss: 0.3581029597450705 acc: 0.6271
[Epoch 31] loss: 0.31364791294383576 acc: 0.628
[Epoch 35] loss: 0.2749864796719626 acc: 0.6281
[Epoch 39] loss: 0.2702424625134872 acc: 0.6367
[Epoch 43] loss: 0.23699996045426183 acc: 0.6241
[Epoch 47] loss: 0.2208114921465478 acc: 0.6288
[Epoch 51] loss: 0.20933684692396531 acc: 0.6377
[Epoch 55] loss: 0.19779910348222385 acc: 0.6302
[Epoch 59] loss: 0.18855161781725296 acc: 0.6258
[Epoch 63] loss: 0.18309827364118927 acc: 0.6301
[Epoch 67] loss: 0.1612465065568114 acc: 0.6256
[Epoch 71] loss: 0.1631838654923012 acc: 0.6224
--> [test] acc: 0.6282
--> [accuracy] finished 0.6282
new state: tensor([704.,   2.,   2.,   5.,   3.], device='cuda:0')
new reward: 0.6282
--> [reward] 0.6282
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     3.0     | 0.6282 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0904, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   2.,   2.,   5.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.725521021334411 acc: 0.5621
[Epoch 7] loss: 3.820125115497033 acc: 0.6408
[Epoch 11] loss: 2.384529115332057 acc: 0.6434
[Epoch 15] loss: 1.1963507784220873 acc: 0.6318
[Epoch 19] loss: 0.6402197522599526 acc: 0.6324
[Epoch 23] loss: 0.461612649957466 acc: 0.6338
[Epoch 27] loss: 0.3585407479363672 acc: 0.633
[Epoch 31] loss: 0.30516311226655607 acc: 0.6376
[Epoch 35] loss: 0.29163569996557426 acc: 0.63
[Epoch 39] loss: 0.26160245579298197 acc: 0.6315
[Epoch 43] loss: 0.2355748386267582 acc: 0.6272
[Epoch 47] loss: 0.2249741290917482 acc: 0.6294
[Epoch 51] loss: 0.21365960050955454 acc: 0.6394
[Epoch 55] loss: 0.19021311827728052 acc: 0.6322
[Epoch 59] loss: 0.19531237840881127 acc: 0.6335
[Epoch 63] loss: 0.16901372849722118 acc: 0.6352
[Epoch 67] loss: 0.17418626838308923 acc: 0.6175
[Epoch 71] loss: 0.16907493557061648 acc: 0.6341
--> [test] acc: 0.6306
--> [accuracy] finished 0.6306
new state: tensor([672.,   2.,   2.,   5.,   3.], device='cuda:0')
new reward: 0.6306
--> [reward] 0.6306
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     5.0      |     3.0     | 0.6306 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0899]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3984, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3989]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   5.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.659441596711688 acc: 0.5811
[Epoch 7] loss: 3.786740573928179 acc: 0.6411
[Epoch 11] loss: 2.3509454843790634 acc: 0.6275
[Epoch 15] loss: 1.17077279845467 acc: 0.6445
[Epoch 19] loss: 0.6354702855162608 acc: 0.6257
[Epoch 23] loss: 0.4366252835592269 acc: 0.6337
[Epoch 27] loss: 0.35524444587652565 acc: 0.6318
[Epoch 31] loss: 0.3286483365842296 acc: 0.6284
[Epoch 35] loss: 0.27040231840499224 acc: 0.6342
[Epoch 39] loss: 0.2584661763718785 acc: 0.6277
[Epoch 43] loss: 0.23943771160376803 acc: 0.6216
[Epoch 47] loss: 0.22277107798372922 acc: 0.6323
[Epoch 51] loss: 0.20869356091193797 acc: 0.6237
[Epoch 55] loss: 0.20612946210686317 acc: 0.625
[Epoch 59] loss: 0.18941201434215849 acc: 0.6183
[Epoch 63] loss: 0.175738188969281 acc: 0.6187
[Epoch 67] loss: 0.17017823266868223 acc: 0.6245
[Epoch 71] loss: 0.15940033563215028 acc: 0.6161
--> [test] acc: 0.6317
--> [accuracy] finished 0.6317
new state: tensor([672.,   2.,   2.,   5.,   4.], device='cuda:0')
new reward: 0.6317
--> [reward] 0.6317
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1983]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.3967]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6298]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7643]], device='cuda:0')
------ ------
delta_t: tensor([[0.6298]], device='cuda:0')
rewards[i]: 0.6317
values[i+1]: tensor([[0.1339]], device='cuda:0')
values[i]: tensor([[0.1345]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6298]], device='cuda:0')
delta_t: tensor([[0.6298]], device='cuda:0')
------ ------
policy_loss: 1.4865535497665405
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6298]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.9819]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.5671]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2518]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3873]], device='cuda:0')
------ ------
delta_t: tensor([[0.6283]], device='cuda:0')
rewards[i]: 0.6306
values[i+1]: tensor([[0.1345]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1354]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2518]], device='cuda:0')
delta_t: tensor([[0.6283]], device='cuda:0')
------ ------
policy_loss: 4.4635844230651855
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2518]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.7228]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.4819]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8660]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0016]], device='cuda:0')
------ ------
delta_t: tensor([[0.6267]], device='cuda:0')
rewards[i]: 0.6282
values[i+1]: tensor([[0.1354]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1356]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8660]], device='cuda:0')
delta_t: tensor([[0.6267]], device='cuda:0')
------ ------
policy_loss: 8.913908004760742
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8660]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.7142]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.9827]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.4460]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5819]], device='cuda:0')
------ ------
delta_t: tensor([[0.5986]], device='cuda:0')
rewards[i]: 0.6003
values[i+1]: tensor([[0.1356]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1359]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.4460]], device='cuda:0')
delta_t: tensor([[0.5986]], device='cuda:0')
------ ------
policy_loss: 14.756217956542969
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.4460]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[10.2430]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.0575]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.0096]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1459]], device='cuda:0')
------ ------
delta_t: tensor([[0.5881]], device='cuda:0')
rewards[i]: 0.5898
values[i+1]: tensor([[0.1359]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1363]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.0096]], device='cuda:0')
delta_t: tensor([[0.5881]], device='cuda:0')
------ ------
policy_loss: 21.952558517456055
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.0096]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[16.6480]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.8100]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.5791]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.7158]], device='cuda:0')
------ ------
delta_t: tensor([[0.5996]], device='cuda:0')
rewards[i]: 0.6014
values[i+1]: tensor([[0.1363]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1367]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.5791]], device='cuda:0')
delta_t: tensor([[0.5996]], device='cuda:0')
------ ------
policy_loss: 30.515296936035156
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.5791]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[25.1958]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[17.0956]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1347]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2722]], device='cuda:0')
------ ------
delta_t: tensor([[0.5914]], device='cuda:0')
rewards[i]: 0.5936
values[i+1]: tensor([[0.1367]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1376]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1347]], device='cuda:0')
delta_t: tensor([[0.5914]], device='cuda:0')
------ ------
policy_loss: 40.403263092041016
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1347]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[36.1712]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[21.9507]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.6852]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8227]], device='cuda:0')
------ ------
delta_t: tensor([[0.5918]], device='cuda:0')
rewards[i]: 0.5932
values[i+1]: tensor([[0.1376]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1376]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.6852]], device='cuda:0')
delta_t: tensor([[0.5918]], device='cuda:0')
------ ------
policy_loss: 51.61600112915039
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.6852]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[49.8843]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[27.4264]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2370]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3746]], device='cuda:0')
------ ------
delta_t: tensor([[0.5987]], device='cuda:0')
rewards[i]: 0.6001
values[i+1]: tensor([[0.1376]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1376]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2370]], device='cuda:0')
delta_t: tensor([[0.5987]], device='cuda:0')
------ ------
policy_loss: 64.1504135131836
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2370]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[66.2413]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.7139]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.7196]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.8567]], device='cuda:0')
------ ------
delta_t: tensor([[0.5350]], device='cuda:0')
rewards[i]: 0.5359
values[i+1]: tensor([[0.1376]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1371]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.7196]], device='cuda:0')
delta_t: tensor([[0.5350]], device='cuda:0')
------ ------
policy_loss: 77.83849334716797
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.7196]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[85.7413]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[39.0000]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.2450]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.3819]], device='cuda:0')
------ ------
delta_t: tensor([[0.5826]], device='cuda:0')
rewards[i]: 0.5837
values[i+1]: tensor([[0.1371]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1369]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.2450]], device='cuda:0')
delta_t: tensor([[0.5826]], device='cuda:0')
------ ------
policy_loss: 92.79589080810547
log_probs[i]: tensor([[-2.3989]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.2450]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[108.7017]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.9207]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7765]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.9130]], device='cuda:0')
------ ------
delta_t: tensor([[0.5939]], device='cuda:0')
rewards[i]: 0.5949
values[i+1]: tensor([[0.1369]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1365]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7765]], device='cuda:0')
delta_t: tensor([[0.5939]], device='cuda:0')
------ ------
policy_loss: 109.02194213867188
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7765]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[134.9140]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[52.4247]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2405]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.3760]], device='cuda:0')
------ ------
delta_t: tensor([[0.5318]], device='cuda:0')
rewards[i]: 0.5322
values[i+1]: tensor([[0.1365]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1355]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2405]], device='cuda:0')
delta_t: tensor([[0.5318]], device='cuda:0')
------ ------
policy_loss: 126.35714721679688
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2405]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[164.7917]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[59.7554]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.7302]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.8645]], device='cuda:0')
------ ------
delta_t: tensor([[0.5621]], device='cuda:0')
rewards[i]: 0.5622
values[i+1]: tensor([[0.1355]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1343]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.7302]], device='cuda:0')
delta_t: tensor([[0.5621]], device='cuda:0')
------ ------
policy_loss: 144.86526489257812
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.7302]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[198.4875]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[67.3915]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.2092]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.3415]], device='cuda:0')
------ ------
delta_t: tensor([[0.5564]], device='cuda:0')
rewards[i]: 0.5557
values[i+1]: tensor([[0.1343]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1323]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.2092]], device='cuda:0')
delta_t: tensor([[0.5564]], device='cuda:0')
------ ------
policy_loss: 164.5133514404297
log_probs[i]: tensor([[-2.3963]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.2092]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 164.5133514404297
value_loss: 198.48745727539062
loss: 263.757080078125



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.3147e-02, -3.6267e-05, -2.7779e-05, -9.8772e-05, -8.5501e-05],
        [ 3.5506e-01,  9.7786e-04,  7.4659e-04,  2.6713e-03,  2.3164e-03],
        [-1.8323e-03, -5.0554e-06, -3.8949e-06, -1.3834e-05, -1.1909e-05],
        [ 1.5179e+00,  4.1753e-03,  3.1913e-03,  1.1436e-02,  9.9293e-03],
        [ 5.7149e+00,  1.5714e-02,  1.2034e-02,  4.3057e-02,  3.7403e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.0077e-04,  9.7842e-05, -1.0414e-04, -1.0632e-04,  1.5259e-04],
        [-5.4202e-03, -2.6399e-03,  2.8091e-03,  2.8689e-03, -4.1170e-03],
        [ 2.8011e-05,  1.3632e-05, -1.4527e-05, -1.4819e-05,  2.1275e-05],
        [-2.3170e-02, -1.1280e-02,  1.2001e-02,  1.2259e-02, -1.7591e-02],
        [-8.7222e-02, -4.2466e-02,  4.5170e-02,  4.6152e-02, -6.6219e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.8364, -1.7935,  1.8341, -1.5584,  1.1998],
        [-0.7007,  0.6843, -0.6998,  0.5946, -0.4578],
        [-0.7654,  0.7475, -0.7644,  0.6495, -0.5001],
        [-0.0414,  0.0404, -0.0413,  0.0351, -0.0270],
        [-0.3290,  0.3213, -0.3286,  0.2792, -0.2149]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-8.5868e-03, -4.3343e-03,  4.1828e-03,  4.5913e-03, -6.5573e-03],
        [ 1.7163e-02,  8.6762e-03, -9.2065e-03, -9.1096e-03,  1.3825e-02],
        [-6.4016e-02, -3.0671e-02,  3.3182e-02,  3.3451e-02, -4.8317e-02],
        [ 9.3150e-03,  4.0965e-03, -5.2509e-03, -4.6657e-03,  6.9710e-03],
        [ 7.3919e-02,  3.5771e-02, -3.7573e-02, -3.8815e-02,  5.5538e-02],
        [ 6.0597e-02,  2.9178e-02, -3.0552e-02, -3.1909e-02,  4.5037e-02],
        [-4.5236e-02, -2.1885e-02,  2.3053e-02,  2.3810e-02, -3.3940e-02],
        [ 1.0382e-02,  5.3497e-03, -4.6899e-03, -5.6959e-03,  7.6763e-03],
        [ 2.9953e-03,  9.3572e-04, -2.5899e-03, -1.1731e-03,  2.5966e-03],
        [-5.7339e-02, -2.7561e-02,  2.9515e-02,  3.0044e-02, -4.3146e-02],
        [ 8.0568e-04,  4.4343e-04, -7.0447e-05, -5.2846e-04,  3.1581e-04]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.8605,  1.8495, -2.0011, -2.0172,  2.9137]], device='cuda:0')
--> [loss] 263.757080078125

---------------------------------- [[#30 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     5.0      |     4.0     | 0.6317 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.640829956744943 acc: 0.5816
[Epoch 7] loss: 3.7743407282073176 acc: 0.6422
[Epoch 11] loss: 2.306807392469757 acc: 0.6432
[Epoch 15] loss: 1.1241436960066067 acc: 0.6389
[Epoch 19] loss: 0.6147896791796398 acc: 0.6405
[Epoch 23] loss: 0.41808888583879944 acc: 0.6225
[Epoch 27] loss: 0.347373798811127 acc: 0.6193
[Epoch 31] loss: 0.2990311014101557 acc: 0.6384
[Epoch 35] loss: 0.2792783154608191 acc: 0.6372
[Epoch 39] loss: 0.25343890230902627 acc: 0.628
[Epoch 43] loss: 0.2340978502565066 acc: 0.6336
[Epoch 47] loss: 0.22393702956921685 acc: 0.6314
[Epoch 51] loss: 0.20383874290501294 acc: 0.6254
[Epoch 55] loss: 0.1838380457128367 acc: 0.6323
[Epoch 59] loss: 0.18327056889629464 acc: 0.6277
[Epoch 63] loss: 0.18039961084675835 acc: 0.6173
[Epoch 67] loss: 0.16700847297543636 acc: 0.6349
[Epoch 71] loss: 0.1508744481862868 acc: 0.6279
--> [test] acc: 0.6213
--> [accuracy] finished 0.6213
new state: tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
new reward: 0.6213
--> [reward] 0.6213
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     4.0     | 0.6213 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.750735286861429 acc: 0.5778
[Epoch 7] loss: 3.9302950201132107 acc: 0.6291
[Epoch 11] loss: 2.5150440917219345 acc: 0.6127
[Epoch 15] loss: 1.2829055100336404 acc: 0.6195
[Epoch 19] loss: 0.6915848137968031 acc: 0.6226
[Epoch 23] loss: 0.4704046153756397 acc: 0.6215
[Epoch 27] loss: 0.37528227734119846 acc: 0.6168
[Epoch 31] loss: 0.34068098597352386 acc: 0.6115
[Epoch 35] loss: 0.29132605651322074 acc: 0.6139
[Epoch 39] loss: 0.26994813174304677 acc: 0.6158
[Epoch 43] loss: 0.24705145141476637 acc: 0.604
[Epoch 47] loss: 0.23357128889997825 acc: 0.6138
[Epoch 51] loss: 0.2149246432782744 acc: 0.6178
[Epoch 55] loss: 0.21294756071961216 acc: 0.6021
[Epoch 59] loss: 0.19143423180588903 acc: 0.6039
[Epoch 63] loss: 0.19155799114690794 acc: 0.6139
[Epoch 67] loss: 0.1769728279956009 acc: 0.6098
[Epoch 71] loss: 0.16898718732349632 acc: 0.6034
--> [test] acc: 0.6096
--> [accuracy] finished 0.6096
new state: tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.6096
--> [reward] 0.6096
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     5.0     | 0.6096 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.724886291929523 acc: 0.5547
[Epoch 7] loss: 3.8325429360579957 acc: 0.6228
[Epoch 11] loss: 2.3662669363305393 acc: 0.6399
[Epoch 15] loss: 1.210046507265714 acc: 0.6334
[Epoch 19] loss: 0.6421807351071969 acc: 0.6276
[Epoch 23] loss: 0.4562163313903162 acc: 0.6315
[Epoch 27] loss: 0.36693788876952343 acc: 0.6314
[Epoch 31] loss: 0.3118195525915993 acc: 0.6187
[Epoch 35] loss: 0.2964809410859976 acc: 0.625
[Epoch 39] loss: 0.253590125609618 acc: 0.6163
[Epoch 43] loss: 0.24200017206237445 acc: 0.6149
[Epoch 47] loss: 0.22518058344507424 acc: 0.6235
[Epoch 51] loss: 0.20043093164491912 acc: 0.6211
[Epoch 55] loss: 0.20445958629865058 acc: 0.6182
[Epoch 59] loss: 0.1910619618349692 acc: 0.62
[Epoch 63] loss: 0.1847583969085551 acc: 0.6198
[Epoch 67] loss: 0.16894343354956 acc: 0.6258
[Epoch 71] loss: 0.16166707956472703 acc: 0.6175
--> [test] acc: 0.6205
--> [accuracy] finished 0.6205
new state: tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
new reward: 0.6205
--> [reward] 0.6205
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     4.0      |     5.0     | 0.6205 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.779797174131779 acc: 0.5438
[Epoch 7] loss: 3.918273077596484 acc: 0.6197
[Epoch 11] loss: 2.4690801502036317 acc: 0.6289
[Epoch 15] loss: 1.2492800137347273 acc: 0.6306
[Epoch 19] loss: 0.6890700765411415 acc: 0.6251
[Epoch 23] loss: 0.4671344788902251 acc: 0.6221
[Epoch 27] loss: 0.3745964591812981 acc: 0.6273
[Epoch 31] loss: 0.32706940396095785 acc: 0.6228
[Epoch 35] loss: 0.2860500333530595 acc: 0.6077
[Epoch 39] loss: 0.27638076323911054 acc: 0.6171
[Epoch 43] loss: 0.24844109340835258 acc: 0.6231
[Epoch 47] loss: 0.23316865397945924 acc: 0.6131
[Epoch 51] loss: 0.21274606134299465 acc: 0.6225
[Epoch 55] loss: 0.20192288855552826 acc: 0.6181
[Epoch 59] loss: 0.19522036230215406 acc: 0.6168
[Epoch 63] loss: 0.185495895974319 acc: 0.6128
[Epoch 67] loss: 0.17503357443320172 acc: 0.6156
[Epoch 71] loss: 0.1818653786831233 acc: 0.6195
--> [test] acc: 0.6243
--> [accuracy] finished 0.6243
new state: tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.6243
--> [reward] 0.6243
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     2.0     |     5.0      |     5.0     | 0.6243 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.809207135454163 acc: 0.53
[Epoch 7] loss: 3.9708816942656435 acc: 0.6319
[Epoch 11] loss: 2.5518032945025606 acc: 0.6385
[Epoch 15] loss: 1.2993245505730209 acc: 0.6316
[Epoch 19] loss: 0.6967754133779298 acc: 0.6069
[Epoch 23] loss: 0.47840662638816384 acc: 0.6181
[Epoch 27] loss: 0.37559719688599674 acc: 0.6253
[Epoch 31] loss: 0.32961056759233215 acc: 0.6134
[Epoch 35] loss: 0.2978688016810152 acc: 0.6054
[Epoch 39] loss: 0.2543516586370328 acc: 0.6059
[Epoch 43] loss: 0.25990088967858904 acc: 0.6106
[Epoch 47] loss: 0.23192114082148388 acc: 0.6074
[Epoch 51] loss: 0.22085682068334517 acc: 0.6192
[Epoch 55] loss: 0.20284167116465013 acc: 0.6126
[Epoch 59] loss: 0.19167339435094954 acc: 0.6157
[Epoch 63] loss: 0.18490994405476294 acc: 0.6109
[Epoch 67] loss: 0.16715040976000603 acc: 0.6172
[Epoch 71] loss: 0.17915498179709896 acc: 0.6166
--> [test] acc: 0.6163
--> [accuracy] finished 0.6163
new state: tensor([672.,   2.,   2.,   5.,   5.], device='cuda:0')
new reward: 0.6163
--> [reward] 0.6163
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     5.0      |     5.0     | 0.6163 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3963, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([672.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.470634298251413 acc: 0.5871
[Epoch 7] loss: 3.4573906756880337 acc: 0.6687
[Epoch 11] loss: 1.8598280233876479 acc: 0.6605
[Epoch 15] loss: 0.8377409039155754 acc: 0.6594
[Epoch 19] loss: 0.4720721186931862 acc: 0.6625
[Epoch 23] loss: 0.35269934129770225 acc: 0.6492
[Epoch 27] loss: 0.29075890002043353 acc: 0.6497
[Epoch 31] loss: 0.26338546781722083 acc: 0.6523
[Epoch 35] loss: 0.2301995992646231 acc: 0.6559
[Epoch 39] loss: 0.21430563336700353 acc: 0.6578
[Epoch 43] loss: 0.18991051622382973 acc: 0.6435
[Epoch 47] loss: 0.1853992871277968 acc: 0.651
[Epoch 51] loss: 0.16991350890311133 acc: 0.6477
[Epoch 55] loss: 0.15225708643045952 acc: 0.6585
[Epoch 59] loss: 0.16581803116747332 acc: 0.6475
[Epoch 63] loss: 0.14056614958002325 acc: 0.6476
[Epoch 67] loss: 0.14410619861910792 acc: 0.6475
[Epoch 71] loss: 0.13318167136305625 acc: 0.6461
--> [test] acc: 0.6452
--> [accuracy] finished 0.6452
new state: tensor([672.,   2.,   3.,   5.,   5.], device='cuda:0')
new reward: 0.6452
--> [reward] 0.6452
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     5.0      |     5.0     | 0.6452 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.481857253919782 acc: 0.5963
[Epoch 7] loss: 3.4512852462356354 acc: 0.6549
[Epoch 11] loss: 1.835560044440467 acc: 0.6513
[Epoch 15] loss: 0.8122029388918901 acc: 0.6478
[Epoch 19] loss: 0.46894622895666555 acc: 0.6558
[Epoch 23] loss: 0.3546697330634917 acc: 0.6526
[Epoch 27] loss: 0.2987230560239738 acc: 0.6491
[Epoch 31] loss: 0.26178521922101144 acc: 0.6544
[Epoch 35] loss: 0.23054916511678025 acc: 0.649
[Epoch 39] loss: 0.20938642771766924 acc: 0.6467
[Epoch 43] loss: 0.19945221735150231 acc: 0.6428
[Epoch 47] loss: 0.18619013473372478 acc: 0.6495
[Epoch 51] loss: 0.16789455642290127 acc: 0.6457
[Epoch 55] loss: 0.1695175084524819 acc: 0.6514
[Epoch 59] loss: 0.1421717350654628 acc: 0.6443
[Epoch 63] loss: 0.1466640792477428 acc: 0.6407
[Epoch 67] loss: 0.1329940393009721 acc: 0.6548
[Epoch 71] loss: 0.13649129473260077 acc: 0.6512
--> [test] acc: 0.6452
--> [accuracy] finished 0.6452
new state: tensor([704.,   2.,   3.,   5.,   5.], device='cuda:0')
new reward: 0.6452
--> [reward] 0.6452
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     3.0     |     5.0      |     5.0     | 0.6452 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0925, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.499127313304131 acc: 0.5875
[Epoch 7] loss: 3.470103791302732 acc: 0.67
[Epoch 11] loss: 1.8486013408283444 acc: 0.6459
[Epoch 15] loss: 0.8330291604427883 acc: 0.6578
[Epoch 19] loss: 0.46694142077013356 acc: 0.6554
[Epoch 23] loss: 0.35321683304198565 acc: 0.6419
[Epoch 27] loss: 0.294330990239692 acc: 0.6449
[Epoch 31] loss: 0.26082644704967506 acc: 0.6535
[Epoch 35] loss: 0.2367375628770236 acc: 0.6486
[Epoch 39] loss: 0.21317954062748595 acc: 0.6447
[Epoch 43] loss: 0.19564559314009325 acc: 0.6487
[Epoch 47] loss: 0.1885408297290697 acc: 0.6491
[Epoch 51] loss: 0.1712072340094501 acc: 0.6551
[Epoch 55] loss: 0.15616056455485047 acc: 0.6496
[Epoch 59] loss: 0.15009997859883034 acc: 0.6477
[Epoch 63] loss: 0.1509499508658867 acc: 0.6532
[Epoch 67] loss: 0.13732913060737845 acc: 0.6414
[Epoch 71] loss: 0.14239826732966573 acc: 0.6416
--> [test] acc: 0.6354
--> [accuracy] finished 0.6354
new state: tensor([672.,   2.,   3.,   5.,   5.], device='cuda:0')
new reward: 0.6354
--> [reward] 0.6354
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     5.0      |     5.0     | 0.6354 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   3.,   5.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.891218722933698 acc: 0.5227
[Epoch 7] loss: 4.240237936949181 acc: 0.5986
[Epoch 11] loss: 2.9044789215335456 acc: 0.6
[Epoch 15] loss: 1.629122073235719 acc: 0.5949
[Epoch 19] loss: 0.8681356187271493 acc: 0.5832
[Epoch 23] loss: 0.5603457343385881 acc: 0.5948
[Epoch 27] loss: 0.4545918642340795 acc: 0.5871
[Epoch 31] loss: 0.38115024986817403 acc: 0.58
[Epoch 35] loss: 0.34214771707373126 acc: 0.5788
[Epoch 39] loss: 0.3256937416837267 acc: 0.5867
[Epoch 43] loss: 0.29733942822574655 acc: 0.5875
[Epoch 47] loss: 0.268394554281593 acc: 0.5843
[Epoch 51] loss: 0.2582417341458904 acc: 0.5849
[Epoch 55] loss: 0.2392810231165203 acc: 0.5862
[Epoch 59] loss: 0.2293990972378027 acc: 0.5952
[Epoch 63] loss: 0.21898033384047924 acc: 0.5918
[Epoch 67] loss: 0.20864761333026544 acc: 0.5826
[Epoch 71] loss: 0.20400866616607818 acc: 0.5897
--> [test] acc: 0.584
--> [accuracy] finished 0.584
new state: tensor([672.,   1.,   3.,   5.,   5.], device='cuda:0')
new reward: 0.584
--> [reward] 0.584
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     3.0     |     5.0      |     5.0     | 0.584  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   3.,   5.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   1.,   3.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.83358007288345 acc: 0.5408
[Epoch 7] loss: 4.116312420124288 acc: 0.6133
[Epoch 11] loss: 2.7355437652229346 acc: 0.6173
[Epoch 15] loss: 1.4656092563019993 acc: 0.6072
[Epoch 19] loss: 0.7663359206236536 acc: 0.599
[Epoch 23] loss: 0.5109986224261772 acc: 0.5982
[Epoch 27] loss: 0.41255439962010326 acc: 0.5938
[Epoch 31] loss: 0.36439953090103766 acc: 0.603
[Epoch 35] loss: 0.31342625889755654 acc: 0.5944
[Epoch 39] loss: 0.29033675801266184 acc: 0.603
[Epoch 43] loss: 0.26902292680967116 acc: 0.5885
[Epoch 47] loss: 0.2525795915085451 acc: 0.595
[Epoch 51] loss: 0.24024486565567038 acc: 0.5833
[Epoch 55] loss: 0.24343634981785894 acc: 0.5863
[Epoch 59] loss: 0.2069370356838569 acc: 0.5947
[Epoch 63] loss: 0.21509790456498903 acc: 0.5972
[Epoch 67] loss: 0.18826470622445082 acc: 0.5924
[Epoch 71] loss: 0.19230522728189253 acc: 0.5854
--> [test] acc: 0.5906
--> [accuracy] finished 0.5906
new state: tensor([672.,   1.,   3.,   5.,   4.], device='cuda:0')
new reward: 0.5906
--> [reward] 0.5906
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     3.0     |     5.0      |     4.0     | 0.5906 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   3.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([640.,   1.,   3.,   5.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.799531796094402 acc: 0.5471
[Epoch 7] loss: 4.115105642687024 acc: 0.5998
[Epoch 11] loss: 2.7744639170596668 acc: 0.6216
[Epoch 15] loss: 1.5203029306800775 acc: 0.6074
[Epoch 19] loss: 0.8042208262912148 acc: 0.6142
[Epoch 23] loss: 0.5320054522246275 acc: 0.5992
[Epoch 27] loss: 0.42623357504339476 acc: 0.6115
[Epoch 31] loss: 0.35257615737350245 acc: 0.6037
[Epoch 35] loss: 0.33277605742197053 acc: 0.6066
[Epoch 39] loss: 0.2984709057556775 acc: 0.6041
[Epoch 43] loss: 0.2808681155133354 acc: 0.6077
[Epoch 47] loss: 0.2652035418378613 acc: 0.608
[Epoch 51] loss: 0.24658033714803587 acc: 0.6
[Epoch 55] loss: 0.23359130805744158 acc: 0.6012
[Epoch 59] loss: 0.21838959729027413 acc: 0.6032
[Epoch 63] loss: 0.21325483904136797 acc: 0.6006
[Epoch 67] loss: 0.2051338032459664 acc: 0.6068
[Epoch 71] loss: 0.1889192131050217 acc: 0.6011
--> [test] acc: 0.6004
--> [accuracy] finished 0.6004
new state: tensor([640.,   1.,   3.,   5.,   4.], device='cuda:0')
new reward: 0.6004
--> [reward] 0.6004
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     3.0     |     5.0      |     4.0     | 0.6004 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   3.,   5.,   4.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([640.,   1.,   3.,   4.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.99757869438747 acc: 0.5405
[Epoch 7] loss: 4.228550022215489 acc: 0.594
[Epoch 11] loss: 2.8566257178478534 acc: 0.6101
[Epoch 15] loss: 1.571720529585848 acc: 0.6025
[Epoch 19] loss: 0.8262575658614678 acc: 0.5962
[Epoch 23] loss: 0.5501995177944298 acc: 0.6046
[Epoch 27] loss: 0.4272501713041302 acc: 0.5951
[Epoch 31] loss: 0.36795536699035514 acc: 0.6007
[Epoch 35] loss: 0.3341109702849518 acc: 0.5984
[Epoch 39] loss: 0.30038640677185774 acc: 0.5926
[Epoch 43] loss: 0.28957325101013076 acc: 0.5898
[Epoch 47] loss: 0.2631140950414569 acc: 0.5913
[Epoch 51] loss: 0.24389493662883024 acc: 0.5902
[Epoch 55] loss: 0.2457082974831657 acc: 0.5882
[Epoch 59] loss: 0.2118739697086575 acc: 0.5927
[Epoch 63] loss: 0.20986410687246437 acc: 0.5819
[Epoch 67] loss: 0.21620647933291237 acc: 0.5968
[Epoch 71] loss: 0.18533144437272073 acc: 0.5897
--> [test] acc: 0.5969
--> [accuracy] finished 0.5969
new state: tensor([640.,   1.,   3.,   4.,   4.], device='cuda:0')
new reward: 0.5969
--> [reward] 0.5969
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     3.0     |     4.0      |     4.0     | 0.5969 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   3.,   4.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([640.,   1.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.058539234768704 acc: 0.5341
[Epoch 7] loss: 4.3607861888987935 acc: 0.584
[Epoch 11] loss: 3.0088579482434654 acc: 0.5969
[Epoch 15] loss: 1.7210957878233526 acc: 0.5871
[Epoch 19] loss: 0.9216106569637423 acc: 0.5826
[Epoch 23] loss: 0.5914536369654834 acc: 0.5728
[Epoch 27] loss: 0.4735935923436185 acc: 0.5731
[Epoch 31] loss: 0.396069546261102 acc: 0.5808
[Epoch 35] loss: 0.3538559317288687 acc: 0.5882
[Epoch 39] loss: 0.32617178105313305 acc: 0.5917
[Epoch 43] loss: 0.2962073900848818 acc: 0.5782
[Epoch 47] loss: 0.27490129580726025 acc: 0.5813
[Epoch 51] loss: 0.2576134899902679 acc: 0.5774
[Epoch 55] loss: 0.2448259664608923 acc: 0.5814
[Epoch 59] loss: 0.24173043101854488 acc: 0.5823
[Epoch 63] loss: 0.2284126789035166 acc: 0.5752
[Epoch 67] loss: 0.20020885472340733 acc: 0.5746
[Epoch 71] loss: 0.21213260496182895 acc: 0.5796
--> [test] acc: 0.5799
--> [accuracy] finished 0.5799
new state: tensor([640.,   1.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.5799
--> [reward] 0.5799
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     3.0     |     4.0      |     5.0     | 0.5799 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([640.,   2.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.460070085799908 acc: 0.6053
[Epoch 7] loss: 3.3856036227072597 acc: 0.6642
[Epoch 11] loss: 1.7924740126408885 acc: 0.6601
[Epoch 15] loss: 0.7831789506670764 acc: 0.6561
[Epoch 19] loss: 0.4719662669321994 acc: 0.659
[Epoch 23] loss: 0.34228405752755187 acc: 0.6607
[Epoch 27] loss: 0.285567032573435 acc: 0.6531
[Epoch 31] loss: 0.2430425491874747 acc: 0.6513
[Epoch 35] loss: 0.2319802121447442 acc: 0.6492
[Epoch 39] loss: 0.2034785067329131 acc: 0.6411
[Epoch 43] loss: 0.19302872593855233 acc: 0.6499
[Epoch 47] loss: 0.17421779512425364 acc: 0.6411
[Epoch 51] loss: 0.1637460825550358 acc: 0.6539
[Epoch 55] loss: 0.15642984867896265 acc: 0.6462
[Epoch 59] loss: 0.14369153512565566 acc: 0.65
[Epoch 63] loss: 0.14765787389381882 acc: 0.651
[Epoch 67] loss: 0.129018607769695 acc: 0.6489
[Epoch 71] loss: 0.13323595059220977 acc: 0.6527
--> [test] acc: 0.6459
--> [accuracy] finished 0.6459
new state: tensor([640.,   2.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.6459
--> [reward] 0.6459
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     4.0      |     5.0     | 0.6459 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0911, 0.0905, 0.0914, 0.0908, 0.0910, 0.0913, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3977, -2.3983, -2.3974, -2.3980, -2.3978, -2.3975,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   2.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.458835080608992 acc: 0.5721
[Epoch 7] loss: 3.383706988962105 acc: 0.6653
[Epoch 11] loss: 1.7906438833307428 acc: 0.6587
[Epoch 15] loss: 0.8043815774266677 acc: 0.6592
[Epoch 19] loss: 0.46261951091039516 acc: 0.6563
[Epoch 23] loss: 0.3478000695409868 acc: 0.6574
[Epoch 27] loss: 0.2897656737252727 acc: 0.6585
[Epoch 31] loss: 0.2563365768888951 acc: 0.6477
[Epoch 35] loss: 0.22452777699160073 acc: 0.6597
[Epoch 39] loss: 0.2112968367002809 acc: 0.6662
[Epoch 43] loss: 0.1814583247060628 acc: 0.658
[Epoch 47] loss: 0.1812870777104898 acc: 0.6635
[Epoch 51] loss: 0.16441080970284733 acc: 0.6562
[Epoch 55] loss: 0.15390653539772914 acc: 0.663
[Epoch 59] loss: 0.14621352755328845 acc: 0.6472
[Epoch 63] loss: 0.14743416430190434 acc: 0.6566
[Epoch 67] loss: 0.13842107053450725 acc: 0.6612
[Epoch 71] loss: 0.12628274663563466 acc: 0.6482
--> [test] acc: 0.6507
--> [accuracy] finished 0.6507
new state: tensor([640.,   2.,   3.,   4.,   5.], device='cuda:0')
new reward: 0.6507
--> [reward] 0.6507
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2107]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4214]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6491]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7848]], device='cuda:0')
------ ------
delta_t: tensor([[0.6491]], device='cuda:0')
rewards[i]: 0.6507
values[i+1]: tensor([[0.1355]], device='cuda:0')
values[i]: tensor([[0.1357]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6491]], device='cuda:0')
delta_t: tensor([[0.6491]], device='cuda:0')
------ ------
policy_loss: 1.5328913927078247
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6491]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0388]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6563]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2870]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4229]], device='cuda:0')
------ ------
delta_t: tensor([[0.6443]], device='cuda:0')
rewards[i]: 0.6459
values[i+1]: tensor([[0.1357]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1359]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2870]], device='cuda:0')
delta_t: tensor([[0.6443]], device='cuda:0')
------ ------
policy_loss: 4.595459938049316
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2870]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.7538]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.4299]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.8520]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9886]], device='cuda:0')
------ ------
delta_t: tensor([[0.5779]], device='cuda:0')
rewards[i]: 0.5799
values[i+1]: tensor([[0.1359]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1365]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.8520]], device='cuda:0')
delta_t: tensor([[0.5779]], device='cuda:0')
------ ------
policy_loss: 9.013326644897461
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.8520]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[5.7024]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.8972]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.4284]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.5656]], device='cuda:0')
------ ------
delta_t: tensor([[0.5949]], device='cuda:0')
rewards[i]: 0.5969
values[i+1]: tensor([[0.1365]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1371]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.4284]], device='cuda:0')
delta_t: tensor([[0.5949]], device='cuda:0')
------ ------
policy_loss: 14.812277793884277
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.4284]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[10.2087]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[9.0126]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.0021]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.1403]], device='cuda:0')
------ ------
delta_t: tensor([[0.5980]], device='cuda:0')
rewards[i]: 0.6004
values[i+1]: tensor([[0.1371]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1382]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.0021]], device='cuda:0')
delta_t: tensor([[0.5980]], device='cuda:0')
------ ------
policy_loss: 21.985170364379883
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.0021]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[16.5487]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.6798]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.5609]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6995]], device='cuda:0')
------ ------
delta_t: tensor([[0.5888]], device='cuda:0')
rewards[i]: 0.5906
values[i+1]: tensor([[0.1382]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1386]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.5609]], device='cuda:0')
delta_t: tensor([[0.5888]], device='cuda:0')
------ ------
policy_loss: 30.504079818725586
log_probs[i]: tensor([[-2.3991]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.5609]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[24.9835]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.8696]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.1073]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2465]], device='cuda:0')
------ ------
delta_t: tensor([[0.5820]], device='cuda:0')
rewards[i]: 0.584
values[i+1]: tensor([[0.1386]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1393]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.1073]], device='cuda:0')
delta_t: tensor([[0.5820]], device='cuda:0')
------ ------
policy_loss: 40.328216552734375
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.1073]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[36.0254]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.0840]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.6994]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.8395]], device='cuda:0')
------ ------
delta_t: tensor([[0.6332]], device='cuda:0')
rewards[i]: 0.6354
values[i+1]: tensor([[0.1393]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1401]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.6994]], device='cuda:0')
delta_t: tensor([[0.6332]], device='cuda:0')
------ ------
policy_loss: 51.56986618041992
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.6994]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[50.0515]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[28.0522]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.2964]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4363]], device='cuda:0')
------ ------
delta_t: tensor([[0.6441]], device='cuda:0')
rewards[i]: 0.6452
values[i+1]: tensor([[0.1401]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1398]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.2964]], device='cuda:0')
delta_t: tensor([[0.6441]], device='cuda:0')
------ ------
policy_loss: 64.238037109375
log_probs[i]: tensor([[-2.3964]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.2964]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[67.3784]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[34.6537]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.8867]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.0271]], device='cuda:0')
------ ------
delta_t: tensor([[0.6433]], device='cuda:0')
rewards[i]: 0.6452
values[i+1]: tensor([[0.1398]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1404]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.8867]], device='cuda:0')
delta_t: tensor([[0.6433]], device='cuda:0')
------ ------
policy_loss: 78.33055877685547
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.8867]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[88.1296]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[41.5025]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.4422]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.5831]], device='cuda:0')
------ ------
delta_t: tensor([[0.6144]], device='cuda:0')
rewards[i]: 0.6163
values[i+1]: tensor([[0.1404]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1409]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.4422]], device='cuda:0')
delta_t: tensor([[0.6144]], device='cuda:0')
------ ------
policy_loss: 93.7503433227539
log_probs[i]: tensor([[-2.3973]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.4422]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[112.6376]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[49.0160]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.0011]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1416]], device='cuda:0')
------ ------
delta_t: tensor([[0.6233]], device='cuda:0')
rewards[i]: 0.6243
values[i+1]: tensor([[0.1409]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1404]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.0011]], device='cuda:0')
delta_t: tensor([[0.6233]], device='cuda:0')
------ ------
policy_loss: 110.51139831542969
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.0011]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[141.1455]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[57.0157]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.5509]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.6907]], device='cuda:0')
------ ------
delta_t: tensor([[0.6197]], device='cuda:0')
rewards[i]: 0.6205
values[i+1]: tensor([[0.1404]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1398]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.5509]], device='cuda:0')
delta_t: tensor([[0.6197]], device='cuda:0')
------ ------
policy_loss: 128.59307861328125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.5509]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[173.8238]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[65.3565]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.0843]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.2234]], device='cuda:0')
------ ------
delta_t: tensor([[0.6090]], device='cuda:0')
rewards[i]: 0.6096
values[i+1]: tensor([[0.1398]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1390]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.0843]], device='cuda:0')
delta_t: tensor([[0.6090]], device='cuda:0')
------ ------
policy_loss: 147.95852661132812
log_probs[i]: tensor([[-2.3984]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.0843]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[211.0182]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.3889]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6249]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7624]], device='cuda:0')
------ ------
delta_t: tensor([[0.6214]], device='cuda:0')
rewards[i]: 0.6213
values[i+1]: tensor([[0.1390]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.1375]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6249]], device='cuda:0')
delta_t: tensor([[0.6214]], device='cuda:0')
------ ------
policy_loss: 168.60289001464844
log_probs[i]: tensor([[-2.3964]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6249]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 168.60289001464844
value_loss: 211.01821899414062
loss: 274.11199951171875



--> [print] for grads need to update
model.Wai.weight.grad tensor([[-1.8483e-02, -5.0720e-05, -6.1121e-05, -1.3469e-04, -1.1621e-04],
        [ 4.1672e-01,  1.1511e-03,  1.3558e-03,  3.0392e-03,  2.6034e-03],
        [-2.3328e-03, -6.3898e-06, -7.7633e-06, -1.6980e-05, -1.4727e-05],
        [ 1.6068e+00,  4.4472e-03,  5.2015e-03,  1.1712e-02,  1.0047e-02],
        [ 5.7470e+00,  1.5929e-02,  1.8556e-02,  4.1914e-02,  3.5891e-02]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[ 2.9669e-04,  1.3568e-04, -1.6370e-04, -1.4797e-04,  2.2847e-04],
        [-6.6744e-03, -3.0555e-03,  3.6751e-03,  3.3327e-03, -5.1354e-03],
        [ 3.7448e-05,  1.7113e-05, -2.0680e-05, -1.8661e-05,  2.8845e-05],
        [-2.5709e-02, -1.1770e-02,  1.4147e-02,  1.2838e-02, -1.9774e-02],
        [-9.1911e-02, -4.2090e-02,  5.0555e-02,  4.5910e-02, -7.0681e-02]],
       device='cuda:0')
model.att.weight.grad tensor([[ 1.8536, -1.8005,  1.8505, -1.5392,  1.1796],
        [-0.6960,  0.6761, -0.6948,  0.5781, -0.4430],
        [-0.7805,  0.7581, -0.7792,  0.6480, -0.4966],
        [-0.0432,  0.0420, -0.0432,  0.0359, -0.0275],
        [-0.3339,  0.3243, -0.3334,  0.2772, -0.2124]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0789,  0.0357, -0.0440, -0.0388,  0.0606],
        [ 0.0749,  0.0336, -0.0420, -0.0367,  0.0581],
        [-0.0237, -0.0107,  0.0127,  0.0117, -0.0180],
        [-0.0519, -0.0235,  0.0286,  0.0257, -0.0398],
        [-0.0647, -0.0292,  0.0362,  0.0317, -0.0500],
        [-0.0049, -0.0023,  0.0026,  0.0024, -0.0039],
        [ 0.0357,  0.0163, -0.0197, -0.0176,  0.0276],
        [ 0.0073,  0.0037, -0.0031, -0.0040,  0.0050],
        [-0.0288, -0.0131,  0.0156,  0.0143, -0.0219],
        [ 0.0406,  0.0182, -0.0225, -0.0198,  0.0315],
        [-0.0635, -0.0286,  0.0355,  0.0311, -0.0491]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 3.8765,  1.7472, -2.1700, -1.8988,  2.9970]], device='cuda:0')
--> [loss] 274.11199951171875

---------------------------------- [[#31 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     4.0      |     5.0     | 0.6507 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     7.0      |     3.0     | 0.4472 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0915, 0.0924, 0.0910, 0.0905, 0.0914, 0.0908, 0.0910, 0.0914, 0.0897,
         0.0904, 0.0898]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3973, -2.3964, -2.3978, -2.3983, -2.3974, -2.3980, -2.3978, -2.3974,
         -2.3991, -2.3984, -2.3990]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   4.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   1.,   3.,   4.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.004294240566166 acc: 0.5316
[Epoch 7] loss: 4.2710804366089805 acc: 0.5683
[Epoch 11] loss: 2.95981373422591 acc: 0.6016
