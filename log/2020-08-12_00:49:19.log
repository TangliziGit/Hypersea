
---------------------------------- [[#0 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+-----+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth | Acc |
+---------+----------+--------------+-------------+--------------+-------------+-----+
| current |   512    |      3       |      5      |      2       |      2      | 0.0 |
|   best  |    0     |      0       |      0      |      0       |      0      | 0.0 |
|  worst  |    0     |      0       |      0      |      0       |      0      | 1.0 |
+---------+----------+--------------+-------------+--------------+-------------+-----+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3979, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   5.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([480.,   3.,   5.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.990009720520595 acc: 0.6432
[Epoch 7] loss: 2.476622619058775 acc: 0.6892
[Epoch 11] loss: 0.829766438883322 acc: 0.6765
[Epoch 15] loss: 0.37911544224995253 acc: 0.6984
[Epoch 19] loss: 0.268418330708733 acc: 0.6798
[Epoch 23] loss: 0.21564598794302445 acc: 0.6934
[Epoch 27] loss: 0.1920432548164902 acc: 0.6905
[Epoch 31] loss: 0.16777719949345912 acc: 0.6824
[Epoch 35] loss: 0.1534125678970114 acc: 0.6914
[Epoch 39] loss: 0.1300407048687577 acc: 0.6962
[Epoch 43] loss: 0.12937429181926544 acc: 0.6958
[Epoch 47] loss: 0.11930648773394124 acc: 0.6925
[Epoch 51] loss: 0.11405852791267301 acc: 0.6886
[Epoch 55] loss: 0.10213444343573935 acc: 0.687
[Epoch 59] loss: 0.09906633129662565 acc: 0.6878
[Epoch 63] loss: 0.10168308319400071 acc: 0.6761
[Epoch 67] loss: 0.08213280818289351 acc: 0.6896
[Epoch 71] loss: 0.09033283485692767 acc: 0.6957
--> [test] acc: 0.6985
--> [accuracy] finished 0.6985
new state: tensor([480.,   3.,   5.,   2.,   2.], device='cuda:0')
new reward: 0.6985
--> [reward] 0.6985
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|   best  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|  worst  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3979, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([480.,   3.,   5.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([480.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.15931583761864 acc: 0.6107
[Epoch 7] loss: 2.7032000928583657 acc: 0.6807
[Epoch 11] loss: 0.9832560156979372 acc: 0.6921
[Epoch 15] loss: 0.4327128346142409 acc: 0.6775
[Epoch 19] loss: 0.2945412659155362 acc: 0.6797
[Epoch 23] loss: 0.244966822431025 acc: 0.6842
[Epoch 27] loss: 0.20466990348742442 acc: 0.68
[Epoch 31] loss: 0.18687987475729811 acc: 0.6683
[Epoch 35] loss: 0.16504945716989772 acc: 0.6778
[Epoch 39] loss: 0.14935292238059938 acc: 0.6783
[Epoch 43] loss: 0.13665183028801703 acc: 0.6696
[Epoch 47] loss: 0.13193503038629012 acc: 0.681
[Epoch 51] loss: 0.1290946631999615 acc: 0.6607
[Epoch 55] loss: 0.11488211967046265 acc: 0.6716
[Epoch 59] loss: 0.10722374217584729 acc: 0.6754
[Epoch 63] loss: 0.10063577796477834 acc: 0.6771
[Epoch 67] loss: 0.11099639431188774 acc: 0.6721
[Epoch 71] loss: 0.08644449693219894 acc: 0.675
--> [test] acc: 0.6811
--> [accuracy] finished 0.6811
new state: tensor([480.,   3.,   4.,   2.,   2.], device='cuda:0')
new reward: 0.6811
--> [reward] 0.6811
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  480.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6811 |
|   best  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|  worst  |  480.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6811 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0908, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3979, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([480.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.131424597309678 acc: 0.6266
[Epoch 7] loss: 2.695358050448815 acc: 0.6866
[Epoch 11] loss: 0.9627811394993911 acc: 0.6805
[Epoch 15] loss: 0.4339968564841525 acc: 0.6771
[Epoch 19] loss: 0.2974710267899401 acc: 0.675
[Epoch 23] loss: 0.23646207476544487 acc: 0.6736
[Epoch 27] loss: 0.2028774713616237 acc: 0.675
[Epoch 31] loss: 0.17819697089979183 acc: 0.6787
[Epoch 35] loss: 0.16011245007557637 acc: 0.6798
[Epoch 39] loss: 0.16036815947645802 acc: 0.6802
[Epoch 43] loss: 0.13910046496840617 acc: 0.6732
[Epoch 47] loss: 0.13532732808938647 acc: 0.6779
[Epoch 51] loss: 0.11522895980702566 acc: 0.6716
[Epoch 55] loss: 0.11125837735619748 acc: 0.6689
[Epoch 59] loss: 0.11603485491520385 acc: 0.6703
[Epoch 63] loss: 0.10399976513012672 acc: 0.6762
[Epoch 67] loss: 0.10145145449596826 acc: 0.6695
[Epoch 71] loss: 0.09727277744637655 acc: 0.6625
--> [test] acc: 0.6619
--> [accuracy] finished 0.6619
new state: tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
new reward: 0.6619
--> [reward] 0.6619
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
|   best  |  480.0   |     3.0      |     5.0     |     2.0      |     2.0     | 0.6985 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3979, -2.3979, -2.3979,
         -2.3979, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([512.,   3.,   4.,   1.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.333900255620327 acc: 0.7108
[Epoch 7] loss: 1.9043580475060835 acc: 0.769
[Epoch 11] loss: 0.5667019232612132 acc: 0.7652
[Epoch 15] loss: 0.2990397089625449 acc: 0.7647
[Epoch 19] loss: 0.20606794026072905 acc: 0.7579
[Epoch 23] loss: 0.17444107596002653 acc: 0.7663
[Epoch 27] loss: 0.1542976321271428 acc: 0.7689
[Epoch 31] loss: 0.14365241241872387 acc: 0.755
[Epoch 35] loss: 0.12077227419675768 acc: 0.7491
[Epoch 39] loss: 0.11611106678190858 acc: 0.7605
[Epoch 43] loss: 0.11706586377552289 acc: 0.7617
[Epoch 47] loss: 0.10300841991601468 acc: 0.7562
[Epoch 51] loss: 0.09655278600523691 acc: 0.7512
[Epoch 55] loss: 0.09100009858265252 acc: 0.7586
[Epoch 59] loss: 0.08896075982955949 acc: 0.7607
[Epoch 63] loss: 0.08272621063373374 acc: 0.7544
[Epoch 67] loss: 0.0888739759714314 acc: 0.7591
[Epoch 71] loss: 0.07739561214439257 acc: 0.759
--> [test] acc: 0.7535
--> [accuracy] finished 0.7535
new state: tensor([512.,   3.,   4.,   1.,   2.], device='cuda:0')
new reward: 0.7535
--> [reward] 0.7535
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0909, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   1.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.151465825869909 acc: 0.6156
[Epoch 7] loss: 2.7291731557730214 acc: 0.6872
[Epoch 11] loss: 0.9815312771464858 acc: 0.6903
[Epoch 15] loss: 0.43948596629702374 acc: 0.6883
[Epoch 19] loss: 0.30189216119306317 acc: 0.6856
[Epoch 23] loss: 0.24740915475865763 acc: 0.6745
[Epoch 27] loss: 0.2069993614816986 acc: 0.676
[Epoch 31] loss: 0.19354518713748745 acc: 0.6834
[Epoch 35] loss: 0.1626808324023662 acc: 0.6792
[Epoch 39] loss: 0.15142295727996952 acc: 0.6718
[Epoch 43] loss: 0.14258145221301813 acc: 0.6792
[Epoch 47] loss: 0.12939865638732986 acc: 0.675
[Epoch 51] loss: 0.1272754234278484 acc: 0.6791
[Epoch 55] loss: 0.11550288612488657 acc: 0.6688
[Epoch 59] loss: 0.11155879724999447 acc: 0.6807
[Epoch 63] loss: 0.11010203649655527 acc: 0.6757
[Epoch 67] loss: 0.09105152322385518 acc: 0.6863
[Epoch 71] loss: 0.09824261732418518 acc: 0.6753
--> [test] acc: 0.6772
--> [accuracy] finished 0.6772
new state: tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
new reward: 0.6772
--> [reward] 0.6772
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6772 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   2.,   2.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.906394550410073 acc: 0.6606
[Epoch 7] loss: 2.4659550529154366 acc: 0.7067
[Epoch 11] loss: 0.8564009316113141 acc: 0.7108
[Epoch 15] loss: 0.3819308122405616 acc: 0.7147
[Epoch 19] loss: 0.2630984958790033 acc: 0.7171
[Epoch 23] loss: 0.21635688648647283 acc: 0.704
[Epoch 27] loss: 0.18826018219523113 acc: 0.7154
[Epoch 31] loss: 0.16043504482299528 acc: 0.717
[Epoch 35] loss: 0.14487309926106115 acc: 0.7168
[Epoch 39] loss: 0.14572927875496694 acc: 0.7105
[Epoch 43] loss: 0.1266570470302992 acc: 0.7183
[Epoch 47] loss: 0.11481950491847819 acc: 0.706
[Epoch 51] loss: 0.1097071745535335 acc: 0.6968
[Epoch 55] loss: 0.11541454252594001 acc: 0.6994
[Epoch 59] loss: 0.09188940894701864 acc: 0.7037
[Epoch 63] loss: 0.10088358583602258 acc: 0.6958
[Epoch 67] loss: 0.09182379016643176 acc: 0.7075
[Epoch 71] loss: 0.08483582817237167 acc: 0.701
--> [test] acc: 0.7097
--> [accuracy] finished 0.7097
new state: tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7097
--> [reward] 0.7097
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     3.0      |     2.0     | 0.7097 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.930322900605018 acc: 0.661
[Epoch 7] loss: 2.475655254157608 acc: 0.7197
[Epoch 11] loss: 0.8651872173933041 acc: 0.7074
[Epoch 15] loss: 0.3848636535036823 acc: 0.6929
[Epoch 19] loss: 0.26342981819377836 acc: 0.7061
[Epoch 23] loss: 0.21164813610103428 acc: 0.7088
[Epoch 27] loss: 0.19138645104673283 acc: 0.7078
[Epoch 31] loss: 0.16218380495915405 acc: 0.7042
[Epoch 35] loss: 0.1547046505966369 acc: 0.7071
[Epoch 39] loss: 0.13449242241356685 acc: 0.7035
[Epoch 43] loss: 0.12193176249408966 acc: 0.7126
[Epoch 47] loss: 0.11439606877382669 acc: 0.7027
[Epoch 51] loss: 0.11304686945217574 acc: 0.7024
[Epoch 55] loss: 0.09878534129332475 acc: 0.6994
[Epoch 59] loss: 0.10480734937564681 acc: 0.7037
[Epoch 63] loss: 0.09560183922300482 acc: 0.6944
[Epoch 67] loss: 0.08395879537157734 acc: 0.7003
[Epoch 71] loss: 0.08996566427785836 acc: 0.7039
--> [test] acc: 0.6993
--> [accuracy] finished 0.6993
new state: tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.6993
--> [reward] 0.6993
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     3.0      |     2.0     | 0.6993 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([512.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.816040892125396 acc: 0.6757
[Epoch 7] loss: 2.3588149222495307 acc: 0.7267
[Epoch 11] loss: 0.8160821267205964 acc: 0.7199
[Epoch 15] loss: 0.3559085613240481 acc: 0.726
[Epoch 19] loss: 0.2628774145175048 acc: 0.7301
[Epoch 23] loss: 0.21241015985922512 acc: 0.7172
[Epoch 27] loss: 0.1826743110192611 acc: 0.7215
[Epoch 31] loss: 0.15967830286844803 acc: 0.7157
[Epoch 35] loss: 0.1453398948730639 acc: 0.7108
[Epoch 39] loss: 0.12544048839154037 acc: 0.7232
[Epoch 43] loss: 0.1294789375746837 acc: 0.7114
[Epoch 47] loss: 0.11904998434721814 acc: 0.7118
[Epoch 51] loss: 0.11003457948975169 acc: 0.7084
[Epoch 55] loss: 0.0998212387675897 acc: 0.7143
[Epoch 59] loss: 0.10375883018054888 acc: 0.7182
[Epoch 63] loss: 0.08879174631984567 acc: 0.7084
[Epoch 67] loss: 0.09021682886149535 acc: 0.7141
[Epoch 71] loss: 0.08998776407947506 acc: 0.7236
--> [test] acc: 0.7087
--> [accuracy] finished 0.7087
new state: tensor([512.,   3.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.7087
--> [reward] 0.7087
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  512.0   |     3.0      |     4.0     |     3.0      |     3.0     | 0.7087 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0909,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([512.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([544.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.84126653466993 acc: 0.6729
[Epoch 7] loss: 2.3571533239482307 acc: 0.7102
[Epoch 11] loss: 0.8172509450932293 acc: 0.7156
[Epoch 15] loss: 0.37072007740607194 acc: 0.7352
[Epoch 19] loss: 0.2640466430865209 acc: 0.7189
[Epoch 23] loss: 0.20796513153702173 acc: 0.7175
[Epoch 27] loss: 0.1801701035467274 acc: 0.7267
[Epoch 31] loss: 0.16400052887021951 acc: 0.7254
[Epoch 35] loss: 0.1485254560737773 acc: 0.7245
[Epoch 39] loss: 0.13325845835732816 acc: 0.7226
[Epoch 43] loss: 0.12726008919808451 acc: 0.7189
[Epoch 47] loss: 0.11234826605071498 acc: 0.7223
[Epoch 51] loss: 0.1060953471164131 acc: 0.7144
[Epoch 55] loss: 0.10499709259438486 acc: 0.716
[Epoch 59] loss: 0.0940324699237009 acc: 0.7196
[Epoch 63] loss: 0.08732998882994399 acc: 0.7144
[Epoch 67] loss: 0.08204876907679545 acc: 0.7176
[Epoch 71] loss: 0.09616912943382493 acc: 0.7226
--> [test] acc: 0.7124
--> [accuracy] finished 0.7124
new state: tensor([544.,   3.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.7124
--> [reward] 0.7124
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     3.0      |     4.0     |     3.0      |     3.0     | 0.7124 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([544.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([576.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.816857513869205 acc: 0.6513
[Epoch 7] loss: 2.3201236229037385 acc: 0.7181
[Epoch 11] loss: 0.7831668831417551 acc: 0.7127
[Epoch 15] loss: 0.3557751576732987 acc: 0.6981
[Epoch 19] loss: 0.25263676147603087 acc: 0.7202
[Epoch 23] loss: 0.21065582326658622 acc: 0.7151
[Epoch 27] loss: 0.18455977031194112 acc: 0.7163
[Epoch 31] loss: 0.15973438823427719 acc: 0.7202
[Epoch 35] loss: 0.14083464512341867 acc: 0.7183
[Epoch 39] loss: 0.13469377056876544 acc: 0.7182
[Epoch 43] loss: 0.11560629577944032 acc: 0.714
[Epoch 47] loss: 0.1272637384736439 acc: 0.7147
[Epoch 51] loss: 0.10573872725438813 acc: 0.7181
[Epoch 55] loss: 0.10502337777818603 acc: 0.7122
[Epoch 59] loss: 0.08544990725700966 acc: 0.7199
[Epoch 63] loss: 0.09685345903596343 acc: 0.7147
[Epoch 67] loss: 0.08918034023893497 acc: 0.7157
[Epoch 71] loss: 0.08789900424832162 acc: 0.7108
--> [test] acc: 0.712
--> [accuracy] finished 0.712
new state: tensor([576.,   3.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.712
--> [reward] 0.712
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     3.0      |     4.0     |     3.0      |     3.0     | 0.712  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3979, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([576.,   3.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([576.,   4.,   4.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.571665022531739 acc: 0.6838
[Epoch 7] loss: 2.014697411481072 acc: 0.7337
[Epoch 11] loss: 0.5949647559515198 acc: 0.7315
[Epoch 15] loss: 0.2824969606931367 acc: 0.7383
[Epoch 19] loss: 0.2124998050043955 acc: 0.7332
[Epoch 23] loss: 0.18260928314141073 acc: 0.7297
[Epoch 27] loss: 0.14646008003579306 acc: 0.7412
[Epoch 31] loss: 0.13754390252758855 acc: 0.7275
[Epoch 35] loss: 0.13035231112214304 acc: 0.7305
[Epoch 39] loss: 0.11614549698312875 acc: 0.7257
[Epoch 43] loss: 0.10119316139784844 acc: 0.7382
[Epoch 47] loss: 0.09249196195369944 acc: 0.7407
[Epoch 51] loss: 0.09312847522956788 acc: 0.7325
[Epoch 55] loss: 0.08234856635822779 acc: 0.74
[Epoch 59] loss: 0.08461515476588932 acc: 0.733
[Epoch 63] loss: 0.0751213446012972 acc: 0.7329
[Epoch 67] loss: 0.08293134790850873 acc: 0.7319
[Epoch 71] loss: 0.06452424114074587 acc: 0.7328
--> [test] acc: 0.741
--> [accuracy] finished 0.741
new state: tensor([576.,   4.,   4.,   3.,   3.], device='cuda:0')
new reward: 0.741
--> [reward] 0.741
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     4.0     |     3.0      |     3.0     | 0.741  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   4.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.634099221747855 acc: 0.6803
[Epoch 7] loss: 2.0975419279102168 acc: 0.735
[Epoch 11] loss: 0.6210175061174442 acc: 0.7386
[Epoch 15] loss: 0.29551740886543487 acc: 0.7423
[Epoch 19] loss: 0.2169201151132012 acc: 0.7315
[Epoch 23] loss: 0.1805432194705738 acc: 0.7308
[Epoch 27] loss: 0.1584315983545216 acc: 0.7412
[Epoch 31] loss: 0.13491928377105375 acc: 0.732
[Epoch 35] loss: 0.12837660387206032 acc: 0.7331
[Epoch 39] loss: 0.11265086383677012 acc: 0.7205
[Epoch 43] loss: 0.10990770785090373 acc: 0.7258
[Epoch 47] loss: 0.09915322661974474 acc: 0.7342
[Epoch 51] loss: 0.0928924212484118 acc: 0.7227
[Epoch 55] loss: 0.0878794494816257 acc: 0.7331
[Epoch 59] loss: 0.07795101212328086 acc: 0.7351
[Epoch 63] loss: 0.08632333772888173 acc: 0.7248
[Epoch 67] loss: 0.0737705435937323 acc: 0.7245
[Epoch 71] loss: 0.0789069974058977 acc: 0.7313
--> [test] acc: 0.7319
--> [accuracy] finished 0.7319
new state: tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7319
--> [reward] 0.7319
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     4.0     |     3.0      |     2.0     | 0.7319 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.560944992715441 acc: 0.6809
[Epoch 7] loss: 2.0533450616671303 acc: 0.7475
[Epoch 11] loss: 0.6328182504714831 acc: 0.7375
[Epoch 15] loss: 0.2931417767482493 acc: 0.7339
[Epoch 19] loss: 0.21899100947086617 acc: 0.733
[Epoch 23] loss: 0.1833278928833354 acc: 0.7208
[Epoch 27] loss: 0.16035485634451632 acc: 0.7194
[Epoch 31] loss: 0.1405430432061291 acc: 0.7379
[Epoch 35] loss: 0.1251165959769693 acc: 0.7347
[Epoch 39] loss: 0.11292671080669174 acc: 0.7257
[Epoch 43] loss: 0.10370537924194409 acc: 0.7335
[Epoch 47] loss: 0.10242776451942028 acc: 0.7237
[Epoch 51] loss: 0.09325666310232314 acc: 0.7288
[Epoch 55] loss: 0.08631275844408313 acc: 0.7298
[Epoch 59] loss: 0.08977858241304489 acc: 0.7254
[Epoch 63] loss: 0.07191629337904322 acc: 0.7357
[Epoch 67] loss: 0.07451394307212737 acc: 0.7288
[Epoch 71] loss: 0.07812190436802166 acc: 0.7233
--> [test] acc: 0.7184
--> [accuracy] finished 0.7184
new state: tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7184
--> [reward] 0.7184
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     4.0     |     3.0      |     2.0     | 0.7184 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([544.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.601696088490889 acc: 0.6961
[Epoch 7] loss: 2.0520454467943563 acc: 0.7443
[Epoch 11] loss: 0.614196887659028 acc: 0.7228
[Epoch 15] loss: 0.2886371792644224 acc: 0.731
[Epoch 19] loss: 0.2126521727765727 acc: 0.7264
[Epoch 23] loss: 0.17856211650430623 acc: 0.7351
[Epoch 27] loss: 0.15946654892524184 acc: 0.7375
[Epoch 31] loss: 0.13986666572978124 acc: 0.7233
[Epoch 35] loss: 0.12760439566369322 acc: 0.73
[Epoch 39] loss: 0.11501521820826527 acc: 0.7301
[Epoch 43] loss: 0.09824461249344031 acc: 0.7352
[Epoch 47] loss: 0.10041545114665747 acc: 0.7281
[Epoch 51] loss: 0.09462506438681709 acc: 0.7413
[Epoch 55] loss: 0.08384672179460868 acc: 0.734
[Epoch 59] loss: 0.08045864981048934 acc: 0.735
[Epoch 63] loss: 0.07750322070696851 acc: 0.7331
[Epoch 67] loss: 0.07656456263537299 acc: 0.7253
[Epoch 71] loss: 0.07763851146586477 acc: 0.7143
--> [test] acc: 0.7285
--> [accuracy] finished 0.7285
new state: tensor([544.,   4.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7285
--> [reward] 0.7285
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     4.0     |     3.0      |     2.0     | 0.7285 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0909, 0.0909, 0.0908, 0.0909, 0.0910, 0.0909, 0.0909, 0.0908,
         0.0910, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3979, -2.3979, -2.3980, -2.3979, -2.3978, -2.3979, -2.3979,
         -2.3980, -2.3978, -2.3979]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([544.,   5.,   4.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.381673050048711 acc: 0.6913
[Epoch 7] loss: 1.8407085911011147 acc: 0.7539
[Epoch 11] loss: 0.4978407429235861 acc: 0.7403
[Epoch 15] loss: 0.2643670494932577 acc: 0.7441
[Epoch 19] loss: 0.20233813711606405 acc: 0.7343
[Epoch 23] loss: 0.16508777702079558 acc: 0.7323
[Epoch 27] loss: 0.13662295448629524 acc: 0.7335
[Epoch 31] loss: 0.12729071704027675 acc: 0.734
[Epoch 35] loss: 0.10812950206210699 acc: 0.7374
[Epoch 39] loss: 0.11056260846774368 acc: 0.7358
[Epoch 43] loss: 0.09475541054902364 acc: 0.7269
[Epoch 47] loss: 0.09108649469588113 acc: 0.7391
[Epoch 51] loss: 0.08255280210914047 acc: 0.7309
[Epoch 55] loss: 0.08888586491162953 acc: 0.736
[Epoch 59] loss: 0.07373407602850872 acc: 0.7351
[Epoch 63] loss: 0.06956776483562034 acc: 0.7289
[Epoch 67] loss: 0.0788945442799817 acc: 0.735
[Epoch 71] loss: 0.06056994798645997 acc: 0.7376
--> [test] acc: 0.7356
--> [accuracy] finished 0.7356
new state: tensor([544.,   5.,   4.,   3.,   2.], device='cuda:0')
new reward: 0.7356
--> [reward] 0.7356
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2710]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5421]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7363]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7226]], device='cuda:0')
------ ------
delta_t: tensor([[0.7363]], device='cuda:0')
rewards[i]: 0.7356
values[i+1]: tensor([[-0.0131]], device='cuda:0')
values[i]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7363]], device='cuda:0')
delta_t: tensor([[0.7363]], device='cuda:0')
------ ------
policy_loss: 1.7415450811386108
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7363]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.3340]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.1259]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4580]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4439]], device='cuda:0')
------ ------
delta_t: tensor([[0.7291]], device='cuda:0')
rewards[i]: 0.7285
values[i+1]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0141]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4580]], device='cuda:0')
delta_t: tensor([[0.7291]], device='cuda:0')
------ ------
policy_loss: 5.213724613189697
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4580]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.6715]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.6750]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.1622]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.1479]], device='cuda:0')
------ ------
delta_t: tensor([[0.7187]], device='cuda:0')
rewards[i]: 0.7184
values[i+1]: tensor([[-0.0141]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0143]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.1622]], device='cuda:0')
delta_t: tensor([[0.7187]], device='cuda:0')
------ ------
policy_loss: 10.374532699584961
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.1622]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.7980]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.2530]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.8728]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.8583]], device='cuda:0')
------ ------
delta_t: tensor([[0.7322]], device='cuda:0')
rewards[i]: 0.7319
values[i+1]: tensor([[-0.0143]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.8728]], device='cuda:0')
delta_t: tensor([[0.7322]], device='cuda:0')
------ ------
policy_loss: 17.239408493041992
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.8728]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[14.2249]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[12.8538]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.5852]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.5707]], device='cuda:0')
------ ------
delta_t: tensor([[0.7411]], device='cuda:0')
rewards[i]: 0.741
values[i+1]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.5852]], device='cuda:0')
delta_t: tensor([[0.7411]], device='cuda:0')
------ ------
policy_loss: 25.812698364257812
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.5852]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[23.3028]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[18.1557]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.2610]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.2470]], device='cuda:0')
------ ------
delta_t: tensor([[0.7116]], device='cuda:0')
rewards[i]: 0.712
values[i+1]: tensor([[-0.0145]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0140]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.2610]], device='cuda:0')
delta_t: tensor([[0.7116]], device='cuda:0')
------ ------
policy_loss: 36.00589370727539
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.2610]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[35.4580]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.3106]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9306]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.9169]], device='cuda:0')
------ ------
delta_t: tensor([[0.7122]], device='cuda:0')
rewards[i]: 0.7124
values[i+1]: tensor([[-0.0140]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9306]], device='cuda:0')
delta_t: tensor([[0.7122]], device='cuda:0')
------ ------
policy_loss: 47.80474090576172
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9306]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[51.0836]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[31.2512]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.5903]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.5765]], device='cuda:0')
------ ------
delta_t: tensor([[0.7090]], device='cuda:0')
rewards[i]: 0.7087
values[i+1]: tensor([[-0.0136]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0138]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.5903]], device='cuda:0')
delta_t: tensor([[0.7090]], device='cuda:0')
------ ------
policy_loss: 61.18537139892578
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.5903]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[70.5161]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[38.8649]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.2342]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.2200]], device='cuda:0')
------ ------
delta_t: tensor([[0.6998]], device='cuda:0')
rewards[i]: 0.6993
values[i+1]: tensor([[-0.0138]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0142]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.2342]], device='cuda:0')
delta_t: tensor([[0.6998]], device='cuda:0')
------ ------
policy_loss: 76.11051177978516
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.2342]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[94.1984]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[47.3647]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.8822]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.8675]], device='cuda:0')
------ ------
delta_t: tensor([[0.7104]], device='cuda:0')
rewards[i]: 0.7097
values[i+1]: tensor([[-0.0142]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0147]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.8822]], device='cuda:0')
delta_t: tensor([[0.7104]], device='cuda:0')
------ ------
policy_loss: 92.58930206298828
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.8822]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[122.2586]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[56.1203]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.4913]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.4760]], device='cuda:0')
------ ------
delta_t: tensor([[0.6780]], device='cuda:0')
rewards[i]: 0.6772
values[i+1]: tensor([[-0.0147]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0153]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.4913]], device='cuda:0')
delta_t: tensor([[0.6780]], device='cuda:0')
------ ------
policy_loss: 110.52873992919922
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.4913]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[155.6381]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[66.7590]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.1706]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.1548]], device='cuda:0')
------ ------
delta_t: tensor([[0.7542]], device='cuda:0')
rewards[i]: 0.7535
values[i+1]: tensor([[-0.0153]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0159]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.1706]], device='cuda:0')
delta_t: tensor([[0.7542]], device='cuda:0')
------ ------
policy_loss: 130.09693908691406
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.1706]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[193.9302]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[76.5842]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.7512]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.7351]], device='cuda:0')
------ ------
delta_t: tensor([[0.6623]], device='cuda:0')
rewards[i]: 0.6619
values[i+1]: tensor([[-0.0159]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0161]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.7512]], device='cuda:0')
delta_t: tensor([[0.6623]], device='cuda:0')
------ ------
policy_loss: 151.0573272705078
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.7512]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[237.5952]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[87.3300]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.3451]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.3289]], device='cuda:0')
------ ------
delta_t: tensor([[0.6813]], device='cuda:0')
rewards[i]: 0.6811
values[i+1]: tensor([[-0.0161]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0162]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.3451]], device='cuda:0')
delta_t: tensor([[0.6813]], device='cuda:0')
------ ------
policy_loss: 173.44200134277344
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.3451]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[287.0695]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[98.9485]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.9473]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.9341]], device='cuda:0')
------ ------
delta_t: tensor([[0.6957]], device='cuda:0')
rewards[i]: 0.6985
values[i+1]: tensor([[-0.0162]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0132]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.9473]], device='cuda:0')
delta_t: tensor([[0.6957]], device='cuda:0')
------ ------
policy_loss: 197.27040100097656
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.9473]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 197.27040100097656
value_loss: 287.0694580078125
loss: 340.80511474609375



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 1.7115e-02,  1.0272e-04,  1.4669e-04,  7.3556e-05,  7.1589e-05],
        [-1.6462e-01, -9.8665e-04, -1.3858e-03, -7.1846e-04, -6.9394e-04],
        [ 2.1713e-05,  1.3429e-07,  1.1702e-07,  7.0385e-08,  9.4613e-08],
        [-2.9437e-01, -1.7641e-03, -2.4461e-03, -1.2975e-03, -1.2494e-03],
        [-1.1380e+00, -6.8203e-03, -9.5060e-03, -5.0105e-03, -4.8185e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-4.8263e-05, -1.5527e-04, -2.1363e-05,  1.4643e-04, -6.3138e-05],
        [ 5.0610e-04,  1.5599e-03,  2.1350e-04, -1.4782e-03,  6.3134e-04],
        [-8.7529e-08, -1.3393e-07, -1.5592e-08,  1.3879e-07, -4.6432e-08],
        [ 9.5505e-04,  2.8679e-03,  3.9122e-04, -2.7262e-03,  1.1574e-03],
        [ 3.5992e-03,  1.0841e-02,  1.4795e-03, -1.0304e-02,  4.3777e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.3642,  0.3408, -0.3626,  0.2794, -0.1596],
        [ 0.1974, -0.1847,  0.1965, -0.1514,  0.0865],
        [ 0.0801, -0.0750,  0.0797, -0.0615,  0.0352],
        [ 0.0165, -0.0155,  0.0165, -0.0127,  0.0072],
        [ 0.0702, -0.0657,  0.0699, -0.0538,  0.0306]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-0.0060, -0.0013,  0.0001,  0.0024,  0.0006],
        [ 0.0219,  0.0609,  0.0081, -0.0582,  0.0242],
        [-0.0140, -0.0406, -0.0055,  0.0388, -0.0163],
        [-0.0024, -0.0125, -0.0018,  0.0111, -0.0052],
        [-0.0051,  0.0004,  0.0003,  0.0017,  0.0007],
        [-0.0140, -0.0406, -0.0055,  0.0388, -0.0163],
        [ 0.0012,  0.0054,  0.0008, -0.0044,  0.0020],
        [ 0.0176,  0.0450,  0.0061, -0.0434,  0.0177],
        [-0.0061, -0.0216, -0.0030,  0.0200, -0.0088],
        [-0.0002, -0.0066, -0.0010,  0.0055, -0.0028],
        [ 0.0070,  0.0117,  0.0014, -0.0123,  0.0044]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 0.8456,  2.4571,  0.3337, -2.3463,  0.9884]], device='cuda:0')
--> [loss] 340.80511474609375

---------------------------------- [[#1 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     5.0      |     4.0     |     3.0      |     2.0     | 0.7356 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([544.,   5.,   4.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([544.,   5.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.623253391984174 acc: 0.6816
[Epoch 7] loss: 2.167883849776614 acc: 0.7259
[Epoch 11] loss: 0.6744432548951843 acc: 0.7191
[Epoch 15] loss: 0.32361157107002597 acc: 0.7235
[Epoch 19] loss: 0.23166847268067053 acc: 0.6896
[Epoch 23] loss: 0.19869927406939855 acc: 0.7142
[Epoch 27] loss: 0.16777310717155408 acc: 0.717
[Epoch 31] loss: 0.14092617989291467 acc: 0.7232
[Epoch 35] loss: 0.13345155116620824 acc: 0.7124
[Epoch 39] loss: 0.12561024058267206 acc: 0.7166
[Epoch 43] loss: 0.10807429483521235 acc: 0.7049
[Epoch 47] loss: 0.10836874138530525 acc: 0.7186
[Epoch 51] loss: 0.0899356231295868 acc: 0.7068
[Epoch 55] loss: 0.09884336699857889 acc: 0.7055
[Epoch 59] loss: 0.08778610208388561 acc: 0.7142
[Epoch 63] loss: 0.08497140812419612 acc: 0.7127
[Epoch 67] loss: 0.08126076229471628 acc: 0.7115
[Epoch 71] loss: 0.07303839912313907 acc: 0.7239
--> [test] acc: 0.7121
--> [accuracy] finished 0.7121
new state: tensor([544.,   5.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.7121
--> [reward] 0.7121
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     5.0      |     3.0     |     3.0      |     2.0     | 0.7121 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([544.,   5.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([544.,   5.,   2.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.577200945685892 acc: 0.6843
[Epoch 7] loss: 2.1160353033820076 acc: 0.7373
[Epoch 11] loss: 0.6694119223071944 acc: 0.735
[Epoch 15] loss: 0.3131303842944067 acc: 0.7467
[Epoch 19] loss: 0.23134880171626654 acc: 0.7464
[Epoch 23] loss: 0.19491590728835606 acc: 0.7448
[Epoch 27] loss: 0.16226112495040726 acc: 0.739
[Epoch 31] loss: 0.15445450343229733 acc: 0.7404
[Epoch 35] loss: 0.13448596435606175 acc: 0.739
[Epoch 39] loss: 0.12085152569297186 acc: 0.7482
[Epoch 43] loss: 0.123282295861341 acc: 0.7519
[Epoch 47] loss: 0.10969381391480469 acc: 0.7481
[Epoch 51] loss: 0.09834206217418776 acc: 0.731
[Epoch 55] loss: 0.09671043233036081 acc: 0.7485
[Epoch 59] loss: 0.10108398234702723 acc: 0.7516
[Epoch 63] loss: 0.07960556423180806 acc: 0.7468
[Epoch 67] loss: 0.09174281372325471 acc: 0.7355
[Epoch 71] loss: 0.08185996226501911 acc: 0.7471
--> [test] acc: 0.7422
--> [accuracy] finished 0.7422
new state: tensor([544.,   5.,   2.,   3.,   2.], device='cuda:0')
new reward: 0.7422
--> [reward] 0.7422
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     5.0      |     2.0     |     3.0      |     2.0     | 0.7422 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([544.,   5.,   2.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.737458702853269 acc: 0.6589
[Epoch 7] loss: 2.3010216050059595 acc: 0.7517
[Epoch 11] loss: 0.7860786244749566 acc: 0.7419
[Epoch 15] loss: 0.35744311903958276 acc: 0.74
[Epoch 19] loss: 0.26497560328401415 acc: 0.7401
[Epoch 23] loss: 0.19699233957349568 acc: 0.7442
[Epoch 27] loss: 0.18696118477026902 acc: 0.7463
[Epoch 31] loss: 0.17378621902245353 acc: 0.7353
[Epoch 35] loss: 0.1491567071424821 acc: 0.7348
[Epoch 39] loss: 0.13594619990052545 acc: 0.7238
[Epoch 43] loss: 0.12941549245989822 acc: 0.7332
[Epoch 47] loss: 0.1149515214601241 acc: 0.7475
[Epoch 51] loss: 0.11221356890247682 acc: 0.7305
[Epoch 55] loss: 0.1048854259652612 acc: 0.7467
[Epoch 59] loss: 0.10260053657625309 acc: 0.7429
[Epoch 63] loss: 0.09204788291302826 acc: 0.739
[Epoch 67] loss: 0.09721492136330784 acc: 0.7426
[Epoch 71] loss: 0.09366050575930707 acc: 0.7452
--> [test] acc: 0.7373
--> [accuracy] finished 0.7373
new state: tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
new reward: 0.7373
--> [reward] 0.7373
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     2.0     |     3.0      |     2.0     | 0.7373 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.667269360226438 acc: 0.6922
[Epoch 7] loss: 2.259588076292401 acc: 0.7401
[Epoch 11] loss: 0.7737545835430665 acc: 0.7516
[Epoch 15] loss: 0.357769076205085 acc: 0.7357
[Epoch 19] loss: 0.24926428696440767 acc: 0.7407
[Epoch 23] loss: 0.21285564885439012 acc: 0.738
[Epoch 27] loss: 0.19417922036088717 acc: 0.7267
[Epoch 31] loss: 0.16167339806199607 acc: 0.7402
[Epoch 35] loss: 0.14248282969464807 acc: 0.7325
[Epoch 39] loss: 0.1448056600449602 acc: 0.732
[Epoch 43] loss: 0.12962104414191927 acc: 0.7301
[Epoch 47] loss: 0.12205976453821754 acc: 0.7383
[Epoch 51] loss: 0.12200922293998205 acc: 0.7298
[Epoch 55] loss: 0.10103707709570017 acc: 0.7297
[Epoch 59] loss: 0.1128265164194681 acc: 0.7233
[Epoch 63] loss: 0.09598203023585676 acc: 0.732
[Epoch 67] loss: 0.09986278924750416 acc: 0.7275
[Epoch 71] loss: 0.08841293115559441 acc: 0.7337
--> [test] acc: 0.7427
--> [accuracy] finished 0.7427
new state: tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
new reward: 0.7427
--> [reward] 0.7427
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     2.0     |     3.0      |     2.0     | 0.7427 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   2.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([544.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.823553222981865 acc: 0.6701
[Epoch 7] loss: 2.4090204908491097 acc: 0.7143
[Epoch 11] loss: 0.8432403150898264 acc: 0.7172
[Epoch 15] loss: 0.3692647352185853 acc: 0.6992
[Epoch 19] loss: 0.2627135406122031 acc: 0.694
[Epoch 23] loss: 0.2156377204758165 acc: 0.702
[Epoch 27] loss: 0.18794930343994934 acc: 0.7047
[Epoch 31] loss: 0.17274996839806706 acc: 0.7018
[Epoch 35] loss: 0.1432357442337791 acc: 0.7119
[Epoch 39] loss: 0.14510867617014425 acc: 0.6956
[Epoch 43] loss: 0.11918618164651687 acc: 0.7136
[Epoch 47] loss: 0.12150879108282688 acc: 0.6987
[Epoch 51] loss: 0.11394862408089973 acc: 0.7097
[Epoch 55] loss: 0.1042577017198705 acc: 0.7089
[Epoch 59] loss: 0.09815369609414655 acc: 0.7031
[Epoch 63] loss: 0.0967068015776403 acc: 0.7032
[Epoch 67] loss: 0.09251725931754312 acc: 0.6973
[Epoch 71] loss: 0.08861828466598659 acc: 0.7069
--> [test] acc: 0.6963
--> [accuracy] finished 0.6963
new state: tensor([544.,   4.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.6963
--> [reward] 0.6963
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  544.0   |     4.0      |     3.0     |     3.0      |     2.0     | 0.6963 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([544.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([576.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.8095106330064255 acc: 0.632
[Epoch 7] loss: 2.4208538857338677 acc: 0.7064
[Epoch 11] loss: 0.8260098576469495 acc: 0.6993
[Epoch 15] loss: 0.37776353199730445 acc: 0.7103
[Epoch 19] loss: 0.256861031609242 acc: 0.7159
[Epoch 23] loss: 0.20344534222646365 acc: 0.7092
[Epoch 27] loss: 0.18477953927558097 acc: 0.7053
[Epoch 31] loss: 0.16877250708258518 acc: 0.7069
[Epoch 35] loss: 0.142358877316184 acc: 0.7042
[Epoch 39] loss: 0.13374948487771898 acc: 0.701
[Epoch 43] loss: 0.12787684202884964 acc: 0.699
[Epoch 47] loss: 0.11590846946857193 acc: 0.7095
[Epoch 51] loss: 0.10349943144510433 acc: 0.7038
[Epoch 55] loss: 0.10611568216610785 acc: 0.7098
[Epoch 59] loss: 0.0965228462983292 acc: 0.7048
[Epoch 63] loss: 0.0966722337014573 acc: 0.7018
[Epoch 67] loss: 0.08881377423876007 acc: 0.7038
[Epoch 71] loss: 0.08558922693746097 acc: 0.7039
--> [test] acc: 0.694
--> [accuracy] finished 0.694
new state: tensor([576.,   4.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.694
--> [reward] 0.694
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     4.0      |     3.0     |     3.0      |     2.0     | 0.694  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([576.,   4.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([576.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.090855254998902 acc: 0.6326
[Epoch 7] loss: 2.8159675255151053 acc: 0.6787
[Epoch 11] loss: 1.1327215277344522 acc: 0.701
[Epoch 15] loss: 0.4804656084512582 acc: 0.6801
[Epoch 19] loss: 0.3217533864128544 acc: 0.6925
[Epoch 23] loss: 0.2509488267371493 acc: 0.6854
[Epoch 27] loss: 0.21685291712870225 acc: 0.6851
[Epoch 31] loss: 0.19003098395407733 acc: 0.6855
[Epoch 35] loss: 0.17167779875923986 acc: 0.6813
[Epoch 39] loss: 0.15937181365202227 acc: 0.6795
[Epoch 43] loss: 0.15476738550769323 acc: 0.682
[Epoch 47] loss: 0.14143986891135765 acc: 0.6817
[Epoch 51] loss: 0.12699943739692193 acc: 0.6789
[Epoch 55] loss: 0.12256017809047762 acc: 0.6828
[Epoch 59] loss: 0.1155791674940692 acc: 0.6775
[Epoch 63] loss: 0.10770297153731404 acc: 0.6804
[Epoch 67] loss: 0.10311778507355 acc: 0.6719
[Epoch 71] loss: 0.1031392978739066 acc: 0.6817
--> [test] acc: 0.681
--> [accuracy] finished 0.681
new state: tensor([576.,   3.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.681
--> [reward] 0.681
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     3.0      |     3.0     |     3.0      |     2.0     | 0.681  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0910, 0.0908, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3978, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.134800087155589 acc: 0.6422
[Epoch 7] loss: 2.848535491225055 acc: 0.6985
[Epoch 11] loss: 1.1380004625757942 acc: 0.6836
[Epoch 15] loss: 0.49090894043464645 acc: 0.6807
[Epoch 19] loss: 0.3127564304720258 acc: 0.6846
[Epoch 23] loss: 0.25254589006009387 acc: 0.6796
[Epoch 27] loss: 0.21530102187400812 acc: 0.6771
[Epoch 31] loss: 0.19331587263194802 acc: 0.6854
[Epoch 35] loss: 0.17655917998317563 acc: 0.6894
[Epoch 39] loss: 0.1575582971700522 acc: 0.6819
[Epoch 43] loss: 0.15281330245782804 acc: 0.6868
[Epoch 47] loss: 0.13899916961885597 acc: 0.6898
[Epoch 51] loss: 0.13139786332955256 acc: 0.6752
[Epoch 55] loss: 0.11288564226320942 acc: 0.676
[Epoch 59] loss: 0.12224401262424447 acc: 0.6845
[Epoch 63] loss: 0.10663897168520085 acc: 0.6846
[Epoch 67] loss: 0.1096384418744813 acc: 0.6853
[Epoch 71] loss: 0.10059545589420382 acc: 0.6829
--> [test] acc: 0.688
--> [accuracy] finished 0.688
new state: tensor([608.,   3.,   3.,   3.,   2.], device='cuda:0')
new reward: 0.688
--> [reward] 0.688
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     2.0     | 0.688  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   2.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.049155171874844 acc: 0.6543
[Epoch 7] loss: 2.715934088086838 acc: 0.7023
[Epoch 11] loss: 1.058386977333242 acc: 0.6914
[Epoch 15] loss: 0.45172648640144664 acc: 0.7034
[Epoch 19] loss: 0.30605823065980775 acc: 0.708
[Epoch 23] loss: 0.23886242818535136 acc: 0.7019
[Epoch 27] loss: 0.20496251722297554 acc: 0.6999
[Epoch 31] loss: 0.18139920105485965 acc: 0.7017
[Epoch 35] loss: 0.16942394936166685 acc: 0.7016
[Epoch 39] loss: 0.1495476652799017 acc: 0.701
[Epoch 43] loss: 0.14125214529026042 acc: 0.6939
[Epoch 47] loss: 0.12802638712665423 acc: 0.7063
[Epoch 51] loss: 0.11808906233919512 acc: 0.6962
[Epoch 55] loss: 0.11481098567261873 acc: 0.6946
[Epoch 59] loss: 0.10666307975354074 acc: 0.6996
[Epoch 63] loss: 0.10663048756089123 acc: 0.6983
[Epoch 67] loss: 0.09119830146843395 acc: 0.6984
[Epoch 71] loss: 0.09957658085862503 acc: 0.7069
--> [test] acc: 0.7054
--> [accuracy] finished 0.7054
new state: tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
new reward: 0.7054
--> [reward] 0.7054
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     3.0     | 0.7054 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.040465042109379 acc: 0.6432
[Epoch 7] loss: 2.687653335540191 acc: 0.704
[Epoch 11] loss: 1.0438731361247238 acc: 0.7039
[Epoch 15] loss: 0.45540374006285234 acc: 0.711
[Epoch 19] loss: 0.2969255605593438 acc: 0.7087
[Epoch 23] loss: 0.24764724868252072 acc: 0.7088
[Epoch 27] loss: 0.20750979355076696 acc: 0.6937
[Epoch 31] loss: 0.18405744726138423 acc: 0.6971
[Epoch 35] loss: 0.16881823426593676 acc: 0.7002
[Epoch 39] loss: 0.15205544721135092 acc: 0.6955
[Epoch 43] loss: 0.14672227997613876 acc: 0.6979
[Epoch 47] loss: 0.13225224182305054 acc: 0.6999
[Epoch 51] loss: 0.12274608169170216 acc: 0.6953
[Epoch 55] loss: 0.11676207150730407 acc: 0.6954
[Epoch 59] loss: 0.10684343561163301 acc: 0.692
[Epoch 63] loss: 0.10738744379808444 acc: 0.7001
[Epoch 67] loss: 0.10205290710215298 acc: 0.6998
[Epoch 71] loss: 0.09622557061519521 acc: 0.6982
--> [test] acc: 0.699
--> [accuracy] finished 0.699
new state: tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
new reward: 0.699
--> [reward] 0.699
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     3.0     | 0.699  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.052235543727875 acc: 0.6379
[Epoch 7] loss: 2.7047894933187138 acc: 0.6969
[Epoch 11] loss: 1.104557700664796 acc: 0.7127
[Epoch 15] loss: 0.46478188724335656 acc: 0.7001
[Epoch 19] loss: 0.30278291706653204 acc: 0.7113
[Epoch 23] loss: 0.24043406614952761 acc: 0.6888
[Epoch 27] loss: 0.2160142981935569 acc: 0.7036
[Epoch 31] loss: 0.17898436763640635 acc: 0.7066
[Epoch 35] loss: 0.1683375787804537 acc: 0.6926
[Epoch 39] loss: 0.15440849926980102 acc: 0.6883
[Epoch 43] loss: 0.13932377582444522 acc: 0.7054
[Epoch 47] loss: 0.13174776865325422 acc: 0.687
[Epoch 51] loss: 0.11987506584300066 acc: 0.6996
[Epoch 55] loss: 0.11870208566250932 acc: 0.7108
[Epoch 59] loss: 0.11455364591654038 acc: 0.6894
[Epoch 63] loss: 0.10184747837912625 acc: 0.6929
[Epoch 67] loss: 0.10675257239300195 acc: 0.6928
[Epoch 71] loss: 0.10100049076154066 acc: 0.7026
--> [test] acc: 0.6985
--> [accuracy] finished 0.6985
new state: tensor([608.,   3.,   3.,   3.,   4.], device='cuda:0')
new reward: 0.6985
--> [reward] 0.6985
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     4.0     | 0.6985 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0908, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.1560194995397195 acc: 0.6347
[Epoch 7] loss: 2.8364138447720073 acc: 0.6909
[Epoch 11] loss: 1.207516530018938 acc: 0.6872
[Epoch 15] loss: 0.5259035167420079 acc: 0.6877
[Epoch 19] loss: 0.34380037577875205 acc: 0.684
[Epoch 23] loss: 0.2545899231310772 acc: 0.6635
[Epoch 27] loss: 0.23373293465055772 acc: 0.6723
[Epoch 31] loss: 0.19996250552051437 acc: 0.6768
[Epoch 35] loss: 0.1819750793240107 acc: 0.6864
[Epoch 39] loss: 0.1715490894435007 acc: 0.6867
[Epoch 43] loss: 0.1496296281047413 acc: 0.6795
[Epoch 47] loss: 0.14217295987999587 acc: 0.684
[Epoch 51] loss: 0.1260603668169378 acc: 0.6787
[Epoch 55] loss: 0.1325109853097202 acc: 0.6828
[Epoch 59] loss: 0.12143613401171573 acc: 0.6852
[Epoch 63] loss: 0.11275172922188592 acc: 0.6811
[Epoch 67] loss: 0.10707437909444047 acc: 0.676
[Epoch 71] loss: 0.1051780029612086 acc: 0.6823
--> [test] acc: 0.6688
--> [accuracy] finished 0.6688
new state: tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6688
--> [reward] 0.6688
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     5.0     | 0.6688 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.173487865406534 acc: 0.6384
[Epoch 7] loss: 2.8654475922474774 acc: 0.6692
[Epoch 11] loss: 1.2443761532302098 acc: 0.68
[Epoch 15] loss: 0.5395238570954717 acc: 0.6957
[Epoch 19] loss: 0.3409828495096101 acc: 0.6846
[Epoch 23] loss: 0.27028293547261023 acc: 0.6927
[Epoch 27] loss: 0.2319030509832913 acc: 0.6884
[Epoch 31] loss: 0.19984046636325548 acc: 0.6911
[Epoch 35] loss: 0.18190167089233467 acc: 0.6892
[Epoch 39] loss: 0.16397581949039264 acc: 0.685
[Epoch 43] loss: 0.16581190356036737 acc: 0.6823
[Epoch 47] loss: 0.13672074950907542 acc: 0.6931
[Epoch 51] loss: 0.13367498726667384 acc: 0.6812
[Epoch 55] loss: 0.13054771969199677 acc: 0.6844
[Epoch 59] loss: 0.1226138729778359 acc: 0.6816
[Epoch 63] loss: 0.1114745893220291 acc: 0.6827
[Epoch 67] loss: 0.1052486116538191 acc: 0.6835
[Epoch 71] loss: 0.09805398269573136 acc: 0.688
--> [test] acc: 0.6856
--> [accuracy] finished 0.6856
new state: tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6856
--> [reward] 0.6856
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     3.0      |     3.0     |     3.0      |     5.0     | 0.6856 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([608.,   3.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.903191180485289 acc: 0.6559
[Epoch 7] loss: 2.4930776023041563 acc: 0.713
[Epoch 11] loss: 0.9089423707112327 acc: 0.7174
[Epoch 15] loss: 0.3932493316090625 acc: 0.7154
[Epoch 19] loss: 0.26540324133356363 acc: 0.6967
[Epoch 23] loss: 0.21956774171518015 acc: 0.7042
[Epoch 27] loss: 0.19421328403189053 acc: 0.7124
[Epoch 31] loss: 0.166292085569552 acc: 0.7111
[Epoch 35] loss: 0.1419611016378912 acc: 0.6978
[Epoch 39] loss: 0.14131196418686595 acc: 0.7036
[Epoch 43] loss: 0.11979788286811517 acc: 0.7018
[Epoch 47] loss: 0.12394611928564356 acc: 0.7114
[Epoch 51] loss: 0.11112024797522047 acc: 0.7038
[Epoch 55] loss: 0.10409327210320155 acc: 0.7003
[Epoch 59] loss: 0.0948620399542372 acc: 0.7048
[Epoch 63] loss: 0.09450169664788563 acc: 0.7008
[Epoch 67] loss: 0.08746454220466421 acc: 0.7091
[Epoch 71] loss: 0.0900997340552928 acc: 0.7031
--> [test] acc: 0.7127
--> [accuracy] finished 0.7127
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.7127
--> [reward] 0.7127
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.7127 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0911, 0.0908, 0.0907, 0.0909, 0.0909, 0.0910, 0.0910, 0.0907,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3977, -2.3980, -2.3981, -2.3979, -2.3980, -2.3978, -2.3978,
         -2.3981, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.916472550700693 acc: 0.6546
[Epoch 7] loss: 2.5041381962921307 acc: 0.7167
[Epoch 11] loss: 0.9263691858047872 acc: 0.7129
[Epoch 15] loss: 0.40238266120976807 acc: 0.7173
[Epoch 19] loss: 0.26167509236785075 acc: 0.7055
[Epoch 23] loss: 0.22320955302899756 acc: 0.7054
[Epoch 27] loss: 0.19382361491518024 acc: 0.6978
[Epoch 31] loss: 0.1686417741934433 acc: 0.6983
[Epoch 35] loss: 0.1505162997261795 acc: 0.6981
[Epoch 39] loss: 0.1314912364625222 acc: 0.704
[Epoch 43] loss: 0.12464175914245113 acc: 0.7054
[Epoch 47] loss: 0.11420705241491766 acc: 0.7037
[Epoch 51] loss: 0.12489552547990837 acc: 0.707
[Epoch 55] loss: 0.09919289017305769 acc: 0.6992
[Epoch 59] loss: 0.09985545180056749 acc: 0.69
[Epoch 63] loss: 0.09184285645376739 acc: 0.7004
[Epoch 67] loss: 0.10044697944320205 acc: 0.7063
[Epoch 71] loss: 0.08070148996975454 acc: 0.7016
--> [test] acc: 0.6925
--> [accuracy] finished 0.6925
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6925
--> [reward] 0.6925
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2399]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4798]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6927]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6830]], device='cuda:0')
------ ------
delta_t: tensor([[0.6927]], device='cuda:0')
rewards[i]: 0.6925
values[i+1]: tensor([[-0.0096]], device='cuda:0')
values[i]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6927]], device='cuda:0')
delta_t: tensor([[0.6927]], device='cuda:0')
------ ------
policy_loss: 1.636962652206421
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6927]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.2181]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.9565]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.3987]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.3889]], device='cuda:0')
------ ------
delta_t: tensor([[0.7130]], device='cuda:0')
rewards[i]: 0.7127
values[i+1]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.3987]], device='cuda:0')
delta_t: tensor([[0.7130]], device='cuda:0')
------ ------
policy_loss: 4.967307090759277
log_probs[i]: tensor([[-2.3981]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.3987]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.3615]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.2867]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.0704]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.0606]], device='cuda:0')
------ ------
delta_t: tensor([[0.6857]], device='cuda:0')
rewards[i]: 0.6856
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.0704]], device='cuda:0')
delta_t: tensor([[0.6857]], device='cuda:0')
------ ------
policy_loss: 9.908119201660156
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.0704]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[7.0568]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.3907]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.7186]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.7088]], device='cuda:0')
------ ------
delta_t: tensor([[0.6689]], device='cuda:0')
rewards[i]: 0.6688
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.7186]], device='cuda:0')
delta_t: tensor([[0.6689]], device='cuda:0')
------ ------
policy_loss: 16.403175354003906
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.7186]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[12.8028]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[11.4920]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3900]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3802]], device='cuda:0')
------ ------
delta_t: tensor([[0.6986]], device='cuda:0')
rewards[i]: 0.6985
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3900]], device='cuda:0')
delta_t: tensor([[0.6986]], device='cuda:0')
------ ------
policy_loss: 24.508230209350586
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3900]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[21.0250]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.4443]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.0552]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.0454]], device='cuda:0')
------ ------
delta_t: tensor([[0.6991]], device='cuda:0')
rewards[i]: 0.699
values[i+1]: tensor([[-0.0098]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.0552]], device='cuda:0')
delta_t: tensor([[0.6991]], device='cuda:0')
------ ------
policy_loss: 34.20783996582031
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.0552]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[32.1634]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.2768]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7198]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7104]], device='cuda:0')
------ ------
delta_t: tensor([[0.7052]], device='cuda:0')
rewards[i]: 0.7054
values[i+1]: tensor([[-0.0097]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0095]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7198]], device='cuda:0')
delta_t: tensor([[0.7052]], device='cuda:0')
------ ------
policy_loss: 45.50181198120117
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7198]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[46.5295]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[28.7323]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.3603]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.3513]], device='cuda:0')
------ ------
delta_t: tensor([[0.6876]], device='cuda:0')
rewards[i]: 0.688
values[i+1]: tensor([[-0.0095]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0090]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.3603]], device='cuda:0')
delta_t: tensor([[0.6876]], device='cuda:0')
------ ------
policy_loss: 58.33038330078125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.3603]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[64.4533]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[35.8475]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.9873]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.9787]], device='cuda:0')
------ ------
delta_t: tensor([[0.6806]], device='cuda:0')
rewards[i]: 0.681
values[i+1]: tensor([[-0.0090]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.9873]], device='cuda:0')
delta_t: tensor([[0.6806]], device='cuda:0')
------ ------
policy_loss: 72.66387939453125
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.9873]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[86.3727]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[43.8387]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.6211]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.6130]], device='cuda:0')
------ ------
delta_t: tensor([[0.6937]], device='cuda:0')
rewards[i]: 0.694
values[i+1]: tensor([[-0.0085]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.6211]], device='cuda:0')
delta_t: tensor([[0.6937]], device='cuda:0')
------ ------
policy_loss: 88.51560974121094
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.6211]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[112.6623]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[52.5793]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.2512]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.2431]], device='cuda:0')
------ ------
delta_t: tensor([[0.6963]], device='cuda:0')
rewards[i]: 0.6963
values[i+1]: tensor([[-0.0081]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.2512]], device='cuda:0')
delta_t: tensor([[0.6963]], device='cuda:0')
------ ------
policy_loss: 105.87959289550781
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.2512]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[144.0381]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[62.7516]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9216]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.9134]], device='cuda:0')
------ ------
delta_t: tensor([[0.7429]], device='cuda:0')
rewards[i]: 0.7427
values[i+1]: tensor([[-0.0080]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0082]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9216]], device='cuda:0')
delta_t: tensor([[0.7429]], device='cuda:0')
------ ------
policy_loss: 124.85025787353516
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9216]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[180.8464]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[73.6164]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.5800]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.5716]], device='cuda:0')
------ ------
delta_t: tensor([[0.7376]], device='cuda:0')
rewards[i]: 0.7373
values[i+1]: tensor([[-0.0082]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.5800]], device='cuda:0')
delta_t: tensor([[0.7376]], device='cuda:0')
------ ------
policy_loss: 145.40109252929688
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.5800]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[223.5100]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.3274]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2373]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2280]], device='cuda:0')
------ ------
delta_t: tensor([[0.7431]], device='cuda:0')
rewards[i]: 0.7422
values[i+1]: tensor([[-0.0084]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0092]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2373]], device='cuda:0')
delta_t: tensor([[0.7431]], device='cuda:0')
------ ------
policy_loss: 167.52676391601562
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2373]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[272.1029]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[97.1857]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8583]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.8479]], device='cuda:0')
------ ------
delta_t: tensor([[0.7134]], device='cuda:0')
rewards[i]: 0.7121
values[i+1]: tensor([[-0.0092]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0104]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8583]], device='cuda:0')
delta_t: tensor([[0.7134]], device='cuda:0')
------ ------
policy_loss: 191.14149475097656
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8583]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 191.14149475097656
value_loss: 272.1028747558594
loss: 327.19293212890625



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 5.1277e-03,  4.0432e-05,  2.1811e-05,  2.7761e-05,  1.9369e-05],
        [-8.6425e-02, -6.5611e-04, -3.9647e-04, -4.6474e-04, -3.3777e-04],
        [-7.1240e-04, -5.2449e-06, -4.9855e-06, -3.8387e-06, -2.7927e-06],
        [-1.7202e-01, -1.2782e-03, -7.3729e-04, -9.1982e-04, -6.9242e-04],
        [-6.3560e-01, -4.6722e-03, -2.8055e-03, -3.3912e-03, -2.5589e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-3.1620e-05, -6.5399e-05, -6.6505e-06,  6.5523e-05, -2.7463e-05],
        [ 5.2179e-04,  1.0979e-03,  1.1515e-04, -1.0976e-03,  4.5900e-04],
        [ 3.8537e-06,  9.1698e-06,  1.1890e-06, -9.0480e-06,  3.6288e-06],
        [ 1.0483e-03,  2.1740e-03,  2.2063e-04, -2.1766e-03,  9.1719e-04],
        [ 3.8570e-03,  8.0282e-03,  8.2110e-04, -8.0359e-03,  3.3841e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.2050,  0.1949, -0.2044,  0.1633, -0.0997],
        [ 0.1478, -0.1404,  0.1474, -0.1176,  0.0715],
        [ 0.0185, -0.0176,  0.0184, -0.0149,  0.0093],
        [ 0.0112, -0.0106,  0.0111, -0.0089,  0.0054],
        [ 0.0275, -0.0262,  0.0274, -0.0219,  0.0134]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[-2.2004e-02, -4.6924e-02, -4.9845e-03,  4.6777e-02, -1.9664e-02],
        [ 1.4003e-02,  2.9313e-02,  2.7257e-03, -2.9433e-02,  1.2829e-02],
        [ 2.1847e-02,  4.5664e-02,  4.4767e-03, -4.5524e-02,  1.9101e-02],
        [-1.7432e-02, -3.7127e-02, -3.9262e-03,  3.7054e-02, -1.5564e-02],
        [ 3.3843e-02,  7.5096e-02,  8.9016e-03, -7.4437e-02,  2.9774e-02],
        [-6.9855e-05, -1.1804e-03, -3.9035e-04,  1.0385e-03, -2.7850e-04],
        [-2.2005e-02, -4.6927e-02, -4.9848e-03,  4.6780e-02, -1.9666e-02],
        [-2.2000e-02, -4.6916e-02, -4.9836e-03,  4.6769e-02, -1.9661e-02],
        [-2.1925e-02, -4.6757e-02, -4.9667e-03,  4.6611e-02, -1.9594e-02],
        [ 1.8824e-02,  4.0439e-02,  4.5231e-03, -4.0334e-02,  1.7606e-02],
        [ 1.6919e-02,  3.5322e-02,  3.6090e-03, -3.5302e-02,  1.5116e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.3293,  2.8348,  0.3011, -2.8259,  1.1879]], device='cuda:0')
--> [loss] 327.19293212890625

---------------------------------- [[#2 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.6925 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.910923041651011 acc: 0.6331
[Epoch 7] loss: 2.4901979766843265 acc: 0.714
[Epoch 11] loss: 0.8798032390939838 acc: 0.7211
[Epoch 15] loss: 0.3852142846006948 acc: 0.706
[Epoch 19] loss: 0.26360457023019757 acc: 0.7123
[Epoch 23] loss: 0.21872549665534435 acc: 0.7069
[Epoch 27] loss: 0.18182729639332085 acc: 0.6979
[Epoch 31] loss: 0.16262847643650477 acc: 0.6986
[Epoch 35] loss: 0.1492084488105934 acc: 0.7033
[Epoch 39] loss: 0.13241085583996742 acc: 0.706
[Epoch 43] loss: 0.12482051880103644 acc: 0.702
[Epoch 47] loss: 0.11919004334460782 acc: 0.6988
[Epoch 51] loss: 0.11134227451241915 acc: 0.6998
[Epoch 55] loss: 0.10173241075073533 acc: 0.7045
[Epoch 59] loss: 0.09908448990501101 acc: 0.7075
[Epoch 63] loss: 0.08548584523776909 acc: 0.7034
[Epoch 67] loss: 0.09492555528502826 acc: 0.7015
[Epoch 71] loss: 0.0818988779666768 acc: 0.7048
--> [test] acc: 0.7117
--> [accuracy] finished 0.7117
new state: tensor([640.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.7117
--> [reward] 0.7117
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.7117 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.942778237487959 acc: 0.6588
[Epoch 7] loss: 2.525048919574684 acc: 0.7087
[Epoch 11] loss: 0.9175125216812734 acc: 0.7129
[Epoch 15] loss: 0.40236706213782664 acc: 0.6987
[Epoch 19] loss: 0.27642017619832016 acc: 0.7063
[Epoch 23] loss: 0.21603797750352213 acc: 0.6914
[Epoch 27] loss: 0.1839052123039046 acc: 0.707
[Epoch 31] loss: 0.16431166796856905 acc: 0.7055
[Epoch 35] loss: 0.15219329148435684 acc: 0.6992
[Epoch 39] loss: 0.13694276097718902 acc: 0.7053
[Epoch 43] loss: 0.13330010478110874 acc: 0.6998
[Epoch 47] loss: 0.11278866163319182 acc: 0.7056
[Epoch 51] loss: 0.10829958916329743 acc: 0.6962
[Epoch 55] loss: 0.11169031376565171 acc: 0.7083
[Epoch 59] loss: 0.08672821825570272 acc: 0.7042
[Epoch 63] loss: 0.10545915358668893 acc: 0.696
[Epoch 67] loss: 0.08710526182647328 acc: 0.7039
[Epoch 71] loss: 0.08581497232809894 acc: 0.6992
--> [test] acc: 0.6978
--> [accuracy] finished 0.6978
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.6978
--> [reward] 0.6978
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.6978 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.89426069521843 acc: 0.669
[Epoch 7] loss: 2.464545570485427 acc: 0.7111
[Epoch 11] loss: 0.8978424400872435 acc: 0.6994
[Epoch 15] loss: 0.3797933064625048 acc: 0.7072
[Epoch 19] loss: 0.26208551730150764 acc: 0.7136
[Epoch 23] loss: 0.21461335701339157 acc: 0.7024
[Epoch 27] loss: 0.1801034196963548 acc: 0.7004
[Epoch 31] loss: 0.16707418506960278 acc: 0.6903
[Epoch 35] loss: 0.14479365608538203 acc: 0.7133
[Epoch 39] loss: 0.1424528881413219 acc: 0.7042
[Epoch 43] loss: 0.12554169970247753 acc: 0.7062
[Epoch 47] loss: 0.11695597131434075 acc: 0.6999
[Epoch 51] loss: 0.10425141441118915 acc: 0.7046
[Epoch 55] loss: 0.1074363694173734 acc: 0.6934
[Epoch 59] loss: 0.09168820689275117 acc: 0.6981
[Epoch 63] loss: 0.10061730401244495 acc: 0.6991
[Epoch 67] loss: 0.09080926735696081 acc: 0.7047
[Epoch 71] loss: 0.0919917626523763 acc: 0.701
--> [test] acc: 0.7029
--> [accuracy] finished 0.7029
new state: tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
new reward: 0.7029
--> [reward] 0.7029
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     3.0      |     5.0     | 0.7029 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   3.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([608.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.006405542421219 acc: 0.6548
[Epoch 7] loss: 2.601421848206264 acc: 0.6986
[Epoch 11] loss: 0.9413623748837835 acc: 0.6808
[Epoch 15] loss: 0.41127906815456156 acc: 0.7
[Epoch 19] loss: 0.2737216785159486 acc: 0.6914
[Epoch 23] loss: 0.22639093028567256 acc: 0.6892
[Epoch 27] loss: 0.19708839364711891 acc: 0.681
[Epoch 31] loss: 0.17321644484510887 acc: 0.6849
[Epoch 35] loss: 0.1549679981806623 acc: 0.685
[Epoch 39] loss: 0.14346087369782007 acc: 0.6853
[Epoch 43] loss: 0.13543189356051138 acc: 0.6856
[Epoch 47] loss: 0.1270765839632877 acc: 0.6748
[Epoch 51] loss: 0.11015129120081968 acc: 0.6905
[Epoch 55] loss: 0.10749039397803624 acc: 0.6863
[Epoch 59] loss: 0.0992502116501722 acc: 0.6885
[Epoch 63] loss: 0.10395286406967681 acc: 0.6889
[Epoch 67] loss: 0.09692261408111724 acc: 0.698
[Epoch 71] loss: 0.08589272270612705 acc: 0.6886
--> [test] acc: 0.6896
--> [accuracy] finished 0.6896
new state: tensor([608.,   4.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6896
--> [reward] 0.6896
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     4.0      |     3.0     |     2.0      |     5.0     | 0.6896 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.033112505360332 acc: 0.6366
[Epoch 7] loss: 2.624532995474003 acc: 0.6843
[Epoch 11] loss: 0.9480109911154756 acc: 0.6913
[Epoch 15] loss: 0.41399304416326 acc: 0.6956
[Epoch 19] loss: 0.2748765391357186 acc: 0.6929
[Epoch 23] loss: 0.22195488398851793 acc: 0.6945
[Epoch 27] loss: 0.19720605120081883 acc: 0.6919
[Epoch 31] loss: 0.17016106836326286 acc: 0.6999
[Epoch 35] loss: 0.15347412784281364 acc: 0.6903
[Epoch 39] loss: 0.14713671442140322 acc: 0.6946
[Epoch 43] loss: 0.12520105634128814 acc: 0.6879
[Epoch 47] loss: 0.12655316050062576 acc: 0.6787
[Epoch 51] loss: 0.10712904203980875 acc: 0.6747
[Epoch 55] loss: 0.11445088803653827 acc: 0.6962
[Epoch 59] loss: 0.10707111650353769 acc: 0.6897
[Epoch 63] loss: 0.09802170877423509 acc: 0.6906
[Epoch 67] loss: 0.09378417364834948 acc: 0.6836
[Epoch 71] loss: 0.08407085891598669 acc: 0.6894
--> [test] acc: 0.6737
--> [accuracy] finished 0.6737
new state: tensor([640.,   4.,   3.,   2.,   5.], device='cuda:0')
new reward: 0.6737
--> [reward] 0.6737
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     2.0      |     5.0     | 0.6737 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   2.,   5.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.664242903442334 acc: 0.6656
[Epoch 7] loss: 2.2405215315806593 acc: 0.7071
[Epoch 11] loss: 0.7367421912453364 acc: 0.726
[Epoch 15] loss: 0.3474451372271304 acc: 0.7159
[Epoch 19] loss: 0.244424051274081 acc: 0.7218
[Epoch 23] loss: 0.19787132095240648 acc: 0.7203
[Epoch 27] loss: 0.1681378479913601 acc: 0.7183
[Epoch 31] loss: 0.15483399222383415 acc: 0.6983
[Epoch 35] loss: 0.13945287063508235 acc: 0.7162
[Epoch 39] loss: 0.12884573087922732 acc: 0.7255
[Epoch 43] loss: 0.12102984181126518 acc: 0.7114
[Epoch 47] loss: 0.10459322045120123 acc: 0.7161
[Epoch 51] loss: 0.10999483959463513 acc: 0.7188
[Epoch 55] loss: 0.09307732988301369 acc: 0.7188
[Epoch 59] loss: 0.0951423319390096 acc: 0.7134
[Epoch 63] loss: 0.0910408998514428 acc: 0.7172
[Epoch 67] loss: 0.08940209108514978 acc: 0.7183
[Epoch 71] loss: 0.07964866595935849 acc: 0.7178
--> [test] acc: 0.7155
--> [accuracy] finished 0.7155
new state: tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
new reward: 0.7155
--> [reward] 0.7155
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     1.0      |     5.0     | 0.7155 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.693694481306978 acc: 0.674
[Epoch 7] loss: 2.3127777090158 acc: 0.73
[Epoch 11] loss: 0.7847063048597416 acc: 0.7239
[Epoch 15] loss: 0.36027000273299187 acc: 0.7221
[Epoch 19] loss: 0.25478984367178603 acc: 0.7208
[Epoch 23] loss: 0.2005653611177107 acc: 0.7257
[Epoch 27] loss: 0.17828993823812783 acc: 0.7251
[Epoch 31] loss: 0.16058731965406242 acc: 0.7208
[Epoch 35] loss: 0.1416371382387055 acc: 0.7195
[Epoch 39] loss: 0.13350457251857956 acc: 0.7257
[Epoch 43] loss: 0.11864736217998273 acc: 0.7142
[Epoch 47] loss: 0.11453757242506842 acc: 0.723
[Epoch 51] loss: 0.10873714900474943 acc: 0.7167
[Epoch 55] loss: 0.09490299143273469 acc: 0.7131
[Epoch 59] loss: 0.09999288158590103 acc: 0.716
[Epoch 63] loss: 0.09865242276755173 acc: 0.7226
[Epoch 67] loss: 0.08492140808438077 acc: 0.7232
[Epoch 71] loss: 0.08533703155525009 acc: 0.7194
--> [test] acc: 0.7081
--> [accuracy] finished 0.7081
new state: tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
new reward: 0.7081
--> [reward] 0.7081
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     4.0      |     3.0     |     1.0      |     5.0     | 0.7081 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   4.,   3.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   3.,   3.,   1.,   5.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.671944728592777 acc: 0.6811
[Epoch 7] loss: 2.288410707050577 acc: 0.7382
[Epoch 11] loss: 0.7997445251811724 acc: 0.7319
[Epoch 15] loss: 0.36273262216745283 acc: 0.7311
[Epoch 19] loss: 0.2560878086863729 acc: 0.7373
[Epoch 23] loss: 0.20428098219415874 acc: 0.7341
[Epoch 27] loss: 0.1784991645337561 acc: 0.7371
[Epoch 31] loss: 0.16276899811780782 acc: 0.7294
[Epoch 35] loss: 0.14432777227390833 acc: 0.7368
[Epoch 39] loss: 0.13708044374611733 acc: 0.7289
[Epoch 43] loss: 0.12265066841326634 acc: 0.7309
[Epoch 47] loss: 0.12032050622717651 acc: 0.727
[Epoch 51] loss: 0.11305377177794075 acc: 0.7424
[Epoch 55] loss: 0.10783842493913344 acc: 0.7331
[Epoch 59] loss: 0.09514787870809398 acc: 0.7374
[Epoch 63] loss: 0.09811556455917428 acc: 0.7332
[Epoch 67] loss: 0.09947290384184446 acc: 0.734
[Epoch 71] loss: 0.07939355612099852 acc: 0.7296
--> [test] acc: 0.739
--> [accuracy] finished 0.739
new state: tensor([640.,   3.,   3.,   1.,   5.], device='cuda:0')
new reward: 0.739
--> [reward] 0.739
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     3.0     |     1.0      |     5.0     | 0.739  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   3.,   1.,   5.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([640.,   3.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.4499220593506115 acc: 0.7074
[Epoch 7] loss: 2.0340991392160013 acc: 0.7624
[Epoch 11] loss: 0.6591388524207465 acc: 0.7524
[Epoch 15] loss: 0.31536448784315446 acc: 0.7486
[Epoch 19] loss: 0.23108722154251146 acc: 0.7559
[Epoch 23] loss: 0.19426399654210985 acc: 0.7534
[Epoch 27] loss: 0.16368358007982334 acc: 0.7553
[Epoch 31] loss: 0.15729242403184057 acc: 0.7522
[Epoch 35] loss: 0.13204527600749355 acc: 0.7477
[Epoch 39] loss: 0.13073325189797547 acc: 0.7519
[Epoch 43] loss: 0.11719539850388112 acc: 0.745
[Epoch 47] loss: 0.10982894713340131 acc: 0.7491
[Epoch 51] loss: 0.10976750833672219 acc: 0.7452
[Epoch 55] loss: 0.10362794116535998 acc: 0.7536
[Epoch 59] loss: 0.08980545180413843 acc: 0.7543
[Epoch 63] loss: 0.0881449627437536 acc: 0.755
[Epoch 67] loss: 0.08950687808272265 acc: 0.7529
[Epoch 71] loss: 0.08338326974944604 acc: 0.7537
--> [test] acc: 0.7402
--> [accuracy] finished 0.7402
new state: tensor([640.,   3.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7402
--> [reward] 0.7402
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     3.0     |     1.0      |     4.0     | 0.7402 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([640.,   3.,   2.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.785918877862604 acc: 0.6636
[Epoch 7] loss: 2.5118172244190253 acc: 0.7385
[Epoch 11] loss: 0.971384584951355 acc: 0.7335
[Epoch 15] loss: 0.4211086724763331 acc: 0.724
[Epoch 19] loss: 0.2890074953789373 acc: 0.7281
[Epoch 23] loss: 0.2313716987986356 acc: 0.7252
[Epoch 27] loss: 0.20341323470682515 acc: 0.725
[Epoch 31] loss: 0.18140658845081734 acc: 0.7253
[Epoch 35] loss: 0.17097536328694093 acc: 0.7263
[Epoch 39] loss: 0.15479490305046978 acc: 0.7243
[Epoch 43] loss: 0.13528316853629888 acc: 0.7252
[Epoch 47] loss: 0.14518873931011161 acc: 0.7225
[Epoch 51] loss: 0.13277927624380878 acc: 0.7152
[Epoch 55] loss: 0.11955337160888611 acc: 0.7218
[Epoch 59] loss: 0.11498694288451462 acc: 0.7222
[Epoch 63] loss: 0.10784938323931992 acc: 0.7226
[Epoch 67] loss: 0.11058416041543188 acc: 0.7207
[Epoch 71] loss: 0.1129553380494942 acc: 0.7283
--> [test] acc: 0.7202
--> [accuracy] finished 0.7202
new state: tensor([640.,   3.,   2.,   1.,   4.], device='cuda:0')
new reward: 0.7202
--> [reward] 0.7202
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     2.0     |     1.0      |     4.0     | 0.7202 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   2.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.766990773970513 acc: 0.6558
[Epoch 7] loss: 2.503636388141481 acc: 0.7329
[Epoch 11] loss: 0.9596278500907561 acc: 0.7297
[Epoch 15] loss: 0.41544740939098396 acc: 0.727
[Epoch 19] loss: 0.2790073946766231 acc: 0.7272
[Epoch 23] loss: 0.23906199942054726 acc: 0.7292
[Epoch 27] loss: 0.20122790873965338 acc: 0.7216
[Epoch 31] loss: 0.1863022650475316 acc: 0.7209
[Epoch 35] loss: 0.17053662936019776 acc: 0.7284
[Epoch 39] loss: 0.15248327479219 acc: 0.7233
[Epoch 43] loss: 0.1414065900749391 acc: 0.7314
[Epoch 47] loss: 0.14059329612830848 acc: 0.7317
[Epoch 51] loss: 0.12359552448162871 acc: 0.729
[Epoch 55] loss: 0.12431790453621218 acc: 0.7189
[Epoch 59] loss: 0.12168662986465636 acc: 0.725
[Epoch 63] loss: 0.11638117892920133 acc: 0.7194
[Epoch 67] loss: 0.10741879582009695 acc: 0.7166
[Epoch 71] loss: 0.10372214743008842 acc: 0.7161
--> [test] acc: 0.7231
--> [accuracy] finished 0.7231
new state: tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7231
--> [reward] 0.7231
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.7231 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.747821201143972 acc: 0.653
[Epoch 7] loss: 2.5125234031006505 acc: 0.7219
[Epoch 11] loss: 0.9480917941102439 acc: 0.7211
[Epoch 15] loss: 0.42650522870461804 acc: 0.7321
[Epoch 19] loss: 0.2901754067410403 acc: 0.7291
[Epoch 23] loss: 0.2341454687106716 acc: 0.7233
[Epoch 27] loss: 0.2118284961499293 acc: 0.7156
[Epoch 31] loss: 0.17786788184653082 acc: 0.7183
[Epoch 35] loss: 0.16844131138004229 acc: 0.7255
[Epoch 39] loss: 0.1513222819706306 acc: 0.7255
[Epoch 43] loss: 0.15464495024536654 acc: 0.7205
[Epoch 47] loss: 0.13793550516285782 acc: 0.7238
[Epoch 51] loss: 0.14140283598092948 acc: 0.7215
[Epoch 55] loss: 0.11784862789555865 acc: 0.7161
[Epoch 59] loss: 0.11946588906470229 acc: 0.7238
[Epoch 63] loss: 0.11624833364682773 acc: 0.7208
[Epoch 67] loss: 0.11046616989318156 acc: 0.7135
[Epoch 71] loss: 0.10259824193438843 acc: 0.7237
--> [test] acc: 0.715
--> [accuracy] finished 0.715
new state: tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.715
--> [reward] 0.715
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     2.0     |     1.0      |     3.0     | 0.715  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.995019875219106 acc: 0.6383
[Epoch 7] loss: 2.8173054099997596 acc: 0.7178
[Epoch 11] loss: 1.2396291081729296 acc: 0.6998
[Epoch 15] loss: 0.5313645039525483 acc: 0.7033
[Epoch 19] loss: 0.34829350190041847 acc: 0.7118
[Epoch 23] loss: 0.273546937874535 acc: 0.7059
[Epoch 27] loss: 0.2382705770361492 acc: 0.7047
[Epoch 31] loss: 0.2164245717622854 acc: 0.7048
[Epoch 35] loss: 0.1911355917487303 acc: 0.7028
[Epoch 39] loss: 0.1752925076476677 acc: 0.7083
[Epoch 43] loss: 0.1750240859092044 acc: 0.7031
[Epoch 47] loss: 0.16566717837486997 acc: 0.7059
[Epoch 51] loss: 0.15281828110704146 acc: 0.7034
[Epoch 55] loss: 0.13764212204052892 acc: 0.7032
[Epoch 59] loss: 0.1376161978159057 acc: 0.706
[Epoch 63] loss: 0.12901490193713086 acc: 0.702
[Epoch 67] loss: 0.13190293133876924 acc: 0.7115
[Epoch 71] loss: 0.12026951855758343 acc: 0.6989
--> [test] acc: 0.7005
--> [accuracy] finished 0.7005
new state: tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7005
--> [reward] 0.7005
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7005 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  512.0   |     3.0      |     4.0     |     2.0      |     2.0     | 0.6619 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.4972426312049025 acc: 0.6011
[Epoch 7] loss: 3.6552448431244287 acc: 0.6634
[Epoch 11] loss: 2.1709987801282913 acc: 0.6545
[Epoch 15] loss: 1.0050858248530141 acc: 0.6381
[Epoch 19] loss: 0.5661337383310584 acc: 0.6467
[Epoch 23] loss: 0.4064663601396105 acc: 0.6467
[Epoch 27] loss: 0.3658994645251871 acc: 0.6484
[Epoch 31] loss: 0.29247475689863 acc: 0.6401
[Epoch 35] loss: 0.29001962769862333 acc: 0.6461
[Epoch 39] loss: 0.26305865345741897 acc: 0.6451
[Epoch 43] loss: 0.24208299344872383 acc: 0.6319
[Epoch 47] loss: 0.2288745224018536 acc: 0.6456
[Epoch 51] loss: 0.21316395411053507 acc: 0.6461
[Epoch 55] loss: 0.20486176864046346 acc: 0.6447
[Epoch 59] loss: 0.1942240868628387 acc: 0.6336
[Epoch 63] loss: 0.1927702536668314 acc: 0.6343
[Epoch 67] loss: 0.17387354994297524 acc: 0.6495
[Epoch 71] loss: 0.1740455846331985 acc: 0.6452
--> [test] acc: 0.6428
--> [accuracy] finished 0.6428
new state: tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6428
--> [reward] 0.6428
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0912, 0.0908, 0.0906, 0.0910, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3976, -2.3980, -2.3982, -2.3978, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([640.,   3.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.354171572286455 acc: 0.6178
[Epoch 7] loss: 3.338236879662175 acc: 0.6414
[Epoch 11] loss: 1.7592531785635692 acc: 0.678
[Epoch 15] loss: 0.7485307044423449 acc: 0.6664
[Epoch 19] loss: 0.4334312814533177 acc: 0.6628
[Epoch 23] loss: 0.34594920014872044 acc: 0.6701
[Epoch 27] loss: 0.29204406352747053 acc: 0.6679
[Epoch 31] loss: 0.261579584244572 acc: 0.6708
[Epoch 35] loss: 0.23262717210166062 acc: 0.6687
[Epoch 39] loss: 0.21124172812003805 acc: 0.6663
[Epoch 43] loss: 0.21245804210872296 acc: 0.6739
[Epoch 47] loss: 0.1879478434905829 acc: 0.6685
[Epoch 51] loss: 0.18807179817353445 acc: 0.6711
[Epoch 55] loss: 0.17704126608731877 acc: 0.6636
[Epoch 59] loss: 0.1645238961276534 acc: 0.6614
[Epoch 63] loss: 0.1533941753911893 acc: 0.6574
[Epoch 67] loss: 0.1528884861368181 acc: 0.664
[Epoch 71] loss: 0.13751202910338217 acc: 0.6605
--> [test] acc: 0.6629
--> [accuracy] finished 0.6629
new state: tensor([640.,   3.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6629
--> [reward] 0.6629
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2200]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.4400]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.6633]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.6583]], device='cuda:0')
------ ------
delta_t: tensor([[0.6633]], device='cuda:0')
rewards[i]: 0.6629
values[i+1]: tensor([[-0.0046]], device='cuda:0')
values[i]: tensor([[-0.0050]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.6633]], device='cuda:0')
delta_t: tensor([[0.6633]], device='cuda:0')
------ ------
policy_loss: 1.566747784614563
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.6633]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.0646]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.6892]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.2997]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.2946]], device='cuda:0')
------ ------
delta_t: tensor([[0.6430]], device='cuda:0')
rewards[i]: 0.6428
values[i+1]: tensor([[-0.0050]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.2997]], device='cuda:0')
delta_t: tensor([[0.6430]], device='cuda:0')
------ ------
policy_loss: 4.659158706665039
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.2997]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.0390]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[3.9488]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.9872]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.9821]], device='cuda:0')
------ ------
delta_t: tensor([[0.7005]], device='cuda:0')
rewards[i]: 0.7005
values[i+1]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0051]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.9872]], device='cuda:0')
delta_t: tensor([[0.7005]], device='cuda:0')
------ ------
policy_loss: 9.40037727355957
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.9872]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[6.6370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[7.1959]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.6825]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.6773]], device='cuda:0')
------ ------
delta_t: tensor([[0.7152]], device='cuda:0')
rewards[i]: 0.715
values[i+1]: tensor([[-0.0051]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.6825]], device='cuda:0')
delta_t: tensor([[0.7152]], device='cuda:0')
------ ------
policy_loss: 15.808653831481934
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.6825]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[12.3461]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[11.4182]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.3791]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.3736]], device='cuda:0')
------ ------
delta_t: tensor([[0.7234]], device='cuda:0')
rewards[i]: 0.7231
values[i+1]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0055]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.3791]], device='cuda:0')
delta_t: tensor([[0.7234]], device='cuda:0')
------ ------
policy_loss: 23.888351440429688
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.3791]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[20.6115]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[16.5308]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.0658]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.0601]], device='cuda:0')
------ ------
delta_t: tensor([[0.7205]], device='cuda:0')
rewards[i]: 0.7202
values[i+1]: tensor([[-0.0055]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.0658]], device='cuda:0')
delta_t: tensor([[0.7205]], device='cuda:0')
------ ------
policy_loss: 33.61322021484375
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.0658]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[31.9661]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[22.7092]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.7654]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.7597]], device='cuda:0')
------ ------
delta_t: tensor([[0.7403]], device='cuda:0')
rewards[i]: 0.7402
values[i+1]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.7654]], device='cuda:0')
delta_t: tensor([[0.7403]], device='cuda:0')
------ ------
policy_loss: 45.0175895690918
log_probs[i]: tensor([[-2.3982]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.7654]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[46.8538]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[29.7752]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.4567]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.4511]], device='cuda:0')
------ ------
delta_t: tensor([[0.7389]], device='cuda:0')
rewards[i]: 0.739
values[i+1]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.4567]], device='cuda:0')
delta_t: tensor([[0.7389]], device='cuda:0')
------ ------
policy_loss: 58.07859802246094
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.4567]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[65.5213]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[37.3352]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.1103]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.1047]], device='cuda:0')
------ ------
delta_t: tensor([[0.7082]], device='cuda:0')
rewards[i]: 0.7081
values[i+1]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.1103]], device='cuda:0')
delta_t: tensor([[0.7082]], device='cuda:0')
------ ------
policy_loss: 72.70712280273438
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.1103]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[88.4007]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[45.7586]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.7645]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.7591]], device='cuda:0')
------ ------
delta_t: tensor([[0.7154]], device='cuda:0')
rewards[i]: 0.7155
values[i+1]: tensor([[-0.0056]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0054]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.7645]], device='cuda:0')
delta_t: tensor([[0.7154]], device='cuda:0')
------ ------
policy_loss: 88.90335083007812
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.7645]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[115.5620]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[54.3226]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.3704]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.3652]], device='cuda:0')
------ ------
delta_t: tensor([[0.6735]], device='cuda:0')
rewards[i]: 0.6737
values[i+1]: tensor([[-0.0054]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.3704]], device='cuda:0')
delta_t: tensor([[0.6735]], device='cuda:0')
------ ------
policy_loss: 106.5509033203125
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.3704]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[147.4539]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[63.7839]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.9865]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.9812]], device='cuda:0')
------ ------
delta_t: tensor([[0.6898]], device='cuda:0')
rewards[i]: 0.6896
values[i+1]: tensor([[-0.0052]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0053]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.9865]], device='cuda:0')
delta_t: tensor([[0.6898]], device='cuda:0')
------ ------
policy_loss: 125.6772232055664
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.9865]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[184.5200]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[74.1321]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.6100]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.6043]], device='cuda:0')
------ ------
delta_t: tensor([[0.7034]], device='cuda:0')
rewards[i]: 0.7029
values[i+1]: tensor([[-0.0053]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.6100]], device='cuda:0')
delta_t: tensor([[0.7034]], device='cuda:0')
------ ------
policy_loss: 146.30020141601562
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.6100]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[227.0454]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[85.0508]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.2223]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.2160]], device='cuda:0')
------ ------
delta_t: tensor([[0.6984]], device='cuda:0')
rewards[i]: 0.6978
values[i+1]: tensor([[-0.0057]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0063]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.2223]], device='cuda:0')
delta_t: tensor([[0.6984]], device='cuda:0')
------ ------
policy_loss: 168.38961791992188
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.2223]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[275.4820]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[96.8732]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.8424]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.8356]], device='cuda:0')
------ ------
delta_t: tensor([[0.7123]], device='cuda:0')
rewards[i]: 0.7117
values[i+1]: tensor([[-0.0063]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0069]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.8424]], device='cuda:0')
delta_t: tensor([[0.7123]], device='cuda:0')
------ ------
policy_loss: 191.96420288085938
log_probs[i]: tensor([[-2.3976]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.8424]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 191.96420288085938
value_loss: 275.48199462890625
loss: 329.7052001953125



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 1.1431e-03,  8.0183e-06,  4.5599e-06,  8.3270e-07,  1.0207e-05],
        [-4.6056e-02, -2.9386e-04, -2.0368e-04, -1.1840e-04, -3.7317e-04],
        [-6.2578e-04, -3.6526e-06, -3.1328e-06, -3.0513e-06, -4.6732e-06],
        [-1.2708e-01, -8.0960e-04, -5.5329e-04, -2.9432e-04, -1.0225e-03],
        [-3.7426e-01, -2.4133e-03, -1.5878e-03, -7.0647e-04, -3.0245e-03]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-7.1951e-06, -1.3445e-05, -7.5031e-07,  1.3296e-05, -6.0404e-06],
        [ 2.7937e-04,  5.5240e-04,  4.3243e-05, -5.4768e-04,  2.4175e-04],
        [ 3.6448e-06,  7.6981e-06,  7.8980e-07, -7.6475e-06,  3.2642e-06],
        [ 7.7237e-04,  1.5189e-03,  1.1580e-04, -1.5061e-03,  6.6694e-04],
        [ 2.2896e-03,  4.4491e-03,  3.1887e-04, -4.4105e-03,  1.9655e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.1299,  0.1260, -0.1297,  0.1094, -0.0695],
        [ 0.1272, -0.1231,  0.1270, -0.1062,  0.0669],
        [-0.0027,  0.0024, -0.0027,  0.0016, -0.0006],
        [ 0.0080, -0.0078,  0.0080, -0.0067,  0.0042],
        [-0.0026,  0.0025, -0.0026,  0.0019, -0.0010]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 6.3132e-03,  1.3345e-02,  1.5315e-03, -1.3088e-02,  5.5318e-03],
        [ 3.2411e-02,  6.6633e-02,  6.0540e-03, -6.5871e-02,  2.8295e-02],
        [ 1.2451e-03,  1.9217e-03,  2.5894e-05, -2.0593e-03,  1.0796e-03],
        [-2.2476e-02, -4.5317e-02, -3.8178e-03,  4.4967e-02, -1.9711e-02],
        [-6.0381e-03, -1.2621e-02, -1.0162e-03,  1.2265e-02, -5.1250e-03],
        [ 2.5197e-02,  5.0820e-02,  4.0828e-03, -5.0298e-02,  2.2064e-02],
        [ 3.4750e-02,  6.8953e-02,  5.4009e-03, -6.8545e-02,  3.0332e-02],
        [-2.5027e-02, -5.0216e-02, -4.2716e-03,  4.9907e-02, -2.1954e-02],
        [ 3.6682e-03,  6.8932e-03,  5.5226e-04, -7.0732e-03,  3.3884e-03],
        [-2.4998e-02, -5.0158e-02, -4.2667e-03,  4.9850e-02, -2.1929e-02],
        [-2.5046e-02, -5.0254e-02, -4.2749e-03,  4.9946e-02, -2.1971e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.5119,  3.0337,  0.2581, -3.0150,  1.3263]], device='cuda:0')
--> [loss] 329.7052001953125

---------------------------------- [[#3 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     3.0      |     1.0     |     1.0      |     3.0     | 0.6629 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([640.,   3.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.521948455544689 acc: 0.6002
[Epoch 7] loss: 3.6402679146708126 acc: 0.6487
[Epoch 11] loss: 2.159822895208283 acc: 0.6578
[Epoch 15] loss: 0.9789673854094332 acc: 0.6455
[Epoch 19] loss: 0.5372468724637233 acc: 0.6445
[Epoch 23] loss: 0.4120760449753774 acc: 0.6423
[Epoch 27] loss: 0.3488112513757194 acc: 0.6409
[Epoch 31] loss: 0.3099295468500737 acc: 0.6432
[Epoch 35] loss: 0.2842530496442295 acc: 0.6425
[Epoch 39] loss: 0.25898040504530645 acc: 0.6407
[Epoch 43] loss: 0.24176692149942489 acc: 0.6522
[Epoch 47] loss: 0.21658375647747913 acc: 0.6496
[Epoch 51] loss: 0.21817897884484827 acc: 0.6384
[Epoch 55] loss: 0.2003105598027863 acc: 0.6464
[Epoch 59] loss: 0.19746732497063782 acc: 0.6413
[Epoch 63] loss: 0.1862935369877178 acc: 0.634
[Epoch 67] loss: 0.17869473855270793 acc: 0.6501
[Epoch 71] loss: 0.17965955767647157 acc: 0.6479
--> [test] acc: 0.6528
--> [accuracy] finished 0.6528
new state: tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
new reward: 0.6528
--> [reward] 0.6528
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6528 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   1.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.0130323319483905 acc: 0.6412
[Epoch 7] loss: 2.8359540542373267 acc: 0.7058
[Epoch 11] loss: 1.2503094772243744 acc: 0.7174
[Epoch 15] loss: 0.5436319133881337 acc: 0.7052
[Epoch 19] loss: 0.34250442057019076 acc: 0.7124
[Epoch 23] loss: 0.274910304881156 acc: 0.7105
[Epoch 27] loss: 0.24193001648201548 acc: 0.6949
[Epoch 31] loss: 0.21697534811075614 acc: 0.7093
[Epoch 35] loss: 0.19280754544479234 acc: 0.7043
[Epoch 39] loss: 0.18353990920345345 acc: 0.7004
[Epoch 43] loss: 0.17631375861099308 acc: 0.7074
[Epoch 47] loss: 0.15076247188488923 acc: 0.7056
[Epoch 51] loss: 0.1556599063922644 acc: 0.7027
[Epoch 55] loss: 0.1424540339253338 acc: 0.7103
[Epoch 59] loss: 0.13143370052099304 acc: 0.6986
[Epoch 63] loss: 0.13324343598902683 acc: 0.7027
[Epoch 67] loss: 0.12599117837100746 acc: 0.7059
[Epoch 71] loss: 0.12402135025366874 acc: 0.6984
--> [test] acc: 0.6989
--> [accuracy] finished 0.6989
new state: tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.6989
--> [reward] 0.6989
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.6989 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.004454057387379 acc: 0.6572
[Epoch 7] loss: 2.8150209897314498 acc: 0.7127
[Epoch 11] loss: 1.2342554273469675 acc: 0.7199
[Epoch 15] loss: 0.5250857463035056 acc: 0.6992
[Epoch 19] loss: 0.3549493051317456 acc: 0.7132
[Epoch 23] loss: 0.27165197997170565 acc: 0.7082
[Epoch 27] loss: 0.2396555127637923 acc: 0.7044
[Epoch 31] loss: 0.21223388329062545 acc: 0.7179
[Epoch 35] loss: 0.19313516386586915 acc: 0.7102
[Epoch 39] loss: 0.17921922132229942 acc: 0.6967
[Epoch 43] loss: 0.17039800462458293 acc: 0.7044
[Epoch 47] loss: 0.16335112105309488 acc: 0.7121
[Epoch 51] loss: 0.14573045481291727 acc: 0.7159
[Epoch 55] loss: 0.15442247970548967 acc: 0.7049
[Epoch 59] loss: 0.1349402817437792 acc: 0.7094
[Epoch 63] loss: 0.12472912146712241 acc: 0.7048
[Epoch 67] loss: 0.128021556916206 acc: 0.7083
[Epoch 71] loss: 0.10878602004237235 acc: 0.7116
--> [test] acc: 0.714
--> [accuracy] finished 0.714
new state: tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.714
--> [reward] 0.714
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.714  |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[6]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act0 (stride_height_sub)
--> [step] to state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.957730488703989 acc: 0.6446
[Epoch 7] loss: 2.7893069250809264 acc: 0.712
[Epoch 11] loss: 1.2247523437813876 acc: 0.7142
[Epoch 15] loss: 0.5222012119086654 acc: 0.7078
[Epoch 19] loss: 0.33955713926726366 acc: 0.7092
[Epoch 23] loss: 0.2740705750330025 acc: 0.7042
[Epoch 27] loss: 0.24942239274358963 acc: 0.6979
[Epoch 31] loss: 0.19998898780892801 acc: 0.7097
[Epoch 35] loss: 0.19290544369431864 acc: 0.7024
[Epoch 39] loss: 0.18351810815794123 acc: 0.7072
[Epoch 43] loss: 0.15421840165059328 acc: 0.7022
[Epoch 47] loss: 0.1690172796706905 acc: 0.7097
[Epoch 51] loss: 0.15002496299140936 acc: 0.7085
[Epoch 55] loss: 0.13819947760180593 acc: 0.7073
[Epoch 59] loss: 0.14081654538724409 acc: 0.7028
[Epoch 63] loss: 0.11675518690108243 acc: 0.7065
[Epoch 67] loss: 0.12660847786758 acc: 0.7089
[Epoch 71] loss: 0.12324507908581202 acc: 0.7078
--> [test] acc: 0.7019
--> [accuracy] finished 0.7019
new state: tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7019
--> [reward] 0.7019
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7019 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([608.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.672346872137026 acc: 0.67
[Epoch 7] loss: 2.3180205004904275 acc: 0.7366
[Epoch 11] loss: 0.8336662697174665 acc: 0.743
[Epoch 15] loss: 0.3787389014183027 acc: 0.7371
[Epoch 19] loss: 0.2596218121640594 acc: 0.7492
[Epoch 23] loss: 0.22256043030525488 acc: 0.7433
[Epoch 27] loss: 0.19359655160447367 acc: 0.751
[Epoch 31] loss: 0.1696947810282964 acc: 0.7311
[Epoch 35] loss: 0.1638174305867661 acc: 0.7393
[Epoch 39] loss: 0.14395387956034153 acc: 0.7393
[Epoch 43] loss: 0.13708443121146172 acc: 0.7393
[Epoch 47] loss: 0.12561838426258023 acc: 0.744
[Epoch 51] loss: 0.11950078959007511 acc: 0.7398
[Epoch 55] loss: 0.10707222592190403 acc: 0.7381
[Epoch 59] loss: 0.11377799916116858 acc: 0.7463
[Epoch 63] loss: 0.11159615766334757 acc: 0.7338
[Epoch 67] loss: 0.09728336442560268 acc: 0.7452
[Epoch 71] loss: 0.09740179630468779 acc: 0.7376
--> [test] acc: 0.7321
--> [accuracy] finished 0.7321
new state: tensor([608.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7321
--> [reward] 0.7321
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7321 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([608.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.654637673626775 acc: 0.6566
[Epoch 7] loss: 2.301668480534078 acc: 0.7435
[Epoch 11] loss: 0.8096842126649283 acc: 0.7443
[Epoch 15] loss: 0.3730992476653565 acc: 0.7421
[Epoch 19] loss: 0.2589963396744388 acc: 0.7539
[Epoch 23] loss: 0.2225431352877594 acc: 0.7496
[Epoch 27] loss: 0.18952850281687267 acc: 0.7401
[Epoch 31] loss: 0.17109420207564902 acc: 0.7485
[Epoch 35] loss: 0.15780133561081136 acc: 0.7431
[Epoch 39] loss: 0.139179393531495 acc: 0.7453
[Epoch 43] loss: 0.13818887643077793 acc: 0.7428
[Epoch 47] loss: 0.12107428805688229 acc: 0.7493
[Epoch 51] loss: 0.117545000197666 acc: 0.7383
[Epoch 55] loss: 0.11409175764445377 acc: 0.7474
[Epoch 59] loss: 0.11453337348727248 acc: 0.7501
[Epoch 63] loss: 0.10238719860117411 acc: 0.741
[Epoch 67] loss: 0.09656580008642004 acc: 0.7403
[Epoch 71] loss: 0.10744226378542837 acc: 0.7468
--> [test] acc: 0.7416
--> [accuracy] finished 0.7416
new state: tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7416
--> [reward] 0.7416
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7416 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3982, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.665019155463295 acc: 0.696
[Epoch 7] loss: 2.2967145401422324 acc: 0.7391
[Epoch 11] loss: 0.8168628288532995 acc: 0.7455
[Epoch 15] loss: 0.37030728113458816 acc: 0.7412
[Epoch 19] loss: 0.26178826668949995 acc: 0.7292
[Epoch 23] loss: 0.21501785773269433 acc: 0.7383
[Epoch 27] loss: 0.19394555860115667 acc: 0.7439
[Epoch 31] loss: 0.17505903295515216 acc: 0.7474
[Epoch 35] loss: 0.14701537189104824 acc: 0.747
[Epoch 39] loss: 0.1442967622119295 acc: 0.7415
[Epoch 43] loss: 0.1301902039381473 acc: 0.7426
[Epoch 47] loss: 0.13008389707885162 acc: 0.7448
[Epoch 51] loss: 0.12220334683609245 acc: 0.7368
[Epoch 55] loss: 0.11240112538570943 acc: 0.739
[Epoch 59] loss: 0.10359654853434856 acc: 0.736
[Epoch 63] loss: 0.09681324234298999 acc: 0.7418
[Epoch 67] loss: 0.1071858283244264 acc: 0.7354
[Epoch 71] loss: 0.08610920602565303 acc: 0.738
--> [test] acc: 0.7384
--> [accuracy] finished 0.7384
new state: tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7384
--> [reward] 0.7384
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7384 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] to state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.672720519782942 acc: 0.6874
[Epoch 7] loss: 2.317293959429197 acc: 0.7443
[Epoch 11] loss: 0.8605074636981158 acc: 0.7418
[Epoch 15] loss: 0.3782447024970256 acc: 0.7386
[Epoch 19] loss: 0.280488516608982 acc: 0.7387
[Epoch 23] loss: 0.22046780307798663 acc: 0.7422
[Epoch 27] loss: 0.1903103540634827 acc: 0.7458
[Epoch 31] loss: 0.18402644532644535 acc: 0.7397
[Epoch 35] loss: 0.15561994294995618 acc: 0.7394
[Epoch 39] loss: 0.14901669046191304 acc: 0.7391
[Epoch 43] loss: 0.1383569698502093 acc: 0.7415
[Epoch 47] loss: 0.12935745011320782 acc: 0.7419
[Epoch 51] loss: 0.11714446999947242 acc: 0.7293
[Epoch 55] loss: 0.12408723924761576 acc: 0.7392
[Epoch 59] loss: 0.10547335101517817 acc: 0.7506
[Epoch 63] loss: 0.10394282689642 acc: 0.7411
[Epoch 67] loss: 0.10127215653263709 acc: 0.7381
[Epoch 71] loss: 0.0959822306626231 acc: 0.7368
--> [test] acc: 0.7416
--> [accuracy] finished 0.7416
new state: tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7416
--> [reward] 0.7416
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7416 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.539909053946395 acc: 0.7017
[Epoch 7] loss: 2.0726000513795695 acc: 0.7649
[Epoch 11] loss: 0.6889556723425303 acc: 0.7494
[Epoch 15] loss: 0.3280438023269215 acc: 0.7541
[Epoch 19] loss: 0.23631865516200165 acc: 0.7504
[Epoch 23] loss: 0.19680640240773903 acc: 0.7403
[Epoch 27] loss: 0.16999343749912232 acc: 0.7481
[Epoch 31] loss: 0.15093094333672843 acc: 0.749
[Epoch 35] loss: 0.14177367768352828 acc: 0.7474
[Epoch 39] loss: 0.124046939364432 acc: 0.7519
[Epoch 43] loss: 0.11625531222552175 acc: 0.7489
[Epoch 47] loss: 0.11981942020523388 acc: 0.7521
[Epoch 51] loss: 0.10454754439442207 acc: 0.7453
[Epoch 55] loss: 0.10611570442778409 acc: 0.7561
[Epoch 59] loss: 0.09706284848096616 acc: 0.7554
[Epoch 63] loss: 0.0940254906709294 acc: 0.752
[Epoch 67] loss: 0.08368738619443934 acc: 0.7501
[Epoch 71] loss: 0.0856099115429449 acc: 0.7535
--> [test] acc: 0.7521
--> [accuracy] finished 0.7521
new state: tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
new reward: 0.7521
--> [reward] 0.7521
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7521 |
|   best  |  512.0   |     3.0      |     4.0     |     1.0      |     2.0     | 0.7535 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[9]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act1 (stride_width_add)
--> [step] new state is not available; not change
--> [step] to state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.502642883364197 acc: 0.7007
[Epoch 7] loss: 2.106293220959051 acc: 0.7439
[Epoch 11] loss: 0.6900882796358193 acc: 0.755
[Epoch 15] loss: 0.31799164028538157 acc: 0.7578
[Epoch 19] loss: 0.23779706817472834 acc: 0.757
[Epoch 23] loss: 0.1953345190524064 acc: 0.7586
[Epoch 27] loss: 0.16529205834964658 acc: 0.7575
[Epoch 31] loss: 0.15753345282229086 acc: 0.7563
[Epoch 35] loss: 0.1367013333133086 acc: 0.7649
[Epoch 39] loss: 0.12892251445071015 acc: 0.7556
[Epoch 43] loss: 0.12205805018891955 acc: 0.7487
[Epoch 47] loss: 0.11264657920054957 acc: 0.7408
[Epoch 51] loss: 0.11288581510572254 acc: 0.7498
[Epoch 55] loss: 0.10140835196140063 acc: 0.7542
[Epoch 59] loss: 0.09363704356525684 acc: 0.7589
[Epoch 63] loss: 0.09035365099249326 acc: 0.7598
[Epoch 67] loss: 0.08282440757025224 acc: 0.7605
[Epoch 71] loss: 0.09691879777428325 acc: 0.747
--> [test] acc: 0.7607
--> [accuracy] finished 0.7607
new state: tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
new reward: 0.7607
--> [reward] 0.7607
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   4.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.665154112879272 acc: 0.6724
[Epoch 7] loss: 2.3240905243646153 acc: 0.7543
[Epoch 11] loss: 0.8701558314797366 acc: 0.7481
[Epoch 15] loss: 0.3901414486539105 acc: 0.7404
[Epoch 19] loss: 0.26889269512689784 acc: 0.7455
[Epoch 23] loss: 0.2201125374197236 acc: 0.7398
[Epoch 27] loss: 0.19434487626023228 acc: 0.7442
[Epoch 31] loss: 0.17077291899782313 acc: 0.7439
[Epoch 35] loss: 0.1611812848030873 acc: 0.737
[Epoch 39] loss: 0.1462932635423825 acc: 0.7375
[Epoch 43] loss: 0.13112277917616316 acc: 0.743
[Epoch 47] loss: 0.12980607169611222 acc: 0.7431
[Epoch 51] loss: 0.10956846854156431 acc: 0.7441
[Epoch 55] loss: 0.12641228320996475 acc: 0.7482
[Epoch 59] loss: 0.11364374749834084 acc: 0.7444
[Epoch 63] loss: 0.1039429113418912 acc: 0.7363
[Epoch 67] loss: 0.10303771510348676 acc: 0.7344
[Epoch 71] loss: 0.09921462755755085 acc: 0.7417
--> [test] acc: 0.7473
--> [accuracy] finished 0.7473
new state: tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7473
--> [reward] 0.7473
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7473 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0906,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.734942460761351 acc: 0.6919
[Epoch 7] loss: 2.4525944467853096 acc: 0.753
[Epoch 11] loss: 1.0070814128936556 acc: 0.7513
[Epoch 15] loss: 0.4512679431196826 acc: 0.75
[Epoch 19] loss: 0.3003807630785324 acc: 0.7573
[Epoch 23] loss: 0.2500671328133558 acc: 0.7466
[Epoch 27] loss: 0.20659135708399592 acc: 0.7433
[Epoch 31] loss: 0.19620044540156564 acc: 0.7464
[Epoch 35] loss: 0.17585841322417758 acc: 0.748
[Epoch 39] loss: 0.17476891833798164 acc: 0.7528
[Epoch 43] loss: 0.14471544471838513 acc: 0.7407
[Epoch 47] loss: 0.1476028966717422 acc: 0.7497
[Epoch 51] loss: 0.13224045398181825 acc: 0.7459
[Epoch 55] loss: 0.1315421001494998 acc: 0.7459
[Epoch 59] loss: 0.1330236594866761 acc: 0.7426
[Epoch 63] loss: 0.11637019868249364 acc: 0.7413
[Epoch 67] loss: 0.11964346711461901 acc: 0.7402
[Epoch 71] loss: 0.11508736976400337 acc: 0.7523
--> [test] acc: 0.7465
--> [accuracy] finished 0.7465
new state: tensor([672.,   1.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7465
--> [reward] 0.7465
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     3.0     |     1.0      |     4.0     | 0.7465 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0905,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([704.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.715828740993119 acc: 0.6719
[Epoch 7] loss: 2.424448917284036 acc: 0.7541
[Epoch 11] loss: 0.9932670096397552 acc: 0.7483
[Epoch 15] loss: 0.44268471395592096 acc: 0.7465
[Epoch 19] loss: 0.300002017982609 acc: 0.7481
[Epoch 23] loss: 0.2456215713458979 acc: 0.7452
[Epoch 27] loss: 0.2115586141024328 acc: 0.7515
[Epoch 31] loss: 0.18963468453282362 acc: 0.7504
[Epoch 35] loss: 0.1745425784112433 acc: 0.7433
[Epoch 39] loss: 0.1629233728544048 acc: 0.7418
[Epoch 43] loss: 0.15801435195462174 acc: 0.7441
[Epoch 47] loss: 0.14891230063501013 acc: 0.7463
[Epoch 51] loss: 0.1312191859669233 acc: 0.7498
[Epoch 55] loss: 0.12003143595161198 acc: 0.7417
[Epoch 59] loss: 0.12374550958766657 acc: 0.7377
[Epoch 63] loss: 0.12054485613551667 acc: 0.7341
[Epoch 67] loss: 0.11244536042058617 acc: 0.7473
[Epoch 71] loss: 0.10700354360334595 acc: 0.7441
--> [test] acc: 0.7468
--> [accuracy] finished 0.7468
new state: tensor([704.,   1.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7468
--> [reward] 0.7468
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     1.0      |     3.0     |     1.0      |     4.0     | 0.7468 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0905,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[3]], device='cuda:0')
--> [step] from state tensor([704.,   1.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx1 act1 (filter_height_add)
--> [step] to state tensor([704.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.655573274016075 acc: 0.6612
[Epoch 7] loss: 2.310354094028168 acc: 0.7474
[Epoch 11] loss: 0.837683859774295 acc: 0.7407
[Epoch 15] loss: 0.38233452686168196 acc: 0.7277
[Epoch 19] loss: 0.2741195338913966 acc: 0.7458
[Epoch 23] loss: 0.2223128797093411 acc: 0.7459
[Epoch 27] loss: 0.192829892724095 acc: 0.7431
[Epoch 31] loss: 0.17656475518975417 acc: 0.743
[Epoch 35] loss: 0.1446689342856026 acc: 0.7388
[Epoch 39] loss: 0.15527313072091478 acc: 0.7414
[Epoch 43] loss: 0.12818295253759912 acc: 0.7379
[Epoch 47] loss: 0.12881241356977796 acc: 0.7388
[Epoch 51] loss: 0.11743162715059641 acc: 0.734
[Epoch 55] loss: 0.1196752471785011 acc: 0.7333
[Epoch 59] loss: 0.10975729485156724 acc: 0.7351
[Epoch 63] loss: 0.10781770288143926 acc: 0.7451
[Epoch 67] loss: 0.1060556575329617 acc: 0.7366
[Epoch 71] loss: 0.0886185622197645 acc: 0.7373
--> [test] acc: 0.7456
--> [accuracy] finished 0.7456
new state: tensor([704.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7456
--> [reward] 0.7456
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  704.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7456 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0913, 0.0908, 0.0905, 0.0911, 0.0908, 0.0910, 0.0910, 0.0905,
         0.0909, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3975, -2.3980, -2.3983, -2.3977, -2.3980, -2.3978, -2.3978,
         -2.3983, -2.3979, -2.3977]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([704.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.659922680434058 acc: 0.6652
[Epoch 7] loss: 2.301603253387734 acc: 0.7395
[Epoch 11] loss: 0.8627922878412487 acc: 0.7427
[Epoch 15] loss: 0.37950683431342586 acc: 0.7455
[Epoch 19] loss: 0.26888207968353006 acc: 0.7415
[Epoch 23] loss: 0.21836006813837439 acc: 0.7435
[Epoch 27] loss: 0.18631580278939566 acc: 0.7423
[Epoch 31] loss: 0.18527808871186907 acc: 0.7464
[Epoch 35] loss: 0.15465080692811542 acc: 0.7422
[Epoch 39] loss: 0.14049228525641935 acc: 0.7449
[Epoch 43] loss: 0.13486187802413313 acc: 0.742
[Epoch 47] loss: 0.12986045667325216 acc: 0.7296
[Epoch 51] loss: 0.12528395408745427 acc: 0.7377
[Epoch 55] loss: 0.10595722799487126 acc: 0.7444
[Epoch 59] loss: 0.11700785272252148 acc: 0.7377
[Epoch 63] loss: 0.0989214376959702 acc: 0.7291
[Epoch 67] loss: 0.10428828472102447 acc: 0.7353
[Epoch 71] loss: 0.09127599333607665 acc: 0.7407
--> [test] acc: 0.7319
--> [accuracy] finished 0.7319
new state: tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
new reward: 0.7319
--> [reward] 0.7319
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.2681]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.5362]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.7323]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.7301]], device='cuda:0')
------ ------
delta_t: tensor([[0.7323]], device='cuda:0')
rewards[i]: 0.7319
values[i+1]: tensor([[-0.0019]], device='cuda:0')
values[i]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.7323]], device='cuda:0')
delta_t: tensor([[0.7323]], device='cuda:0')
------ ------
policy_loss: 1.7318576574325562
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.7323]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[1.3495]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.1628]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.4706]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.4684]], device='cuda:0')
------ ------
delta_t: tensor([[0.7457]], device='cuda:0')
rewards[i]: 0.7456
values[i+1]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0023]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.4706]], device='cuda:0')
delta_t: tensor([[0.7457]], device='cuda:0')
------ ------
policy_loss: 5.23491096496582
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.4706]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[3.7751]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[4.8512]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.2025]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.2005]], device='cuda:0')
------ ------
delta_t: tensor([[0.7466]], device='cuda:0')
rewards[i]: 0.7468
values[i+1]: tensor([[-0.0023]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.2025]], device='cuda:0')
delta_t: tensor([[0.7466]], device='cuda:0')
------ ------
policy_loss: 10.491554260253906
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.2025]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[8.0586]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.5670]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9270]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.9250]], device='cuda:0')
------ ------
delta_t: tensor([[0.7464]], device='cuda:0')
rewards[i]: 0.7465
values[i+1]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0020]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9269]], device='cuda:0')
delta_t: tensor([[0.7464]], device='cuda:0')
------ ------
policy_loss: 17.486297607421875
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9269]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[14.7023]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.2874]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6452]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6430]], device='cuda:0')
------ ------
delta_t: tensor([[0.7475]], device='cuda:0')
rewards[i]: 0.7473
values[i+1]: tensor([[-0.0020]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6452]], device='cuda:0')
delta_t: tensor([[0.7475]], device='cuda:0')
------ ------
policy_loss: 26.20235824584961
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6452]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[24.2479]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[19.0913]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3694]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3673]], device='cuda:0')
------ ------
delta_t: tensor([[0.7606]], device='cuda:0')
rewards[i]: 0.7607
values[i+1]: tensor([[-0.0022]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3694]], device='cuda:0')
delta_t: tensor([[0.7606]], device='cuda:0')
------ ------
policy_loss: 36.655704498291016
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3694]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[37.1386]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[25.7813]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.0775]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0757]], device='cuda:0')
------ ------
delta_t: tensor([[0.7519]], device='cuda:0')
rewards[i]: 0.7521
values[i+1]: tensor([[-0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0018]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.0775]], device='cuda:0')
delta_t: tensor([[0.7519]], device='cuda:0')
------ ------
policy_loss: 48.80762481689453
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.0775]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[53.7738]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[33.2705]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.7681]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.7666]], device='cuda:0')
------ ------
delta_t: tensor([[0.7413]], device='cuda:0')
rewards[i]: 0.7416
values[i+1]: tensor([[-0.0018]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0015]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.7681]], device='cuda:0')
delta_t: tensor([[0.7413]], device='cuda:0')
------ ------
policy_loss: 62.61493682861328
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.7681]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[74.5635]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[41.5795]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.4482]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.4473]], device='cuda:0')
------ ------
delta_t: tensor([[0.7378]], device='cuda:0')
rewards[i]: 0.7384
values[i+1]: tensor([[-0.0015]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0009]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.4482]], device='cuda:0')
delta_t: tensor([[0.7378]], device='cuda:0')
------ ------
policy_loss: 78.05069732666016
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.4482]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[99.9455]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[50.7640]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.1249]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.1244]], device='cuda:0')
------ ------
delta_t: tensor([[0.7411]], device='cuda:0')
rewards[i]: 0.7416
values[i+1]: tensor([[-0.0009]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.1249]], device='cuda:0')
delta_t: tensor([[0.7411]], device='cuda:0')
------ ------
policy_loss: 95.1087875366211
log_probs[i]: tensor([[-2.3975]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.1249]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[130.2527]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[60.6144]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.7855]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.7853]], device='cuda:0')
------ ------
delta_t: tensor([[0.7319]], device='cuda:0')
rewards[i]: 0.7321
values[i+1]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0002]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.7855]], device='cuda:0')
delta_t: tensor([[0.7319]], device='cuda:0')
------ ------
policy_loss: 113.75447845458984
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.7855]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[165.6149]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[70.7244]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.4098]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.4093]], device='cuda:0')
------ ------
delta_t: tensor([[0.7021]], device='cuda:0')
rewards[i]: 0.7019
values[i+1]: tensor([[-0.0002]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.4098]], device='cuda:0')
delta_t: tensor([[0.7021]], device='cuda:0')
------ ------
policy_loss: 133.8955078125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.4098]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[206.4760]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[81.7222]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.0400]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.0392]], device='cuda:0')
------ ------
delta_t: tensor([[0.7143]], device='cuda:0')
rewards[i]: 0.714
values[i+1]: tensor([[-0.0005]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0008]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.0400]], device='cuda:0')
delta_t: tensor([[0.7143]], device='cuda:0')
------ ------
policy_loss: 155.5479736328125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.0400]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[253.0255]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[93.0990]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.6488]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.6477]], device='cuda:0')
------ ------
delta_t: tensor([[0.6992]], device='cuda:0')
rewards[i]: 0.6989
values[i+1]: tensor([[-0.0008]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0010]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.6488]], device='cuda:0')
delta_t: tensor([[0.6992]], device='cuda:0')
------ ------
policy_loss: 178.66175842285156
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.6488]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[305.1060]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[104.1610]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[10.2059]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[10.2041]], device='cuda:0')
------ ------
delta_t: tensor([[0.6536]], device='cuda:0')
rewards[i]: 0.6528
values[i+1]: tensor([[-0.0010]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[-0.0019]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[10.2059]], device='cuda:0')
delta_t: tensor([[0.6536]], device='cuda:0')
------ ------
policy_loss: 203.11123657226562
log_probs[i]: tensor([[-2.3980]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[10.2059]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 203.11123657226562
value_loss: 305.1059875488281
loss: 355.66424560546875



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 6.2207e-04,  3.0747e-07,  2.8852e-06,  8.7383e-07,  2.5999e-06],
        [-2.6490e-02, -5.8229e-05, -1.0412e-04, -3.9770e-05, -1.2508e-04],
        [-3.9860e-04, -1.6457e-06, -1.3570e-06, -6.4674e-07, -2.0888e-06],
        [-7.0120e-02, -1.3241e-04, -2.7670e-04, -1.0356e-04, -3.2781e-04],
        [-2.0381e-01, -2.4819e-04, -9.4355e-04, -2.9188e-04, -9.4873e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-4.3316e-06, -6.8747e-06, -2.6570e-07,  7.0277e-06, -3.8744e-06],
        [ 1.7374e-04,  3.0357e-04,  1.8850e-05, -3.0739e-04,  1.5449e-04],
        [ 2.4746e-06,  4.7885e-06,  4.0084e-07, -4.8039e-06,  2.1776e-06],
        [ 4.6076e-04,  7.9531e-04,  4.7366e-05, -8.0629e-04,  4.1088e-04],
        [ 1.3414e-03,  2.2504e-03,  1.1208e-04, -2.2856e-03,  1.2059e-03]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.0724,  0.0702, -0.0722,  0.0613, -0.0395],
        [ 0.0820, -0.0795,  0.0818, -0.0690,  0.0447],
        [-0.0112,  0.0108, -0.0112,  0.0092, -0.0061],
        [ 0.0047, -0.0045,  0.0047, -0.0039,  0.0025],
        [-0.0030,  0.0029, -0.0030,  0.0024, -0.0017]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 8.5068e-03,  1.4618e-02,  1.2382e-03, -1.4929e-02,  7.1208e-03],
        [ 2.8111e-02,  5.2068e-02,  3.0028e-03, -5.2327e-02,  2.5589e-02],
        [ 2.0943e-02,  3.9020e-02,  3.6286e-03, -3.9242e-02,  1.7908e-02],
        [-2.2877e-02, -4.1886e-02, -2.9288e-03,  4.2171e-02, -2.0175e-02],
        [-1.5208e-02, -2.7187e-02, -1.9897e-03,  2.7517e-02, -1.3209e-02],
        [ 5.5296e-02,  1.0043e-01,  7.2251e-03, -1.0125e-01,  4.8535e-02],
        [ 2.5425e-03,  3.6521e-03,  7.5465e-05, -3.8389e-03,  1.9440e-03],
        [-2.8630e-02, -5.2673e-02, -3.7634e-03,  5.3021e-02, -2.5388e-02],
        [-2.8488e-02, -5.2412e-02, -3.7447e-03,  5.2758e-02, -2.5262e-02],
        [ 8.4690e-03,  1.7111e-02,  1.0246e-03, -1.6964e-02,  8.3574e-03],
        [-2.8665e-02, -5.2738e-02, -3.7681e-03,  5.3086e-02, -2.5420e-02]],
       device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.7301,  3.1830,  0.2274, -3.2040,  1.5342]], device='cuda:0')
--> [loss] 355.66424560546875

---------------------------------- [[#4 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     4.0     | 0.7319 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[8]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   4.], device='cuda:0')
--> [step] action selected: idx4 act0 (stride_width_sub)
--> [step] to state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.690732022685468 acc: 0.6798
[Epoch 7] loss: 2.347874245832643 acc: 0.7259
[Epoch 11] loss: 0.8456142932710136 acc: 0.7497
[Epoch 15] loss: 0.371260959902764 acc: 0.742
[Epoch 19] loss: 0.2774812088269369 acc: 0.7328
[Epoch 23] loss: 0.2193584046719591 acc: 0.7354
[Epoch 27] loss: 0.1967791675225548 acc: 0.7356
[Epoch 31] loss: 0.1772747493975455 acc: 0.749
[Epoch 35] loss: 0.15221690258089823 acc: 0.7414
[Epoch 39] loss: 0.14892706933105007 acc: 0.7372
[Epoch 43] loss: 0.1350481877522662 acc: 0.7326
[Epoch 47] loss: 0.1307537631561046 acc: 0.7396
[Epoch 51] loss: 0.12038449545872792 acc: 0.7339
[Epoch 55] loss: 0.11123338461284289 acc: 0.7408
[Epoch 59] loss: 0.1103945505021371 acc: 0.7279
[Epoch 63] loss: 0.10810164468275989 acc: 0.7369
[Epoch 67] loss: 0.09613600863909583 acc: 0.7374
[Epoch 71] loss: 0.10432001755482344 acc: 0.7415
--> [test] acc: 0.7286
--> [accuracy] finished 0.7286
new state: tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7286
--> [reward] 0.7286
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7286 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.642811306297322 acc: 0.6779
[Epoch 7] loss: 2.3145444161446806 acc: 0.7462
[Epoch 11] loss: 0.8421199244859121 acc: 0.7356
[Epoch 15] loss: 0.3676533185021804 acc: 0.7477
[Epoch 19] loss: 0.26607966582860104 acc: 0.7398
[Epoch 23] loss: 0.22167059545502868 acc: 0.7372
[Epoch 27] loss: 0.1871532948039796 acc: 0.7439
[Epoch 31] loss: 0.17426310932439992 acc: 0.7416
[Epoch 35] loss: 0.15468126093335163 acc: 0.7402
[Epoch 39] loss: 0.13900491304795645 acc: 0.7348
[Epoch 43] loss: 0.1414421207278662 acc: 0.7455
[Epoch 47] loss: 0.11568707299402074 acc: 0.7299
[Epoch 51] loss: 0.125131837116159 acc: 0.7418
[Epoch 55] loss: 0.11776453491105028 acc: 0.7356
[Epoch 59] loss: 0.11569564716766595 acc: 0.7473
[Epoch 63] loss: 0.10580759256592263 acc: 0.7479
[Epoch 67] loss: 0.0970734471724371 acc: 0.7308
[Epoch 71] loss: 0.09208631376042734 acc: 0.7343
--> [test] acc: 0.7412
--> [accuracy] finished 0.7412
new state: tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7412
--> [reward] 0.7412
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7412 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.712706317224771 acc: 0.694
[Epoch 7] loss: 2.3515597125682075 acc: 0.7454
[Epoch 11] loss: 0.8538829710124932 acc: 0.7496
[Epoch 15] loss: 0.384428054651679 acc: 0.7454
[Epoch 19] loss: 0.26882700843122 acc: 0.7485
[Epoch 23] loss: 0.20738069219764826 acc: 0.7481
[Epoch 27] loss: 0.19424067856148458 acc: 0.7424
[Epoch 31] loss: 0.1738099316206506 acc: 0.738
[Epoch 35] loss: 0.1644489611677654 acc: 0.7428
[Epoch 39] loss: 0.14078103411047127 acc: 0.7448
[Epoch 43] loss: 0.13320647610846878 acc: 0.7331
[Epoch 47] loss: 0.1186469659927394 acc: 0.7405
[Epoch 51] loss: 0.1265214135551163 acc: 0.7321
[Epoch 55] loss: 0.11309113958850503 acc: 0.7343
[Epoch 59] loss: 0.11643302476937499 acc: 0.7422
[Epoch 63] loss: 0.10820208299372708 acc: 0.7361
[Epoch 67] loss: 0.09160489680917214 acc: 0.7394
[Epoch 71] loss: 0.09136839571070698 acc: 0.7391
--> [test] acc: 0.7376
--> [accuracy] finished 0.7376
new state: tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7376
--> [reward] 0.7376
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7376 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([640.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.702549916703988 acc: 0.6722
[Epoch 7] loss: 2.3314274650095674 acc: 0.7375
[Epoch 11] loss: 0.8488458632860724 acc: 0.7469
[Epoch 15] loss: 0.37954136222134083 acc: 0.739
[Epoch 19] loss: 0.2639307199722475 acc: 0.7466
[Epoch 23] loss: 0.22180964483800905 acc: 0.742
[Epoch 27] loss: 0.20389899311830167 acc: 0.7407
[Epoch 31] loss: 0.16946409784300287 acc: 0.7363
[Epoch 35] loss: 0.15000958812406376 acc: 0.7327
[Epoch 39] loss: 0.14786060386554092 acc: 0.7398
[Epoch 43] loss: 0.13815574199342362 acc: 0.734
[Epoch 47] loss: 0.12066845543434858 acc: 0.7408
[Epoch 51] loss: 0.12296610865313226 acc: 0.7405
[Epoch 55] loss: 0.12295269132937159 acc: 0.7345
[Epoch 59] loss: 0.10659880131808207 acc: 0.7228
[Epoch 63] loss: 0.09738021112961785 acc: 0.737
[Epoch 67] loss: 0.1030060446444813 acc: 0.7482
[Epoch 71] loss: 0.09837262243832774 acc: 0.7362
--> [test] acc: 0.7372
--> [accuracy] finished 0.7372
new state: tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
new reward: 0.7372
--> [reward] 0.7372
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     3.0     |     1.0      |     3.0     | 0.7372 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   3.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([672.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.010525335588723 acc: 0.6433
[Epoch 7] loss: 2.7938359459037976 acc: 0.7144
[Epoch 11] loss: 1.2251671639740314 acc: 0.7153
[Epoch 15] loss: 0.5093101391001887 acc: 0.7117
[Epoch 19] loss: 0.3431173663067124 acc: 0.7161
[Epoch 23] loss: 0.27733794821526314 acc: 0.7073
[Epoch 27] loss: 0.22202529663177173 acc: 0.7107
[Epoch 31] loss: 0.21234955755121948 acc: 0.7117
[Epoch 35] loss: 0.2017810773021063 acc: 0.7083
[Epoch 39] loss: 0.17003659890009487 acc: 0.7019
[Epoch 43] loss: 0.16774731011861158 acc: 0.7032
[Epoch 47] loss: 0.15625461658505757 acc: 0.7052
[Epoch 51] loss: 0.14826830382199238 acc: 0.7126
[Epoch 55] loss: 0.14006435204549786 acc: 0.7081
[Epoch 59] loss: 0.11933450768356357 acc: 0.7133
[Epoch 63] loss: 0.1376034629571697 acc: 0.7109
[Epoch 67] loss: 0.11657909046301185 acc: 0.6874
[Epoch 71] loss: 0.12014064994459624 acc: 0.7064
--> [test] acc: 0.7036
--> [accuracy] finished 0.7036
new state: tensor([672.,   2.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7036
--> [reward] 0.7036
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     2.0      |     2.0     |     1.0      |     3.0     | 0.7036 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   2.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 4.95356953098341 acc: 0.6222
[Epoch 7] loss: 2.83975276937875 acc: 0.7177
[Epoch 11] loss: 1.3672391434802729 acc: 0.722
[Epoch 15] loss: 0.59356468810183 acc: 0.7053
[Epoch 19] loss: 0.3644509961366501 acc: 0.7175
[Epoch 23] loss: 0.30449570763779954 acc: 0.7117
[Epoch 27] loss: 0.2535348367160353 acc: 0.7152
[Epoch 31] loss: 0.23637206787410217 acc: 0.7139
[Epoch 35] loss: 0.2108156555058325 acc: 0.7049
[Epoch 39] loss: 0.1929962528557481 acc: 0.715
[Epoch 43] loss: 0.18798277998471732 acc: 0.7112
[Epoch 47] loss: 0.17736710696254887 acc: 0.7197
[Epoch 51] loss: 0.15994467363094964 acc: 0.7118
[Epoch 55] loss: 0.15886972870444283 acc: 0.7129
[Epoch 59] loss: 0.15392808446748768 acc: 0.7054
[Epoch 63] loss: 0.14104011661344495 acc: 0.712
[Epoch 67] loss: 0.13566825234109675 acc: 0.7078
[Epoch 71] loss: 0.12964202332570304 acc: 0.709
--> [test] acc: 0.7158
--> [accuracy] finished 0.7158
new state: tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7158
--> [reward] 0.7158
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7158 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[2]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx1 act0 (filter_height_sub)
--> [step] to state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.043360188184187 acc: 0.6429
[Epoch 7] loss: 2.913709549952651 acc: 0.7192
[Epoch 11] loss: 1.4240134492173524 acc: 0.7249
[Epoch 15] loss: 0.6136903813123094 acc: 0.7119
[Epoch 19] loss: 0.3910135810842256 acc: 0.709
[Epoch 23] loss: 0.3126743905832205 acc: 0.7089
[Epoch 27] loss: 0.26033268141491184 acc: 0.7035
[Epoch 31] loss: 0.23721554958263932 acc: 0.7107
[Epoch 35] loss: 0.21339803193564838 acc: 0.7085
[Epoch 39] loss: 0.20424049999207602 acc: 0.6979
[Epoch 43] loss: 0.1910912750879555 acc: 0.7058
[Epoch 47] loss: 0.1816828474676346 acc: 0.7085
[Epoch 51] loss: 0.16439436768572255 acc: 0.7068
[Epoch 55] loss: 0.17196317988059595 acc: 0.7093
[Epoch 59] loss: 0.15464988837966606 acc: 0.7064
[Epoch 63] loss: 0.14006023898082393 acc: 0.706
[Epoch 67] loss: 0.13987894114785734 acc: 0.7009
[Epoch 71] loss: 0.1496221312593259 acc: 0.7069
--> [test] acc: 0.7125
--> [accuracy] finished 0.7125
new state: tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7125
--> [reward] 0.7125
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  672.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7125 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([672.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([640.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.002170142920121 acc: 0.6272
[Epoch 7] loss: 2.866017287573241 acc: 0.71
[Epoch 11] loss: 1.4115707342658201 acc: 0.7232
[Epoch 15] loss: 0.62339413249889 acc: 0.7072
[Epoch 19] loss: 0.38207268365718366 acc: 0.7098
[Epoch 23] loss: 0.30763000807703456 acc: 0.6968
[Epoch 27] loss: 0.27641080519246397 acc: 0.7154
[Epoch 31] loss: 0.23395294753258186 acc: 0.7172
[Epoch 35] loss: 0.22304952077691437 acc: 0.6973
[Epoch 39] loss: 0.2055718441734381 acc: 0.7141
[Epoch 43] loss: 0.18034380354473126 acc: 0.7062
[Epoch 47] loss: 0.1763024034462107 acc: 0.7035
[Epoch 51] loss: 0.18274700906737457 acc: 0.706
[Epoch 55] loss: 0.16338355320355738 acc: 0.7092
[Epoch 59] loss: 0.14687287639540708 acc: 0.7075
[Epoch 63] loss: 0.14888742191912344 acc: 0.7103
[Epoch 67] loss: 0.13436137587356065 acc: 0.7125
[Epoch 71] loss: 0.14020907446441938 acc: 0.7011
--> [test] acc: 0.7083
--> [accuracy] finished 0.7083
new state: tensor([640.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7083
--> [reward] 0.7083
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  640.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7083 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([640.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.046630183144298 acc: 0.6493
[Epoch 7] loss: 2.928027847005278 acc: 0.7232
[Epoch 11] loss: 1.4823036705479598 acc: 0.7221
[Epoch 15] loss: 0.660242694884996 acc: 0.7183
[Epoch 19] loss: 0.39568860052853744 acc: 0.7038
[Epoch 23] loss: 0.31922819941302244 acc: 0.7114
[Epoch 27] loss: 0.256468353273771 acc: 0.6999
[Epoch 31] loss: 0.25283805352380817 acc: 0.7095
[Epoch 35] loss: 0.22073129973972164 acc: 0.7063
[Epoch 39] loss: 0.2002074165183984 acc: 0.6989
[Epoch 43] loss: 0.19245492626705665 acc: 0.6967
[Epoch 47] loss: 0.17987188151406358 acc: 0.7052
[Epoch 51] loss: 0.17250947379495216 acc: 0.7078
[Epoch 55] loss: 0.1607824995056214 acc: 0.706
[Epoch 59] loss: 0.15180359024446113 acc: 0.7154
[Epoch 63] loss: 0.1577177304724503 acc: 0.706
[Epoch 67] loss: 0.1342493475121839 acc: 0.7118
[Epoch 71] loss: 0.13775336643790498 acc: 0.7188
--> [test] acc: 0.7192
--> [accuracy] finished 0.7192
new state: tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7192
--> [reward] 0.7192
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7192 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.037982940064062 acc: 0.6515
[Epoch 7] loss: 2.9101187599741896 acc: 0.7172
[Epoch 11] loss: 1.4687374603108068 acc: 0.7234
[Epoch 15] loss: 0.634318943923849 acc: 0.7078
[Epoch 19] loss: 0.3927925794868899 acc: 0.7105
[Epoch 23] loss: 0.30992098532312207 acc: 0.7084
[Epoch 27] loss: 0.26990515302123547 acc: 0.7116
[Epoch 31] loss: 0.24306390580513973 acc: 0.7088
[Epoch 35] loss: 0.21466257789498552 acc: 0.7052
[Epoch 39] loss: 0.21387923384071006 acc: 0.7043
[Epoch 43] loss: 0.1928454030071721 acc: 0.7117
[Epoch 47] loss: 0.17339675948071434 acc: 0.7073
[Epoch 51] loss: 0.1712778107920552 acc: 0.7132
[Epoch 55] loss: 0.16294215622005503 acc: 0.7023
[Epoch 59] loss: 0.16400808145356416 acc: 0.7166
[Epoch 63] loss: 0.14646219943895403 acc: 0.706
[Epoch 67] loss: 0.13873166391861808 acc: 0.7128
[Epoch 71] loss: 0.13965294662091282 acc: 0.6985
--> [test] acc: 0.7075
--> [accuracy] finished 0.7075
new state: tensor([576.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7075
--> [reward] 0.7075
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7075 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.033843046716412 acc: 0.6476
[Epoch 7] loss: 2.8799588059068033 acc: 0.7041
[Epoch 11] loss: 1.4124729855705405 acc: 0.7258
[Epoch 15] loss: 0.6198955716855843 acc: 0.7174
[Epoch 19] loss: 0.38161892080893905 acc: 0.7142
[Epoch 23] loss: 0.31743584936746705 acc: 0.7163
[Epoch 27] loss: 0.2634655866924378 acc: 0.7147
[Epoch 31] loss: 0.239525489218514 acc: 0.7069
[Epoch 35] loss: 0.21486120081156532 acc: 0.7027
[Epoch 39] loss: 0.21478696118759186 acc: 0.7128
[Epoch 43] loss: 0.1899709386531921 acc: 0.7167
[Epoch 47] loss: 0.18134960240643958 acc: 0.7066
[Epoch 51] loss: 0.16731854485909994 acc: 0.7141
[Epoch 55] loss: 0.1615856013348912 acc: 0.7025
[Epoch 59] loss: 0.1571663061056829 acc: 0.7066
[Epoch 63] loss: 0.15118450224768762 acc: 0.7058
[Epoch 67] loss: 0.14344432008216906 acc: 0.72
[Epoch 71] loss: 0.13501263903977964 acc: 0.7094
--> [test] acc: 0.7058
--> [accuracy] finished 0.7058
new state: tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
new reward: 0.7058
--> [reward] 0.7058
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     1.0      |     3.0     | 0.7058 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   1.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([608.,   1.,   2.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.584577994578329 acc: 0.5665
[Epoch 7] loss: 3.554325601405195 acc: 0.6678
[Epoch 11] loss: 2.157847829906227 acc: 0.6678
[Epoch 15] loss: 1.0932768844925533 acc: 0.6648
[Epoch 19] loss: 0.6064162578986353 acc: 0.6624
[Epoch 23] loss: 0.4475471344835999 acc: 0.6648
[Epoch 27] loss: 0.35509656213433544 acc: 0.658
[Epoch 31] loss: 0.3236684671167256 acc: 0.6629
[Epoch 35] loss: 0.29135187092901726 acc: 0.6591
[Epoch 39] loss: 0.26954896495108255 acc: 0.6618
[Epoch 43] loss: 0.2501781047833964 acc: 0.6608
[Epoch 47] loss: 0.22433649846936202 acc: 0.6648
[Epoch 51] loss: 0.22669170741849315 acc: 0.6531
[Epoch 55] loss: 0.21469113617763877 acc: 0.6625
[Epoch 59] loss: 0.19627516831883499 acc: 0.6592
[Epoch 63] loss: 0.18758644420258186 acc: 0.657
[Epoch 67] loss: 0.18424902435587456 acc: 0.6612
[Epoch 71] loss: 0.18764065035864178 acc: 0.6571
--> [test] acc: 0.6583
--> [accuracy] finished 0.6583
new state: tensor([608.,   1.,   2.,   2.,   3.], device='cuda:0')
new reward: 0.6583
--> [reward] 0.6583
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     2.0      |     3.0     | 0.6583 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  640.0   |     2.0      |     1.0     |     1.0      |     3.0     | 0.6428 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 5.996437841669068 acc: 0.541
[Epoch 7] loss: 4.293972866462015 acc: 0.6111
[Epoch 11] loss: 3.1062627741900246 acc: 0.614
[Epoch 15] loss: 1.9548368407865924 acc: 0.6145
[Epoch 19] loss: 1.0797750012344107 acc: 0.6002
[Epoch 23] loss: 0.7180868046610709 acc: 0.6029
[Epoch 27] loss: 0.5421729776960658 acc: 0.6006
[Epoch 31] loss: 0.45406014683282436 acc: 0.5992
[Epoch 35] loss: 0.41992679361701774 acc: 0.5964
[Epoch 39] loss: 0.3972607317030468 acc: 0.5972
[Epoch 43] loss: 0.3511335825800057 acc: 0.5993
[Epoch 47] loss: 0.34527689847818877 acc: 0.586
[Epoch 51] loss: 0.3243957251653342 acc: 0.592
[Epoch 55] loss: 0.3046981274231296 acc: 0.5905
[Epoch 59] loss: 0.28777602799427326 acc: 0.5873
[Epoch 63] loss: 0.28344770089325394 acc: 0.6001
[Epoch 67] loss: 0.25791950736080516 acc: 0.5959
[Epoch 71] loss: 0.27462602395783453 acc: 0.5998
--> [test] acc: 0.5949
--> [accuracy] finished 0.5949
new state: tensor([608.,   1.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.5949
--> [reward] 0.5949
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.5949 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.5949 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.00284519189459 acc: 0.5306
[Epoch 7] loss: 4.305907856015598 acc: 0.599
[Epoch 11] loss: 3.1426067028356637 acc: 0.6022
[Epoch 15] loss: 1.9957551636430613 acc: 0.6091
[Epoch 19] loss: 1.126257802919506 acc: 0.6059
[Epoch 23] loss: 0.7321600812580198 acc: 0.6082
[Epoch 27] loss: 0.5564134867571275 acc: 0.6001
[Epoch 31] loss: 0.4910272464723043 acc: 0.5961
[Epoch 35] loss: 0.43266771734237214 acc: 0.5989
[Epoch 39] loss: 0.38789483290546767 acc: 0.6066
[Epoch 43] loss: 0.3630431480324634 acc: 0.6031
[Epoch 47] loss: 0.3535106493388791 acc: 0.603
[Epoch 51] loss: 0.341060309566062 acc: 0.591
[Epoch 55] loss: 0.31755788478038044 acc: 0.5953
[Epoch 59] loss: 0.30220061315275976 acc: 0.6026
[Epoch 63] loss: 0.27474453586541936 acc: 0.5923
[Epoch 67] loss: 0.2695336834476341 acc: 0.5963
[Epoch 71] loss: 0.27302980087006756 acc: 0.5977
--> [test] acc: 0.595
--> [accuracy] finished 0.595
new state: tensor([576.,   1.,   1.,   2.,   3.], device='cuda:0')
new reward: 0.595
--> [reward] 0.595
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.595  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     2.0      |     3.0     | 0.5949 |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0914, 0.0909, 0.0904, 0.0911, 0.0909, 0.0910, 0.0909, 0.0905,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3974, -2.3979, -2.3984, -2.3977, -2.3979, -2.3978, -2.3979,
         -2.3983, -2.3979, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   2.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.786986326317653 acc: 0.4515
[Epoch 7] loss: 5.555535671656089 acc: 0.4924
[Epoch 11] loss: 4.768977300132937 acc: 0.5089
[Epoch 15] loss: 3.88165950729414 acc: 0.501
[Epoch 19] loss: 2.803389046808033 acc: 0.4978
[Epoch 23] loss: 1.8148109830172776 acc: 0.4812
[Epoch 27] loss: 1.204904246989571 acc: 0.4814
[Epoch 31] loss: 0.8964689170651119 acc: 0.4743
[Epoch 35] loss: 0.7737253157569625 acc: 0.4695
[Epoch 39] loss: 0.7012364807945993 acc: 0.4679
[Epoch 43] loss: 0.6290193191679466 acc: 0.4648
[Epoch 47] loss: 0.5625171306664529 acc: 0.4717
[Epoch 51] loss: 0.535659817430903 acc: 0.4724
[Epoch 55] loss: 0.53176000314143 acc: 0.4688
[Epoch 59] loss: 0.4774633881962284 acc: 0.4803
[Epoch 63] loss: 0.47543794322577887 acc: 0.4706
[Epoch 67] loss: 0.4413645465898773 acc: 0.4696
[Epoch 71] loss: 0.42903715556920946 acc: 0.471
--> [test] acc: 0.473
--> [accuracy] finished 0.473
new state: tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.473
--> [reward] 0.473
--> [print] for vars during loss calculation

[i]: 14
------ ------
value_loss: tensor([[0.1120]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[0.2239]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[0.4732]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[0.4774]], device='cuda:0')
------ ------
delta_t: tensor([[0.4732]], device='cuda:0')
rewards[i]: 0.473
values[i+1]: tensor([[0.0044]], device='cuda:0')
values[i]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[0.4732]], device='cuda:0')
delta_t: tensor([[0.4732]], device='cuda:0')
------ ------
policy_loss: 1.1106997728347778
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[0.4732]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 13
------ ------
value_loss: tensor([[0.6777]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[1.1315]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.0637]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.0676]], device='cuda:0')
------ ------
delta_t: tensor([[0.5953]], device='cuda:0')
rewards[i]: 0.595
values[i+1]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0039]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.0637]], device='cuda:0')
delta_t: tensor([[0.5953]], device='cuda:0')
------ ------
policy_loss: 3.6373684406280518
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.0637]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 12
------ ------
value_loss: tensor([[2.0357]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[2.7160]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[1.6480]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[1.6518]], device='cuda:0')
------ ------
delta_t: tensor([[0.5949]], device='cuda:0')
rewards[i]: 0.5949
values[i+1]: tensor([[0.0039]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[1.6480]], device='cuda:0')
delta_t: tensor([[0.5949]], device='cuda:0')
------ ------
policy_loss: 7.5648369789123535
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[1.6480]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 11
------ ------
value_loss: tensor([[4.6568]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[5.2421]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.2896]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.2936]], device='cuda:0')
------ ------
delta_t: tensor([[0.6580]], device='cuda:0')
rewards[i]: 0.6583
values[i+1]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0040]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.2896]], device='cuda:0')
delta_t: tensor([[0.6580]], device='cuda:0')
------ ------
policy_loss: 13.030998229980469
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.2896]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 10
------ ------
value_loss: tensor([[9.0740]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[8.8344]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[2.9723]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[2.9765]], device='cuda:0')
------ ------
delta_t: tensor([[0.7056]], device='cuda:0')
rewards[i]: 0.7058
values[i+1]: tensor([[0.0040]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[2.9723]], device='cuda:0')
delta_t: tensor([[0.7056]], device='cuda:0')
------ ------
policy_loss: 20.13275909423828
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[2.9723]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 9
------ ------
value_loss: tensor([[15.7367]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[13.3254]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[3.6504]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[3.6542]], device='cuda:0')
------ ------
delta_t: tensor([[0.7079]], device='cuda:0')
rewards[i]: 0.7075
values[i+1]: tensor([[0.0042]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[3.6504]], device='cuda:0')
delta_t: tensor([[0.7079]], device='cuda:0')
------ ------
policy_loss: 28.861785888671875
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[3.6504]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 8
------ ------
value_loss: tensor([[25.1261]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[18.7790]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.3335]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[4.3369]], device='cuda:0')
------ ------
delta_t: tensor([[0.7196]], device='cuda:0')
rewards[i]: 0.7192
values[i+1]: tensor([[0.0038]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0034]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.3335]], device='cuda:0')
delta_t: tensor([[0.7196]], device='cuda:0')
------ ------
policy_loss: 39.228694915771484
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.3335]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 7
------ ------
value_loss: tensor([[37.6194]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[24.9865]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[4.9986]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.0018]], device='cuda:0')
------ ------
delta_t: tensor([[0.7085]], device='cuda:0')
rewards[i]: 0.7083
values[i+1]: tensor([[0.0034]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[4.9986]], device='cuda:0')
delta_t: tensor([[0.7085]], device='cuda:0')
------ ------
policy_loss: 51.190589904785156
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[4.9986]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 6
------ ------
value_loss: tensor([[53.6437]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[32.0487]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[5.6612]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[5.6643]], device='cuda:0')
------ ------
delta_t: tensor([[0.7125]], device='cuda:0')
rewards[i]: 0.7125
values[i+1]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[5.6612]], device='cuda:0')
delta_t: tensor([[0.7125]], device='cuda:0')
------ ------
policy_loss: 64.74161529541016
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[5.6612]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 5
------ ------
value_loss: tensor([[73.6164]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[39.9453]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.3202]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.3234]], device='cuda:0')
------ ------
delta_t: tensor([[0.7157]], device='cuda:0')
rewards[i]: 0.7158
values[i+1]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0032]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.3202]], device='cuda:0')
delta_t: tensor([[0.7157]], device='cuda:0')
------ ------
policy_loss: 79.87303924560547
log_probs[i]: tensor([[-2.3979]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.3202]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 4
------ ------
value_loss: tensor([[97.8417]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[48.4507]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[6.9607]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[6.9638]], device='cuda:0')
------ ------
delta_t: tensor([[0.7036]], device='cuda:0')
rewards[i]: 0.7036
values[i+1]: tensor([[0.0032]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[6.9607]], device='cuda:0')
delta_t: tensor([[0.7036]], device='cuda:0')
------ ------
policy_loss: 96.53839111328125
log_probs[i]: tensor([[-2.3977]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[6.9607]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 3
------ ------
value_loss: tensor([[126.9357]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[58.1879]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[7.6281]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[7.6314]], device='cuda:0')
------ ------
delta_t: tensor([[0.7371]], device='cuda:0')
rewards[i]: 0.7372
values[i+1]: tensor([[0.0031]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0033]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[7.6281]], device='cuda:0')
delta_t: tensor([[0.7371]], device='cuda:0')
------ ------
policy_loss: 114.80205535888672
log_probs[i]: tensor([[-2.3974]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[7.6281]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 2
------ ------
value_loss: tensor([[161.2961]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[68.7209]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.2898]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.2926]], device='cuda:0')
------ ------
delta_t: tensor([[0.7380]], device='cuda:0')
rewards[i]: 0.7376
values[i+1]: tensor([[0.0033]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0028]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.2898]], device='cuda:0')
delta_t: tensor([[0.7380]], device='cuda:0')
------ ------
policy_loss: 134.6552276611328
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.2898]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 1
------ ------
value_loss: tensor([[201.3370]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[80.0818]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[8.9488]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[8.9509]], device='cuda:0')
------ ------
delta_t: tensor([[0.7419]], device='cuda:0')
rewards[i]: 0.7412
values[i+1]: tensor([[0.0028]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[8.9488]], device='cuda:0')
delta_t: tensor([[0.7419]], device='cuda:0')
------ ------
policy_loss: 156.0889892578125
log_probs[i]: tensor([[-2.3978]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[8.9488]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)

[i]: 0
------ ------
value_loss: tensor([[247.3126]], device='cuda:0', grad_fn=<AddBackward0>)
advantage.pow(2): tensor([[91.9511]], device='cuda:0', grad_fn=<PowBackward0>)
advantage: tensor([[9.5891]], device='cuda:0', grad_fn=<SubBackward0>)
R: tensor([[9.5900]], device='cuda:0')
------ ------
delta_t: tensor([[0.7298]], device='cuda:0')
rewards[i]: 0.7286
values[i+1]: tensor([[0.0021]], device='cuda:0', grad_fn=<AddmmBackward>)
values[i]: tensor([[0.0009]], device='cuda:0', grad_fn=<AddmmBackward>)
------ ------
gae: tensor([[9.5891]], device='cuda:0')
delta_t: tensor([[0.7298]], device='cuda:0')
------ ------
policy_loss: 179.06297302246094
log_probs[i]: tensor([[-2.3983]], device='cuda:0', grad_fn=<GatherBackward>)
gae: tensor([[9.5891]], device='cuda:0')
entropies[i]: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
----------------------- 
policy_loss: 179.06297302246094
value_loss: 247.31260681152344
loss: 302.7192687988281



--> [print] for grads need to update
model.Wai.weight.grad tensor([[ 3.5814e-04,  5.2963e-07,  7.0788e-08,  9.3972e-07,  7.4588e-07],
        [-1.4437e-02, -3.3147e-05, -3.4235e-05, -2.8389e-05, -5.0834e-05],
        [-3.7821e-04, -1.0506e-06, -1.8301e-06, -5.2270e-07, -2.0792e-06],
        [-1.7608e-02, -3.7487e-05,  1.6994e-05, -4.6006e-05, -8.3832e-06],
        [-1.4062e-02, -2.6626e-05,  2.9360e-04, -9.3481e-05,  2.7149e-04]],
       device='cuda:0')
model.Wh.weight.grad  tensor([[-3.0953e-06, -4.4871e-06,  7.2966e-08,  4.5902e-06, -2.6840e-06],
        [ 1.0746e-04,  1.7246e-04,  4.0539e-06, -1.7495e-04,  9.4092e-05],
        [ 2.3159e-06,  4.3137e-06,  3.0837e-07, -4.3282e-06,  2.0756e-06],
        [ 1.6343e-04,  2.2295e-04, -7.7047e-06, -2.2929e-04,  1.3918e-04],
        [ 2.9630e-04,  2.4386e-04, -7.0033e-05, -2.6546e-04,  2.3510e-04]],
       device='cuda:0')
model.att.weight.grad tensor([[-0.0126,  0.0116, -0.0126,  0.0089, -0.0048],
        [ 0.0537, -0.0520,  0.0536, -0.0450,  0.0289],
        [-0.0360,  0.0353, -0.0360,  0.0314, -0.0208],
        [ 0.0033, -0.0032,  0.0033, -0.0028,  0.0018],
        [-0.0084,  0.0083, -0.0084,  0.0075, -0.0051]], device='cuda:0')
model.inputs.grad     None
player.hx.grad        None
player.cx.grad        None
model.Uv.grad         None
model.Uh.grad         None
model.Uhv.grad        None
model.att_.grad       None
model.alpha.grad      None
model.zt.grad         None
model.actor_linear.weight.grad   tensor([[ 0.0636,  0.1103,  0.0056, -0.1113,  0.0565],
        [ 0.0139,  0.0235,  0.0006, -0.0237,  0.0122],
        [ 0.0207,  0.0356,  0.0015, -0.0360,  0.0185],
        [-0.0262, -0.0457, -0.0022,  0.0461, -0.0233],
        [ 0.0065,  0.0111,  0.0002, -0.0111,  0.0059],
        [-0.0264, -0.0460, -0.0022,  0.0464, -0.0234],
        [-0.0264, -0.0461, -0.0023,  0.0464, -0.0235],
        [-0.0164, -0.0288, -0.0017,  0.0290, -0.0144],
        [ 0.0113,  0.0220,  0.0023, -0.0219,  0.0100],
        [-0.0264, -0.0460, -0.0022,  0.0464, -0.0235],
        [ 0.0059,  0.0102,  0.0006, -0.0103,  0.0050]], device='cuda:0')
model.critic_linear.weight.grad  tensor([[ 1.5959,  2.7824,  0.1361, -2.8037,  1.4181]], device='cuda:0')
--> [loss] 302.7192687988281

---------------------------------- [[#5 iteration]] ----------------------------------
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.473  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  576.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.473  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.782348939829775 acc: 0.4512
[Epoch 7] loss: 5.522789072197721 acc: 0.485
[Epoch 11] loss: 4.756141616407867 acc: 0.5108
[Epoch 15] loss: 3.889405967786794 acc: 0.5017
[Epoch 19] loss: 2.836134026498746 acc: 0.4951
[Epoch 23] loss: 1.8460596091378376 acc: 0.4879
[Epoch 27] loss: 1.2254645365202212 acc: 0.479
[Epoch 31] loss: 0.9105931710442314 acc: 0.4677
[Epoch 35] loss: 0.8009530525568807 acc: 0.4746
[Epoch 39] loss: 0.6719927520245847 acc: 0.4802
[Epoch 43] loss: 0.6139450664239009 acc: 0.4788
[Epoch 47] loss: 0.5891216066058563 acc: 0.4722
[Epoch 51] loss: 0.5330987203380336 acc: 0.4776
[Epoch 55] loss: 0.5252831459302655 acc: 0.4719
[Epoch 59] loss: 0.4869985906955074 acc: 0.4722
[Epoch 63] loss: 0.4746449986696624 acc: 0.4755
[Epoch 67] loss: 0.4251879109546085 acc: 0.481
[Epoch 71] loss: 0.4075181236314347 acc: 0.4724
--> [test] acc: 0.47
--> [accuracy] finished 0.47
new state: tensor([608.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.47
--> [reward] 0.47
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[0]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx0 act0 (of_filter_sub)
--> [step] to state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.767315307236694 acc: 0.4542
[Epoch 7] loss: 5.525891588167156 acc: 0.4846
[Epoch 11] loss: 4.739256105459559 acc: 0.5071
[Epoch 15] loss: 3.8552043430335687 acc: 0.5115
[Epoch 19] loss: 2.7934473147020316 acc: 0.499
[Epoch 23] loss: 1.8031510815519811 acc: 0.4883
[Epoch 27] loss: 1.2208171903019975 acc: 0.4823
[Epoch 31] loss: 0.9210781355000213 acc: 0.4763
[Epoch 35] loss: 0.762432501596563 acc: 0.4864
[Epoch 39] loss: 0.6779675031238047 acc: 0.4793
[Epoch 43] loss: 0.6193185784589604 acc: 0.4805
[Epoch 47] loss: 0.5904313334075691 acc: 0.4807
[Epoch 51] loss: 0.5250345553221453 acc: 0.4843
[Epoch 55] loss: 0.5175635577191401 acc: 0.4782
[Epoch 59] loss: 0.46632854899158105 acc: 0.4804
[Epoch 63] loss: 0.46849338907052945 acc: 0.4855
[Epoch 67] loss: 0.43690463773372684 acc: 0.4807
[Epoch 71] loss: 0.42323684819814417 acc: 0.4808
--> [test] acc: 0.4865
--> [accuracy] finished 0.4865
new state: tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
new reward: 0.4865
--> [reward] 0.4865
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     3.0      |     3.0     | 0.4865 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[7]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   3.,   3.], device='cuda:0')
--> [step] action selected: idx3 act1 (stride_height_add)
--> [step] to state tensor([576.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.604592756237215 acc: 0.4733
[Epoch 7] loss: 5.316730294386139 acc: 0.51
[Epoch 11] loss: 4.523846875066343 acc: 0.529
[Epoch 15] loss: 3.6291557486404846 acc: 0.5272
[Epoch 19] loss: 2.565233098119116 acc: 0.5214
[Epoch 23] loss: 1.6293012548971664 acc: 0.5147
[Epoch 27] loss: 1.0581911821347063 acc: 0.5041
[Epoch 31] loss: 0.7975388894052914 acc: 0.5045
[Epoch 35] loss: 0.6679290298305814 acc: 0.5092
[Epoch 39] loss: 0.5865731053887997 acc: 0.4981
[Epoch 43] loss: 0.5599521324371971 acc: 0.493
[Epoch 47] loss: 0.4910942538238852 acc: 0.4935
[Epoch 51] loss: 0.4722949174635322 acc: 0.493
[Epoch 55] loss: 0.46556858635028764 acc: 0.5039
[Epoch 59] loss: 0.4378478538406932 acc: 0.4948
[Epoch 63] loss: 0.38675251950407424 acc: 0.4962
[Epoch 67] loss: 0.4028446754069089 acc: 0.5026
[Epoch 71] loss: 0.35973877196088244 acc: 0.4973
--> [test] acc: 0.492
--> [accuracy] finished 0.492
new state: tensor([576.,   1.,   1.,   4.,   3.], device='cuda:0')
new reward: 0.492
--> [reward] 0.492
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     1.0     |     4.0      |     3.0     | 0.492  |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([576.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.17065831797812 acc: 0.5213
[Epoch 7] loss: 4.65122285325204 acc: 0.561
[Epoch 11] loss: 3.5740483080029795 acc: 0.5684
[Epoch 15] loss: 2.3713122025856275 acc: 0.5719
[Epoch 19] loss: 1.3554117164152968 acc: 0.5641
[Epoch 23] loss: 0.8082708270691544 acc: 0.5729
[Epoch 27] loss: 0.597662641564408 acc: 0.5507
[Epoch 31] loss: 0.5028113255377316 acc: 0.5443
[Epoch 35] loss: 0.4525453643706601 acc: 0.5573
[Epoch 39] loss: 0.39694879208322226 acc: 0.5605
[Epoch 43] loss: 0.37384725161506543 acc: 0.5504
[Epoch 47] loss: 0.34072145697234385 acc: 0.5479
[Epoch 51] loss: 0.32641215797971046 acc: 0.5582
[Epoch 55] loss: 0.3041428430148822 acc: 0.5586
[Epoch 59] loss: 0.31113502335594134 acc: 0.5626
[Epoch 63] loss: 0.27123473564405803 acc: 0.5583
[Epoch 67] loss: 0.2783392966555817 acc: 0.5514
[Epoch 71] loss: 0.25435142077939094 acc: 0.5417
--> [test] acc: 0.5511
--> [accuracy] finished 0.5511
new state: tensor([576.,   1.,   2.,   4.,   3.], device='cuda:0')
new reward: 0.5511
--> [reward] 0.5511
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  576.0   |     1.0      |     2.0     |     4.0      |     3.0     | 0.5511 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[1]], device='cuda:0')
--> [step] from state tensor([576.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx0 act1 (of_filter_add)
--> [step] to state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.175258413909951 acc: 0.5156
[Epoch 7] loss: 4.633222080100223 acc: 0.566
[Epoch 11] loss: 3.525598922958764 acc: 0.5759
[Epoch 15] loss: 2.3219339121180727 acc: 0.5793
[Epoch 19] loss: 1.309375061403455 acc: 0.5695
[Epoch 23] loss: 0.8134351793648033 acc: 0.5706
[Epoch 27] loss: 0.5943743646659357 acc: 0.5644
[Epoch 31] loss: 0.4796634684686008 acc: 0.5617
[Epoch 35] loss: 0.44963047292340746 acc: 0.5611
[Epoch 39] loss: 0.3945100123415251 acc: 0.5742
[Epoch 43] loss: 0.37784702182673585 acc: 0.5636
[Epoch 47] loss: 0.3318785456773799 acc: 0.5554
[Epoch 51] loss: 0.324086525406012 acc: 0.5648
[Epoch 55] loss: 0.30493665186454877 acc: 0.5629
[Epoch 59] loss: 0.2905986997920572 acc: 0.5601
[Epoch 63] loss: 0.27420339990130926 acc: 0.5483
[Epoch 67] loss: 0.2713740357648 acc: 0.5608
[Epoch 71] loss: 0.2587196327617292 acc: 0.5595
--> [test] acc: 0.5614
--> [accuracy] finished 0.5614
new state: tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
new reward: 0.5614
--> [reward] 0.5614
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     2.0     |     4.0      |     3.0     | 0.5614 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[4]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act0 (filter_width_sub)
--> [step] to state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.617557332643767 acc: 0.4494
[Epoch 7] loss: 5.328535925854197 acc: 0.501
[Epoch 11] loss: 4.514997204398865 acc: 0.5338
[Epoch 15] loss: 3.598308163378245 acc: 0.5271
[Epoch 19] loss: 2.5157762001390043 acc: 0.5071
[Epoch 23] loss: 1.585253702328943 acc: 0.5108
[Epoch 27] loss: 1.0388237938970861 acc: 0.5047
[Epoch 31] loss: 0.8092473158446114 acc: 0.5003
[Epoch 35] loss: 0.6680520991497028 acc: 0.4951
[Epoch 39] loss: 0.5755104333105142 acc: 0.5006
[Epoch 43] loss: 0.5406301099082926 acc: 0.497
[Epoch 47] loss: 0.49945487225871255 acc: 0.4946
[Epoch 51] loss: 0.4697900869481056 acc: 0.5046
[Epoch 55] loss: 0.4397151985389116 acc: 0.4953
[Epoch 59] loss: 0.4157286243408423 acc: 0.4966
[Epoch 63] loss: 0.41210803936909685 acc: 0.4942
[Epoch 67] loss: 0.38956086373056675 acc: 0.494
[Epoch 71] loss: 0.36307193011598055 acc: 0.4939
--> [test] acc: 0.4885
--> [accuracy] finished 0.4885
new state: tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
new reward: 0.4885
--> [reward] 0.4885
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     4.0      |     3.0     | 0.4885 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[10]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx5 act0 (none)
--> [step] to state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.620229959487915 acc: 0.4655
[Epoch 7] loss: 5.354968489435932 acc: 0.4864
[Epoch 11] loss: 4.563249225659138 acc: 0.5284
[Epoch 15] loss: 3.673091523025347 acc: 0.523
[Epoch 19] loss: 2.611559397271832 acc: 0.5137
[Epoch 23] loss: 1.6580186044925924 acc: 0.5027
[Epoch 27] loss: 1.0927138149433429 acc: 0.4999
[Epoch 31] loss: 0.8307930771594919 acc: 0.4987
[Epoch 35] loss: 0.6749473040556664 acc: 0.4933
[Epoch 39] loss: 0.6107741864707769 acc: 0.4986
[Epoch 43] loss: 0.5686799835537553 acc: 0.5038
[Epoch 47] loss: 0.5084670059992682 acc: 0.4906
[Epoch 51] loss: 0.4664265258318704 acc: 0.4974
[Epoch 55] loss: 0.4702893339450021 acc: 0.4973
[Epoch 59] loss: 0.4378377212702161 acc: 0.5009
[Epoch 63] loss: 0.3970893554108413 acc: 0.4969
[Epoch 67] loss: 0.40737854344698854 acc: 0.4963
[Epoch 71] loss: 0.37183836324359565 acc: 0.4984
--> [test] acc: 0.4957
--> [accuracy] finished 0.4957
new state: tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
new reward: 0.4957
--> [reward] 0.4957
+---------+----------+--------------+-------------+--------------+-------------+--------+
|   Type  | OfFilter | FilterHeight | FilterWidth | StrideHeight | StrideWidth |  Acc   |
+---------+----------+--------------+-------------+--------------+-------------+--------+
| current |  608.0   |     1.0      |     1.0     |     4.0      |     3.0     | 0.4957 |
|   best  |  672.0   |     2.0      |     4.0     |     1.0      |     4.0     | 0.7607 |
|  worst  |  608.0   |     1.0      |     1.0     |     3.0      |     3.0     |  0.47  |
+---------+----------+--------------+-------------+--------------+-------------+--------+

--> [print] for vars during action selection
prob: tensor([[0.0910, 0.0915, 0.0909, 0.0903, 0.0912, 0.0909, 0.0910, 0.0908, 0.0904,
         0.0909, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward>)
log prob: tensor([[-2.3978, -2.3973, -2.3979, -2.3985, -2.3976, -2.3979, -2.3978, -2.3980,
         -2.3984, -2.3980, -2.3978]], device='cuda:0',
       grad_fn=<LogSoftmaxBackward>)
entropy: tensor([2.3979], device='cuda:0', grad_fn=<NegBackward>)
action: tensor([[5]], device='cuda:0')
--> [step] from state tensor([608.,   1.,   1.,   4.,   3.], device='cuda:0')
--> [step] action selected: idx2 act1 (filter_width_add)
--> [step] to state tensor([608.,   1.,   2.,   4.,   3.], device='cuda:0')
--> [train] start
[Epoch 3] loss: 6.184338469029693 acc: 0.5228
[Epoch 7] loss: 4.661200559047787 acc: 0.5691
